\chapter{Conclusions}
\label{chapter:conclusions}

We have discussed the prospects and problems of automatic image and video captioning in this thesis.
%
We motivated the problem of caption generation as step-up from the simpler image
classification problem and as a move towards more complete understanding of visual
media.
%%
Captioning is also a good task to measure the progress in both visual feature
extraction and language generation problems.

After identifying the basic building blocks of a captioning system, we reviewed
several relevant methods from the literature.
%%
Choosing the popular encoder-decoder based captioning pipeline presented
in~\cite{Vinyals_2015_CVPR} as the baseline model and several extensions were
proposed to improve it's performance in this thesis.
%%
We also highlighted the complexities involved in evaluating a caption generation
method and discussed the four popular evaluation metrics used in the literature.

Based on the results showing transferability of features from convolutional
neural networks~(CNN) trained for image classification, the vector of
activations from one such CNN~(GoogLeNet) was adapted as the primary image
feature vector to be used in the image captioning models presented in this
thesis.
%%
It was also shown that augmenting these CNN features with explicit object and
scene detector features greatly improves the performance.
%%
In the case of videos, owing to the lack of a dominant feature representation,
few different video feature extraction techniques were experimented with.
%%
Three video features aimed at action recognition were utilized in different
experiments, namely the dense trajectories, improved dense trajectories and C3D
features.
%%
Additionally, frame-level features extracted using image CNNs and object
detectors were also utilized and it was shown that the best results are obtained
when combining the two feature extraction paradigms.

The baseline language model was extended by adding an additional input channel
and by increasing its depth.
%%
Both these extensions were shown to improve the overall performance.
%%
The experiments presented in this thesis also demonstrated that the recently
introduced residual connections are effective even when adapted to Long-Short
Term Memory~(LSTM) networks.
%%
The residual connections help achieve lower training and validation costs, and
also improve the training speeds.

An efficient ensembling technique based on an evaluator network was also
presented,
%%
The evaluator network utilizes a CNN sentence encoder and a similarity measure,
to pick the best caption from a pool of candidates generated by multiple
language models.
%%
This ensembling technique was shown to work best in video captioning of the
MSR-VTT dataset, wherein the language models participating in the ensemble were
good at different parts of the dataset.
%%
In the image captioning task on COCO dataset, using the CNN based evaluator only
moderately improves the performance as per the automatic metrics, but it greatly
improves the diversity of the generated captions.

Utilizing the video captioning methods presented here, we participated in two
video captioning challenges namely the LSMDC and MSR-VTT challenges.
%%
Our methods won both the competitions as judged by human evaluators.
%%
The work presented in this thesis has also led to three
publications~\cite{shetty2015video,ShettyACMMM2016Wrk,ShettyACMMM2016}.

To put the current state of the captioning systems into context, we can draw a
layman's analogy to the progression of visual description capabilities in
humans.
%%
When we learn to speak, in the first stage we learn to utter only a few
independent words.
%%
In the second stage, we learn to repeat and re-compose a few sentences heard from
our teachers.
%%
Only in the later stages we learn to precisely reason about every word we utter
and can invent novel descriptions or write long essays about a single image. 
%%
We see the current image and video captioning systems to be in the second stage,
wherein they tend to repeat and in some cases recompose parts of the captions
they have seen in the training set.
%%
There is still much work to be done before machines can generate well-reasoned
novel descriptions and maybe even thousand word essays describing the content
of the visual input.

