\chapter{Conclusions}
\label{chapter:conclusions}

We have discussed the problems of image and video captioning in this thesis.
%
We motivated the problem of caption generation as step-up from the simpler image
classification problem and a move towards more complete understanding of visual
media.
%%
It is also a good task to measure progress in both visual feature extraction and
language generation problems.

After identifying the basic building blocks of a captioning system, we reviewed
several relevant methods from the literature.
%%
We chose the popular encoder-decoder based captioning pipeline presented
in~\cite{Vinyals_2015_CVPR} as our baseline model and propose several extensions
to it.
%%
We also discussed the complexities involved in evaluating a caption generation
method and discussed the four popular evaluation metrics used in the literature.

Based on the results showing transferability of features from CNN trained for
image classification, we adapt it as the primary image feature vector.
%%
But we showed that augmenting these CNN features with explicit object and scene
detector features greatly improves the performance.
%%
In case of videos, owing to lack of a dominant feature representation, we
experiment with a few different techniques.
%%
We utilize three features aimed at action recognition, dense trajectories,
improved dense trajectories and C3D features.
%%
Additionally, we also use frame-level features extracted using image CNNs and
object detectors and show that the best results are obtained when combining the
two paradigms.

We extend the baseline model by adding an additional input channel and
increasing its depth and show that both these extensions improve the overall
performance.
%%
We also show that the recently introduced residual connections are effective
even when adapted to LSTM networks.
%%
They help achieve lower training and validation costs, and also improve training
speeds.

We also present a good ensembling technique based on an evaluator network,
utilizing a CNN sentence encoder network and a similarity measure, to combine
multiple language models.
%%
This ensembling technique was shown to work best on the video captioning on
the MSR-VTT dataset, wherein the language models participating in the ensemble were
good at different parts of the dataset.
%%
On the image captioning task on COCO dataset, using the CNN based evaluator only
moderately improves the performance as per the automatic metrics, but it greatly
improves the diversity in the generated captions.

Utilizing the video captioning methods presented here, we participated in two
video captioning challenges namely LSMDC and MSR-VTT challenges.
%%
Our methods won both the competitions as judged by human evaluators.
%%
The work presented in this thesis has also led to three
publications~\cite{shetty2015video,ShettyACMMM2016Wrk,ShettyACMMM2016}

To put the current state of the captioning systems into context, we can draw a
lay-man's analogy to the progression of visual description capabilities in
humans.
%%
When we learn to speak, in the first stage we learn to utter only a few
independent words.
%%
In the second stage we learn to repeat and re-compose few sentences heard from
our teachers.
%%
Only in the later stages we learn to precisely reason about every word we utter
and can compose novel descriptions or write long essays about a single image. 
%%
We see the current image and video captioning systems to be in the second stage,
wherein they tend to repeat and in some cases recompose parts of the captions
they have seen in the training set.
%%
There is still much work to be done before machines can generate well-reasoned
novel descriptions and maybe even thousand word essays describing the content
of the visual input.

