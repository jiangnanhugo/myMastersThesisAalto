\chapter{Looking forward}
\label{chapter:discussion}
\section{What is achieved and what is lacking?}
\fixme{about 2 pages: Fresh Writing}
\red{Now that we have seen in detail, how image and video captioning systems work,
and how well they perform in terms of automatic evaluation metrics.}
%%
We have seen that the image captioning system works relatively better than the
video captioning system, partly owing to smaller datasets and partly to
in-efficient feature representations for videos.
%%
In many cases, captions generated for images are descriptive and accurate, as
seen from examples in \red{Appendix\ref{appXX}}.
%%
There is good amount of novel captions generated by the models, showing that it
not only learns to just mimic the training data, but also to use the phrases
seen in the training set in different compositions.
The generative language model is also good at using correct grammar
while describing the content, as seen in Table~\ref{tab:resLsmdcTestHum}
%%

But there are still many areas where the captioning models fall short.
%%
First and fore-most the vocabulary the models learn to use is very limited, with
even our most diverse image captioning model, CNN ensmeble based C21, using only
1/8th of the word seen in the training data. \red{Does this go here? or in the
previous chapter}
%%
This limits the utility of such a model outside of the specific dataset it was
trained on.
%%
A method to integrate new words into the vocabulary of the model, without
re-training it entirely would be a good extension to make such captioning
models viable to use on images and videos in the wild.

Another major bottleneck hindering the progress of captioning systems is the
lack of effective and efficient methods to evaluate them.
%%
Both kinds of automatic evaluation metrics, ones adopted from machine
translation and ones devised specifically for this task, fall short in matching
up to human judgement of the quality the captions generated.
%%
These metrics perform better with access to larger number of reference captions,
but those are expensive to collect.
%%
Alternatively, if one relies solely on human judgements, it still is a tedious
and expensive process to get captions from every variant of an algorithm
evaluated.

From a designer's perspective, a drawback of a LSTM based captioning system is
that the interpretability of these models is low.
%%
Specifically, it is not very apparent how much the visual features are
responsible for generation of a specific word and how much it is caused solely
by the bias in the language model.
%%
This interpretability becomes especially important to diagnose the erroneous
captions, for eg. when we see the model incorrectly caption an image containing
"a white fire hydrant" as a "red fire hydrant", it is hard to tell if the image
feature incorrectly encoded the color as red or if the language model, due to
the bias in the training data towards red fire hydrants, overrides the image
features to produce the wrong caption.  
We can observe this 
Talk about limited vocabulary
Image and Videos in the wild
Need for costly annotation and evaluation

\section{Future directions}
\fixme{about 3-4 pages: Fresh Writing}
\subsection{Scene Graph prediction/ matching}
\subsection{More than one sentence}
\subsection{Longer video summarization, movie story extraction}
\subsection{Better video encodings?}
\subsection{Inferring knowledge from this kind of captioning}

