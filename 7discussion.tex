\chapter{Looking Forward}
\label{chapter:discussion}
We have, thus far, reviewed the methods and architectures used to build
state-of-the-art image and video captioning systems and analyzing their
performance qualitatively and quantitatively.
%%
Now, it is time to take a step back and determine the progress that has been
made and discuss the problems that yet needs to be addressed.
%%
In this chapter, we will identify the strengths and weaknesses of the visual
captioning systems presented in this thesis.
%%
Then, we will discuss a few interesting research problems that will help further
improve the visual captioning systems and the more general task of machine
understanding of visual media.  \section{Where Do We Stand?}

So far in this thesis we have seen in detail, how image and video captioning
systems work, and how well they perform in terms of automatic evaluation
metrics, heuristic diversity statistics and human evaluation.
%%
We have seen that the image captioning system works relatively better than the
video captioning system, partly due to smaller training datasets for video and
partly due to deficient feature representations for videos.
%%
In particular, video captioning systems perform reasonably well on clips
containing less shot boundaries and camera motion, for example in categories
like \emph{how-to} and \emph{games} videos on the MSR-VTT dataset.
%%
On the other hand they suffer in categories containing complex video styles, for
example in the case of \emph{news} one could have multi-pane videos, and
\emph{movie} videos usually contain lots of scene changes.
%%

In many cases, captions generated for images are descriptive and accurate, as
seen from examples in \red{Appendix\ref{appXX}}.
%%
One can find a good amount of novel captions generated by the models, showing
that they not only learn to just mimic the training data, but also to use the
phrases seen in the training set in novel compositions.
%%
The generative language model is also able to use correct grammar while
describing the visual content, sometimes even better than human captions, as
seen in Table~\ref{tab:resLsmdcTestHum}.
%%

Nevertheless, there are still many areas where the captioning models fall short.
%%
First and foremost, the vocabulary the models learn to use is very limited.
%%
Even our most diverse image captioning model, CNN-ensemble-based C21,
is using only 1/8th of the words seen in the training data. 
%%
This limits the utility of such a model outside of the specific dataset it was
trained on.
%%
A method to integrate new words into the vocabulary of the model, without
re-training it entirely, would be a good extension to make such captioning
models viable to use on images and videos in the wild.

Another major bottleneck hindering the progress of captioning systems is the
lack of effective and efficient methods to evaluate them.
%%
Both kinds of automatic evaluation metrics, ones adopted from machine
translation and ones devised specifically for this task, fall short in matching
up to human judgement of the quality of the captions generated.
%%
These metrics perform better when they have access to a larger number of
reference captions, but those are expensive to collect.
%%
Alternatively, if one relies solely on human judgements, it still is a tedious
and expensive process to get captions from every variant of an algorithm
evaluated.

From an algorithm design perspective, despite their quite impressive
performance, a drawback of the LSTM-based captioning system is that the
interpretability of these models is low.
%%
Specifically, it is not apparent how much the visual features are responsible
for the generation of a specific word and how much it is caused solely by the
bias in the language model.
%%
This interpretability becomes especially important when aiming to diagnose the
erroneous captions.
%%
For example, when we see a model incorrectly caption an image containing a white
fire hydrant as a ``red fire hydrant'', it is hard to tell if the image feature
incorrectly encoded the color as red or if the language model, due to the bias
in the training data towards red fire hydrants, overrides the image features to
produce the wrong caption.  
%%

\section{Future Directions}
%%
Based on the shortcomings just discussed, we now enumerate a few problems
worthwhile to explore, in order to address these shortcomings and to further the
image and video understanding research.
%%
Here we try to define and motivate the problems, but with only a limited
discussion on how they could be solved.

\subsection{Generating Multiple Captions} Any image and video is a rich source
of information, and can be described by a multitude of captions emphasizing
different aspects in them. 
%%
Our captioning system only generates a single sentence and could thus be an
incomplete description of the visual content.
%%
Even when we use beam search to sample multiple captions, all the top-ranked
captions in the beam tend to be related to each other, sometimes differing only
in grammatical arrangements, and thus still only cover a small portion of the
available visual information.
%%

A system which can generate multiple captions, ideally with minimum overlap and
maximum coverage of the visual information, would be a welcome extension.
%%
One way to design such a system would be to further condition the caption
generation with an ``information vector'' which encodes the visual information
already described in previously generated captions.
%%
The captioning could also be conditioned on specific bounding boxes
instead of the entire image, as done in~\cite{johnson2015densecap}, and produce
captions describing different parts of the image.
%%
The system~\cite{johnson2015densecap} was trained using the regional captions
present in the Visual Genome dataset~\cite{krishnavisualgenome}.
%%
Still, although captioning smaller bounding boxes is a way to generate multiple
descriptions, this often leads to mundane captions describing single objects
and ignoring the larger context in the image.

\subsection{Better Video Features}
We have seen that the video captioning systems proposed in this thesis perform
inferior to the image captioning system.
%%
This is especially exacerbated in longer videos and videos with large scene
changes, where we see that the captions describe only certain small portions of
the video.
%%
One important cause is that the video features we have used are not good at
capturing long-term dependencies and are not designed to handle abrupt scene
changes.
%%
Both action-based features, trajectory-based and the C3D features, only
recognize short-term actions, due to restrictions on the trajectory length and
video-segment length, respectively.
%%
The pooling techniques used to combine frame-level features into a single
vector also lose all temporal information and thus fail to capture such long-
term activities.

Thus, a good video feature encoding mechanism, which preserves long term temporal
information and can gracefully handle shot boundaries, is a vital component which
could improve the video captioning drastically.
%%
This could also enable us to build a captioning system for long videos and even
generate detailed summaries of long videos.
%%
An attempt to generate descriptive paragraph captions for videos was recently made 
in~\cite{yu2015video}.
%%
The authors use two recurrent networks in a hierarchical manner, the lower one
to generate the current sentence and the upper one to keep track of the
paragraph state.
%%
The paragraph network re-initializes the sentence-generating RNN after every
sentence, based on the current state of the paragraph.
%%
Nevertheless, such an approach is still faraway from a complete video summarizer,
which could watch long videos and provide a concise summary of the events in
them.

\subsection{Scene Graph Prediction and Matching}
Although captioning is a good way to summarize the visual content in an
image or a video for humans, it is not the optimal input representation of such
information for machines to process further.
%%
This is because, any application which uses this summary of visual
information based on captions would also need to be able to correctly parse the
caption and extract the needed information.
%%
This is not straightforward due to the inherent ambiguities in the natural language.
%%

A computationally more useful and less ambiguous representation of the visual
information is in the form of scene graphs.
%%
Scene graphs consists of nodes which represent the entities present in the scene
and edges which encode the relationship between these entities.
%%
The nodes can also have a list of attributes associated with them, encoding
information such as the color, position, etc.
%%
Thus, instead of describing an image using a natural language caption, we could
represent it using a scene graph.
%%
Unlike natural language descriptions, a scene graph representation would be unique
for a set of objects and attributes.
%%
This allows further processing stages to easily parse the requisite information.
%%
Such a representation could also be used to easily aggregate explicit knowledge
about objects and their most characteristic attributes.

Recently, in~\cite{schuster2015generating} a method to generate scene graphs
from textual description of images was presented.
%%
There have also been attempts retrieve images using scene graphs as input
in~\cite{johnson2015image}.
%%
The authors of~\cite{johnson2015image} also show that scene-graph-based
retrieval is more accurate than using captioning methods to rank and retrieve
images directly from this textual description.
%%
In general, the problem of generating scene graphs taking images as input is
still unsolved.
