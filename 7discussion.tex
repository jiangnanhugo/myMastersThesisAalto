\chapter{Looking forward}
\label{chapter:discussion}
\section{Where Do We Stand?}

So far in this report we have seen in detail, how image and video captioning
systems work, and how well they perform in terms of automatic evaluation
metrics and heuristic diversity statistics.
%%
We have seen that the image captioning system works relatively better than the
video captioning system, partly due to smaller training datasets for video and
partly to deficient feature representations for videos.
%%
In particular, video captioning systems perform reasonably well on clips
containing less shot boundaries and camera motion, for eg. in categories like
"how-to videos" and "games" on MSR-VTT dataset.
%%
But they suffer in categories containing complex video styles, for eg. in case
of "news" one could have multi-pane videos, and videos containing lots of scene
changes, eg. in "movies".
%%

In many cases, captions generated for images are descriptive and accurate, as
seen from examples in \red{Appendix\ref{appXX}}.
%%
There is good amount of novel captions generated by the models, showing that it
not only learns to just mimic the training data, but also to use the phrases
seen in the training set in different compositions.
%%
The generative language model also uses correct grammar while describing the
visual content, sometimes even better than human captions, as seen in
Table~\ref{tab:resLsmdcTestHum}
%%

But there are still many areas where the captioning models fall short.
%%
First and fore-most the vocabulary the models learn to use is very limited,
with even our most diverse image captioning model, CNN ensemble based C21,
using only 1/8th of the word seen in the training data. 
%%
This limits the utility of such a model outside of the specific dataset it was
trained on.
%%
A method to integrate new words into the vocabulary of the model, without
re-training it entirely would be a good extension to make such captioning
models viable to use on images and videos in the wild.

Another major bottleneck hindering the progress of captioning systems is the
lack of effective and efficient methods to evaluate them.
%%
Both kinds of automatic evaluation metrics, ones adopted from machine
translation and ones devised specifically for this task, fall short in matching
up to human judgement of the quality the captions generated.
%%
These metrics perform better with access to larger number of reference
captions, but those are expensive to collect.
%%
Alternatively, if one relies solely on human judgements, it still is a tedious
and expensive process to get captions from every variant of an algorithm
evaluated.

From a algorithm design perspective, despite the impressive performance , a
drawback of a LSTM based captioning system is that the interpretability of
these models is low.
%%
Specifically, it is not very apparent how much the visual features are
responsible for generation of a specific word and how much it is caused solely
by the bias in the language model.
%%
This interpretability becomes especially important to diagnose the erroneous
captions.
%%
For example when we see the model incorrectly caption an image containing a white
fire hydrant as a "red fire hydrant", it is hard to tell if the image feature
incorrectly encoded the color as red or if the language model, due to the bias
in the training data towards red fire hydrants, overrides the image features to
produce the wrong caption.  
%%

\section{Future Directions}
%%
Based on the shortcomings discussed in the previous sub-section, we now discuss
a few problems worthwhile to explore, to address these shortcomings and
further the image and video understanding research.
%%
Here we only try to define and motivate the problems but with only a limited
discussion on how they could be solved.

\subsection{Generating Multiple Captions}
Any image and video is a rich source of information, and can be described by a
multitude of captions describing different aspects of them. 
%%
Our captioning system only generates a single sentence, and thus could be an
in-complete description of the visual content.
%%
Even when we use beam-search to sample multiple captions, all the top ranked
captions in the beam tend to be related each other, sometimes differing only in
grammatical arrangements, and thus still only cover a small portion of visual
information.
%%

A system which can generate multiple captions, ideally with minimum overlap
and maximum coverage of the visual information, would be a welcome extension.
%%
One way to design such a system would be to further condition the caption
generation with an "information vector" which encodes the visual information
already described in previously generated captions.
%%
The captioning could also instead be conditioned on specific bounding boxes
instead of entire image, as done in~\cite{johnson2015densecap}, and produce
captions describing different parts of the image.
%%
This system was trained using the regional captions present in the Visual
Genome dataset.
%%
But, although captioning smaller bounding boxes is a way to generate multiple
descriptions, this often leads to mundane captions describing a single object
and ignoring the larger context in the image.

\subsection{Better Video Encodings}
We have seen that our video captioning systems, perform inferior to our image
captioning system.
%%
This is especially exacerbated in longer videos and videos with large scene
changes, where we see that the captions describe only certain small portions of
the video.
%%
One important reason is that the video features we have used are not good at
capturing long term dependencies and are not designed to handle abrupt scene
changes.
%%
Both the action based features, trajectory based and the C3D features, only
recognize short term actions, due to restrictions on trajectory length and
video-segment length respectively.
%%
The pooling techniques used to combine, frame-level features into a single
vector, also looses all temporal information and thus fails to capture such long
term variations.

Thus, a good video feature encoding mechanism which preserves long term temporal
information and can gracefully handle shot boundaries is a vital component which
could improve the video captioning drastically.
%%
This could also enable us to build captioning system for long videos and even
generate detailed summaries of long videos.
%%
An attempt to generate descriptive paragraph captions for videos was made 
in~\cite{yu2015video} recently.
%%
Here the authors use two recurrent networks in a hierarchical manner, lower one
to generate the current sentence and the upper one to keep track of the
paragraph state.
%%
The paragraph network re-initializes the sentence generating RNN after every
sentence, based on the current state of the paragraph.
%%
But, this is still far-off from a complete video summarizer, which could watch
long videos and provide a concise summary of the events in the video.
\subsection{Scene Graph Prediction and Matching}
Although the captioning is a good way to summarize the visual content in an
image or a video for humans, it is not the optimal representation of such
information for machines to process.
%%
This is because, to build any application which uses this summary of visual
information based on captions, we also need to be able to correctly parse the
caption and extract the needed information.
%%
This is not straightforward due to the inherent ambiguities in natural language.
%%

A more computationally useful and less ambiguous representation of the visual
information, is in form of scene graphs.
%%
Scene graphs are graphs with nodes being the entities present in the scene
and edges encode the relationship between these entities.
%%
The nodes can also have a list of attributes associated with it, encoding
information such as color, position etc.
%%
Thus, instead of describing an image using a natural language caption, we could
represent it using a scene graph.
%%
Unlike natural language descriptions, scene graph representation would be unique
for a set of objects and attributes.
%%
This allows further stages to easily parse the requisite information.
%%
Such a representation could also be used to easily aggregate explicit knowledge
about objects and their most common attributes.

Recently, in~\cite{schuster2015generating} a method to generate scene graphs
from textual description of images is presented.
%%
There has also been an attempt retrieve images using scene graphs as input
in~\cite{johnson2015image}.
%%
Here, authors also show that this retrieval is more accurate than using
captioning methods to rank and retrieve images directly from the textual
description.
%%
But the problem of generating scene graphs taking images as input is still
unsolved.
