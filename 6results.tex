\chapter{Experiments and Results}
\label{chapter:results}

Now that we have discussed several feature extraction methods and alternate
language model architectures, it is time to put to test these proposals and
evaluate how well they perform on the captioning task.
%%
In this chapter we will present the results from the experiments on three
separate datasets, one for image captioning (MS-COCO) and two for video
captioning~(LSMDC and MSR-VTT).
%%
We will mainly rely on the four evaluation metrics described in
section~\ref{sec:EvaluationMetrics}, namely BLEU, METEOR, ROUGE-L and CIDEr, to
evaluate the performance of our models on the captioning task.
%%
But as already discussed, these evaluation metrics only approximately track the
human judgements of how good the generated captions are.
%%
However, it is a costly exercise to collect human evaluations for every model we
wish to compare.
%%
Luckily enough, human judgements were collected and used to evaluate such
systems in three captioning challenges held over the course of last year.
%%
We will present results from our participation in these challenges, and present
human evaluations for few of our best models, obtained from these competitions.

Since the COCO dataset is the largest of the three datasets we have used, we
conduct all the language model experiments on this dataset.
%%
This means, the performance analysis of our proposed extensions namely, addition
of persist features, increasing depth with residual connections and class based
factorization of output is conducted on this dataset.
%%
Also, since the language model we use in video captioning has the same
architecture as the one used in image captioning, we can assume the same results
hold there too.
%%
Thus we limit our experiments in video captioning task to testing different
video features.


\section{Image Captioning}
\fixme{3 pages: tables from ACMMM paper, text should be re-written with more
qualitative analysis}
\red{Also highlight the drawbacks here using examples from of the COCO dataset}

In this section we report results of our experiments on image captioning
conducted on the MS-COCO dataset.
%%
In the first subsection we will discuss the experimental setup, i.e. how the
models were trained and the hyper-parameters used, and the evaluation setup.
%%
Next we report the results of our internal evaluation on the validation set
comparing various models and features.
%%
This is followed by results from the test set obtained from the Microsoft
codalab portal.

\subsection{Experimental setup}
The implementation details of the language models we train on COCO dataset
remains the same as described in section~\ref{subsec:implDetails}.
%%
The CNN feature extraction and the Faster R-CNN models are based on the Caffe
library~\cite{jia2014caffe}.
%%
Using the code released for the Faster R-CNN
network\footnote{https://github.com/rbgirshick/py-faster-rcnn}, we train it on
the MS-COCO dataset to detect the 80 object categories annotated in COCO.
%%
Then this detector is run on the entire COCO dataset and the spatial map feature
vectors are created using the bounding boxes output by the detectors.

%%
The CNN evaluator we use is created with bi-, tri-, 4-, and 5-gram filters and
$N_{\text{filt}}=100$ filters of each type.
%%
The word vectors are chosen to have $N_{\text{word vec dim}}=100$ dimensions.
%%
For each image we sample $k=50$ negative captions.
%%

To evaluate the utility of the proposed set of image features and LSTM network
architectures, we use the COCO 2014 validation set and the five reference
sentences available for all images in it.
%%
The performance of a model is measured using the perplexity assigned by the
model to the ground truth sentences in the validation set (referred to as
``Pplx'' in tables), as well as with the four standard evaluation metrics
(BLEU-4, METEOR, ROUGE-L and CIDEr) to compare the generated captions against
the references.

Microsoft COCO team has also made an evaluation server available on
CodaLab\footnote{\url{https://competitions.codalab.org/competitions/3221}} where
researchers can upload their captions for the test set and view the resulting
evaluation metrics.
%%
We use this portal to evaluate our models on the COCO Image Captioning Challenge
2014 test set. 
%%
Here, we also compare our performance against other well-performing or
state-of-the-art entries in the CodaLab leaderboard.

%% ---------------------------------------------------------------------------

\subsection{Results on Validation Set}
In Chapters~\ref{chapter:langModel} \& ~\ref{chapter:langModel} we proposed
several image features and language model extensions respectively to improve the
captioning system over the baseline model.
%%
In order to obtain the best posssible captioning system for the COCO dataset, we
need to measure the performance of these different features and language model
combinations.
%%
Since training a model for every combination of feature and choice of language
model parameters is prohibitively expensive, we run seperate experiments to
determine the best features and the best language model choices, while keeping
the other fixed.
%%
\red{We show that these two aspects are fairly independent and chosing the best
feature and best language model from the above mentioned experiments gives us
our best captioning model.}
%%

%--------------------------------------------------------------------------------------
\subsubsection{Evaluating the Init and Persist paths}
\label{subsubsec:InitVpersist}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c||c|c||c|c|c|c|c|}
    \hline
    \bf Model & \multicolumn{2}{c||}{\bf Features 
    } & \multicolumn{5}{c|}{\bf Performance metrics}\\
     \cline{2-8}
    \bf \# & init & persist &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C1 & gCNN  & ---  & 0.259 & 0.222 & 0.490 & 0.750 & 10.82  \\
    C2 & gCNN  & SVM80& 0.259 & 0.226 & 0.492 & 0.782& \red{xxx}  \\
    C3 & gCNN  & gCNN &\bf 0.302 & 0.243 &\bf 0.523 & 0.897 & 10.25  \\
    C4 & SVM80 & gCNN &\bf 0.302 &\bf0.244 &\bf 0.523 &\bf0.909 & 10.30  \\
    C5 & SVM80 & SVM80& xxxxx & xxxxx & xxxxx & xxxxx & xxxxx  \\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{ Evaluating the utility of the init and persist input channels to the
          LSTM language model}
  \label{tab:resCocInitVPers}
\end{table*}

In Chapter~\ref{chapter:langModel} we introduced a new input to the LSTM language
model, the \emph{persist} path, positing that it is beneficial for the language
model to have access to the visual features throughout the caption generation
process.
%%
This new input also enables us to provide two different features as input to the
language model.
%%
Table~\ref{tab:resCocInitVPers} presents the results from experiments trying to
determine the best use for the two input channels, \emph{init} and
\emph{persist}.
%%
In these experiments, our language model has only a singel layer of LSTM cells
with both the word-embeddings and LSTM layer being of 512 dimensions.

Model C1 only uses the \emph{gCNN} (from GoogleLeNet) image features as the
\emph{init} input and is our baseline model.
%%
Compared to this, additionally providing the 80 dimensional detector features,
\emph{SVM80},
using the \emph{persist} channel improves the performance slightly, in model C2.
%%
Instead, if we provide the gCNN features to both the inputs, as in model C3,
there is dramatic improvement in the performance in all the four metrics, as
well as in validation perplexity.
%%
This tells us that it is beneficial for the language model to have access to the
CNN features throughout the caption generation process.
%%

Instead of redundantly using the same \emph{gCNN} features in both \emph{init} and
\emph{persist}, we can now replace the \emph{init} feature with the \emph{SVM80}
feature to get a marginal performance improvement as seen in model C4.
%%
Model C5 tells us that using only \emph{SVM80} features is not good and CNN
features need to be presented in the \emph{persist} to get the best performance. 
%%

%--------------------------------------------------------------------------------------
\subsubsection{Finding the best Image features}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \bf Model & \bf \multirow{2}{*}{Init feature} & \multicolumn{5}{c|}{\bf Performance metrics}\\
    \cline{3-7}
    \bf \# &\bf &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C4 & SVM80               & 0.302 & 0.244 & 0.523 & 0.909 & 10.30  \\
    C6 & FRC80               & 0.316 & 0.249 &\bf0.534 & 0.952 & 10.15  \\
    C7 & SUN397              & 0.301 & 0.241 & 0.521 & 0.894 & 10.40  \\
    C8 & SUN397$\oplus$FRC80 & 0.315 &\bf0.250 & 0.532 &0.954 &10.05  \\\hline
    C9 & 4$\times$4IoU       & 0.302 & 0.244 & 0.522 & 0.913 & 10.21  \\
    C10 & 4$\times$4Gauss    & 0.308 & 0.246 & 0.527 & 0.921 & 10.15  \\
    C11 & 3+3Gauss           & 0.308 & 0.247 & 0.527 & 0.928 & 10.08  \\\hline
    C12 &\parbox[c][][c]{4cm}{\smallskip\centering 3+3Gauss$\oplus$SUN397\\$\oplus$FRC80\smallskip} 
                             &\bf0.318&\bf0.250&0.533 &\bf0.957&\bf9.93\\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{ Evaluating the efficacy various image feautures using fixed language model
          configuration}
  \label{tab:resCocFeatExpt}
\end{table*}

Next we report from our experiments to determine the best image features to pair
with the CNN features.
%%
Using the results from previous subsection as guideline, we keep the gCNN
features as \emph{persist} input in all the experiments, and only change the
feature input in the \emph{init} channel.
%%
The language model parameters also remain the same as
subsection~\ref{subsubsec:InitVpersist}.
%%
Table~\ref{tab:resCocFeatExpt} presents the results from these experiments.
%%
Note that, here we use the ``$\oplus$'' symbol to denote the vector
concatenation operation.

Comparing the results of models C4 and C6, we see that the FRC80 features
outperforms the SVM80 features with a specially significant gain in the CIDEr
metric.
%%
The Faster R-CNN based object features thus seem to overcome the simpler SVM
detector output based features.
%%
This also supports our hypothesis that good explicit object detectors can
effectively compliment the CNN image features. 
%%
The object detectors are trained to detect multiple objects explicitly, and
although they don't encode any information about the object shape or other
attributes, just the information about probability of occurence of different
objects seems very beneficial to the captioning task.

Using the Scene detection features, \emph{SUN397}, alone as \emph{init} input in
model C7 worsens the performance.
%%
But augmenting the \emph{FRC80} object features with scene information by
concatenating \emph{SUN397} features as shown in model C8 improves the
performance over C6 in 3 metrics.

Next we compare the spatial grid features in models C9 through C11.
%%
We find that using the integral of Gaussian performs better than using the
intersection-over-union (IoU) measure when constructing these features as seen
by comparing C9 and C10. 
%%
In general, however, the spatial grid features do not match the performance of
the FRC80 features, even though FRC80 only encodes a subset of the information
represented in the spatial grid features.
%%
This could be due to the fact that the spatial grid features are of much higher
dimension than the \emph{FRC80} feature vectors.
%%
This hypothesis is also strengthened by observing that model C11, which uses
smaller \emph{3+3Gauss} features, performs the best among the models using the spatial
grid features.

%%
Next we train model C12 with concatenating the \emph{FRC80} , \emph{SUN397}  and
\emph{3+3Gauss}. 
This model now has access to object detection , scene type and object location
information apart from the CNN features and is our best performing model with
this language model configuration.

\subsubsection{How deep should we go?}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \bf Model & \bf \multirow{2}{*}{Depth} & \multicolumn{5}{c|}{\bf Performance metrics}\\
    \cline{3-7}
    \bf \# &\bf &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C8  & 1   & 0.315 & 0.250 & 0.532 & 0.954 &10.05  \\\hline
    C13 & 2   & 0.318 & 0.252 & 0.535 &\bf0.967 & 10.14  \\
    C14 & 3   & 0.316 & 0.253 & 0.533 & 0.964   & 10.34  \\
    C15 & 4   & xxxxx & xxxxx & xxxxx & xxxxx   & xxxxx  \\\hline
    C16 &2-res&\bf0.320& 0.253 &\bf0.536&0.966  & 9.92   \\
    C17 &3-res& 0.316 &\bf0.254&0.532 & 0.962   &\bf9.69 \\
    C18 &4-res& xxxxx &xxxxx&xxxxx & xxxxx   &xxxxx\\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Results from experiments with language model depth, with fixed input features}
  \label{tab:resultsVal}
\end{table*}

We now present experiments with depth of the LSTM language model.
%%
For these experiments, LSTM layer size and word encoding size is still held at
512 dimensions, but only the number of LSTM layers is changed.
%%
The \emph{SUN397$\oplus$FRC80} features are used a \emph{init} input and
\emph{gCNN} features are used as \emph{persist} input.



\subsubsection{Ensembling and Class based factorization}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \bf Model & \bf \multirow{2}{*}{Init Feature} & \multicolumn{5}{c|}{\bf Performance metrics}\\
    \cline{3-7}
    \bf \# &\bf &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C19-cls &\parbox[c][][c]{4cm}{\smallskip\centering 3+3Gauss$\oplus$SUN397\\$\oplus$FRC80\smallskip} 
                             & 0.287 & 0.245 & 0.523 & 0.911 & 10.10 \\\hline
    C17 & SUN397$\oplus$FRC80& 0.316 &\bf0.254&0.532 & 0.962   &\bf9.69 \\
    C20 &\parbox[c][][c]{4cm}{\smallskip\centering 3+3Gauss$\oplus$SUN397\\$\oplus$FRC80\smallskip} 
                             & 0.319 & 0.252 & 0.535 & 0.970 & 9.72 \\\hline
    C21& CMME                & xxxxx & xxxxx & xxxxx & xxxxx & -- \\
    C22& CNN Evaluator       &\bf0.320&\bf0.254 &\bf0.536 &\bf0.978 & -- \\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Comparison of the best depth 3 models and ensembling techniques}
  \label{tab:resultsVal}
\end{table*}

\subsubsection{Qualitative and language diversity analysis of the captions}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|}
    \hline
    \bf Model\# &\bf vocab size & Unique Sent & New Sent \\\hline\hline
    C1      & 513  & xxxx & xxxx  \\
    C8      & 962  & xxxx & xxxx  \\
    C16     & 983  & xxxx & xxxx  \\
    C17     & 1112 & xxxx & xxxx  \\
    C19-cls & xxxx & xxxx & xxxx  \\
    C20     & 1112 & xxxx & xxxx  \\
    C21     & xxxx & xxxx & xxxx  \\
    C22     & 1303 & xxxx & xxxx  \\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Language diversity statistics of our best models }
  \label{tab:resultsVal}
\end{table*}


Table \ref{tab:resultsVal} shows the perplexity and evaluation metrics on the
validation set for the different feature combinations and language models we
have studied.

The columns \emph{init} and \emph{persist} indicate what visual features were
used as the initializing and persistent inputs to the language model,
respectively.
%%
Entries containing multiple features separated with a `+' sign imply that these
features have been concatenated together to form a higher-dimensional feature
vector.
%%
The column \emph{depth} specifies the number $N$ of LSTM layers in the model,
with $N$-res being an LSTM network with $N$ layers and residual connections.

Model \#1 uses the gCNN feature vector as the \emph{init} feature without any
\emph{persist} feature and it serves as our baseline model.
%%
Model \#2 moves the gCNN feature to the \emph{persist} input and uses the SVM80
features for the \emph{init} input. 
%%
Comparing these two, we can clearly see that adding the \emph{persist} feature
greatly improves the performance in all metrics.

Then we compare the effect of increasing the number of LSTM layers in models \#8
to \#11. 
%%
Without residual connections, both the 2- and 3-layer models perform similarly
in terms of the evaluation metrics, but the 3-layer model has worse perplexity.
%%
Adding the residual connections significantly improves the perplexity while the
performance on the metrics remain approximately the same.

For model \#12, we train a 3-layer LSTM with residual connections and a feature
vector containing FRC80, SUN397 and the 3+3Gauss features as the \emph{init}
input. This model achieves the best CIDEr among all of our single models.

Model \#13 is our best performing model on the validation set. 
%%
We use the CNN evaluator based ensemble to choose the best candidate caption for
each image from a candidate pool generated by six of our best models.
%%
The six models used here include \#3, \#9, \#11, \#12, and two models trained
using concatenating the SUN397 with two spatial grid features 3+3Gauss and
4$\times$4Gauss, respectively.

Table \ref{tab:resultsVal} also shows in the \emph{vocab} column the size of the
vocabulary used by the models when generating captions on the validation set.
%%
This is a good metric to capture how diverse the captions generated by each
model are. 
%%
We see that adding the \emph{persist} feature and increasing the number of
layers increase the vocabulary size.
%%
Also the ensemble model has a significantly larger vocabulary, most likely
because it picks the captions from a diverse pool of candidates.


%% ---------------------------------------------------------------------------


Table 1. Comparing init and persist pipes
Table 2. Comparing features with the swapped pipeline 
Table 3. Testing depth and class factorization 
Table 4. Ensembling 

\subsection{Results on Test Set}

\begin{table*}[htp]
  \newcommand{\mct}[1]{%
    \multicolumn{2}{c|}{\bf#1}}
  \centering
  \caption{COCO 2014 test set results from CodaLab}
  \begin{adjustbox}{center}
  \begin{tabular}{||l|c|c|c|c|c|c|c|c|c||}
    \hline\hline
    \multirow{2}{*}{\bf CodaLab name}&\multirow{2}{*}{\bf\#}
                       &\mct{BLEU-4} &\mct{METEOR} &\mct{ROUGE-L}&\mct{CIDEr}\\\cline{3-10}
                &     & c5    & c40   &  c5   & c40   & c5  &  c40  &  c5  &  c40 \\\hline\hline
    frcnnBigger (ours)& 12    & 0.315 & 0.597 & 0.251 &0.340& 0.531 &\bf0.683&\bf0.956&\bf0.968\\
    --- (ours)        & 13    & 0.310 & 0.596 & 0.250 &0.338& 0.529 & 0.681& 0.948& 0.961\\
    ATT\_VC~\cite{you2016image}& ---   &\bf0.316&0.599 & 0.250 &0.335&\bf0.535&0.682& 0.943& 0.958\\
    --- (ours)        & 11    & 0.309 & 0.588 & 0.251 &0.342& 0.529 & 0.680& 0.943& 0.948\\
    OriolVinyals~\cite{Vinyals_2015_CVPR}      & ---   & 0.309 & 0.587 &\bf0.254&\bf0.346& 0.530 & 0.682& 0.943& 0.946\\
    MSR\_Captivator~\cite{Fang2015} & ---   & 0.308 &\bf0.601& 0.248 &0.339& 0.526 & 0.680& 0.931& 0.937\\
    human~\cite{Chen2015}& ---   & 0.217 & 0.471 & 0.252 &0.335& 0.484 & 0.626 & 0.854 & 0.910\\\hline
    \hline
  \end{tabular}
  \end{adjustbox}
  \label{tab:resultsTest}
\end{table*}

We uploaded the captions generated with models \#11, \#12 and \#13 for the test
images to the CodaLab portal to evaluate the performance on the COCO 2014 test
set and to be able to compare against the best performing and state-of-the-art
results by other research groups.

Table~\ref{tab:resultsTest} shows the evaluation metrics obtained by these
models along with a few other top entries in the CodaLab portal. 
%%
We can see that the performances of our models drop slightly on the test set
compared to that on the validation set. 
%%
Also, our best model on the validation set, \#13 is outperformed by model \#12
on the test set.

Note that each metric in Table~\ref{tab:resultsTest} has two columns, \emph{c5}
and \emph{c40}. 
%%
This is because COCO data set contains 40 reference captions for a small subset
of images in the test set, referred to as \emph{c40}. 
%%
As already mentioned, using a larger number of reference captions makes the
metrics better correlated with human judgments and thus the metrics obtained on
the \emph{c40} are more reliable.
%%
The \emph{c5} metrics are obtained from the regular test set with only five
reference captions. 

Our best model on the test set, model \#12, is visible in the CodaLab
leaderboard as ``frcnnBigger''.  
%%
We observe that this model tops the leaderboard in three out of the eight
metrics shown in Table~\ref{tab:resultsTest}, more than any other model.
%%
Had we ignored the somewhat questionable BLEU-4 measure, our result would be the
top one in three measures out of six.
%%
In either case, we can conclude that our method achieves the state-of-the-art
results in this benchmark.


%=================================================================================
%=================================================================================
\section{Video Captioning}
\fixme{2 pages: from ICCV paper, and fresh}
%=================================================================================
\subsection{On LSMDC}

\begin{table*}[t]
  \newcommand{\modpar}[4]{%
    \multirow{2}{*}{\emph{#1}} & \multirow{2}{*}{#2} & \multirow{2}{*}{#3}
    & \multirow{2}{*}{#4}}
\centering
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
        \hline\hline
        \bf model    &\bf init &\bf persist &\bf perplex &\bf beam size &\bf avg.len &\bf Bleu\_4 &\bf CIDEr &\bf METEOR &\bf ROUGE\_L    & \\\hline\hline
        \modpar{1 coco-kf}{kf}{--}{ - }      & 1 & 9.62  &   0.003 &   0.049 &   0.053 &   0.116 &     \\\cline{5-11}
                                       & & & & 5 & 8.74  &   0.003 &   0.045 &   0.044 &   0.105 & (p) \\\hline
        \modpar{2 coco-kf+cls}{kf}{cls}{ - } & 1 & 10.17 &   0.003 &   0.045 &   0.053 &   0.113 &     \\\cline{5-11}
                                       & & & & 5 & 9.04  &   0.003 &   0.045 &   0.044 &   0.103 &     \\\hline
        \modpar{3 coco-cls+kf}{cls}{kf}{ - } & 1 & 9.62  &   0.003 &   0.052 &   0.053 &   0.114 &     \\\cline{5-11}
                                       & & & & 5 & 8.66  &   0.003 &   0.049 &   0.045 &   0.104 &     \\\hline\hline
        \modpar{4 kf}{kf}{--}{56.08}         & 1 & 5.24  &   0.004 &   0.071 &\bf0.058 &   0.140 &     \\\cline{5-11}
                                       & & & & 5 & 3.39  &   0.002 &   0.063 &   0.043 &   0.114 &     \\\hline
        \modpar{5 kf+cls}{kf}{cls}{60.78}    & 1 & 5.36  &   0.004 &   0.073 &\bf0.060 &   0.142 &     \\\cline{5-11}
                                       & & & & 5 & 3.46  &   0.001 &   0.054 &   0.043 &   0.111 &     \\\hline
        \modpar{6 cls+kf}{cls}{kf}{59.07}    & 1 & 5.12  &\bf0.005 &   0.087 &\bf0.059 &\bf0.144 &     \\\cline{5-11}
                                       & & & & 5 & 3.50  &   0.003 &   0.071 &   0.047 &   0.122 &     \\\hline\hline
        \modpar{7 traj}{traj}{--}{54.89}     & 1 & 5.28  &\bf0.005 &   0.087 &   0.057 &\bf0.145 &     \\\cline{5-11}
                                       & & & & 5 & 3.75  &   0.003 &   0.074 &   0.047 &   0.123 &     \\\hline
        \modpar{8 traj+cls}{traj}{cls}{59.75}& 1 & 5.28  &\bf0.005 &   0.081 &   0.057 &   0.141 &     \\\cline{5-11}
                                       & & & & 5 & 3.48  &   0.003 &   0.074 &   0.047 &   0.123 &     \\\hline
        \modpar{9 cls+traj}{cls}{traj}{55.14}& 1 & 5.33  &\bf0.006 &\bf0.092 &\bf0.058 &\bf0.146 & (b) \\\cline{5-11}
                                       & & & & 5 & 3.80  &   0.004 &   0.082 &   0.049 &   0.128 &     \\\hline\hline
    \end{tabular}
    \medskip
    \caption{Results obtained on the public test set of LSMDC2015. 
      %%
      ``kf'' stands for using keyframe-based features, ``traj'' for
      dense trajectory-based video features and ``cls'' for visual 
      content classification results as inputs to the \emph{init}
      and \emph{persistent} input lines of the LSTM network.
      %%
      Submissions (p) and (b) were the ones visible in the
      CodaLab leaderboard for the public and blind test sets, 
      respectively, by the closing time of the LSMDC 2015 Challenge.}
    \label{tab:results}
\end{table*}

To evaluate various forms of our model we used the the LSMDC 2015 public test
set as the benchmark. The evaluation is performed using four standard metrics
used in the LSMDC evaluation server namely:
METEOR~\cite{denkowski-lavie:2014:W14-33},
BLEU~\cite{Papineni:2002:BMA:1073083.1073135}, ROUGE-L~\cite{lin2004rouge} and
CIDEr~\cite{Vedantam_2015_CVPR}.
%%
Table~\ref{tab:results} shows these four metrics computed for different models.
%%
In addition to the metrics, we also show the perplexity of the model on the
public test set and the average lengths of the generated sentences.
%%
Results are provided always for beam sizes 1 and 5 used in the caption
generation stage.

\newcommand{\model}[1]{\emph{#1}}

In order to get a quick baseline, we used models trained earlier on the COCO
dataset to generate captions on the LSMDC test set with a simple rule-based
translation applied to their output.  This translation is done in order to
better match the LSMDC vocabulary and is implemented using the simple
$w_{\text{in}} \longrightarrow w_{\text{out}}$ rule:
%%
\begin{align} \label{eqTrans} w_{\text{out}} = \begin{cases} \text{SOMEONE},&
\text{if } w_{\text{in}} \in \{\text{man}, \text{woman}, \\&
\text{\mbox{\qquad\qquad person}}, \text{boy}, \text{girl} \}\\ w_{\text{in}},&
\text{otherwise.} \end{cases} \end{align}

Models 1--3 in Table~\ref{tab:results} are such translated models trained on the
COCO dataset.
%%
Model \model{1~coco-kf} was trained on the COCO dataset using concatenated
GoogLeNet-based features with a total dimensionality of 4096 as the \emph{init}
features. 
%%
This approach matches the use of the NeuralTalk model described
in~\cite{Vinyals_2015_CVPR} and~\cite{Karpathy_2015_CVPR}. 
%%
Model \model{2~coco-kf+cls} was trained using GoogLeNet as the \emph{init}
features and the outputs of the 80 SVM classifiers as the \emph{persistent}
feature, while model \model{3~coco-cls+kf} was trained with the role of these
two feature types reversed.
%%
The results of these models have in our earlier experiments shown increasingly
better performance on the COCO dataset itself, but we can hardly observe such
progression in the translated results on the LSMDC dataset.

Next, we trained three models similar to the above COCO models, but now with
captions available and features and content classification results calculated
from the keyframes of the videos in the LSMDC 2015 dataset.
%%
The results are presented as models 4--6 in Table~\ref{tab:results}. 
%%
Here we can see the benefit of using \emph{persistent} features as the model
\model{6~cls+kf} performs better than the models trained solely on keyframe
features.

Finally, we trained three models using the dense trajectory video features and
the keyframe-based SVM output features, presented in Table~\ref{tab:results} as
models 7--9.
%%
Again we see that using the higher-dimensional feature, here the dense
trajectory feature, as the \emph{persistent} input to the LSTM network gives the
best performance among the group of models.
%%
The result of model \model{9~cls+traj} can also be regarded as the best one
obtained in our experiments and therefore we have used it in our final blind
test data submission to the LSMDC 2015 Challenge.

As we can see from Table 1, the \emph{persistent} dense trajectory video
features combined with the \emph{init} SVM classifier features from keyframes
outperform all the other models in three out of the four metrics used.
%%
Comparing this with model \model{6~cls+kf} shows that using video features as
opposed to just keyframe features gives a better performance.
%%
It also indicates that combining the keyframe and video features is better than
using just the video features.

A rather surprising finding is that using larger beam sizes in inference lead to
poorer performance.
%%
This is slightly counterintuitive, but can be understood when we look at the
lengths of the sentences produced by these two beam sizes. 
%%
For example, model \model{9~cls+traj} produces sentences with the average length
of 5.33 words with beam size 1, while with beam size 5 the average length drops
to just 3.79 words. This is because with higher beam sizes the model always
picks the most likely sentence and penalizes heavily any word it is unsure of.
This results in the model picking very generic sentences like \emph{``SOMEONE
looks at SOMEONE''} over more descriptive ones.

The results~(p) and~(b) in Table~\ref{tab:results} match our public and blind
test data submissions, respectively, visible in the CodaLab leaderboard%
\footnote{\scriptsize\url{https://www.codalab.org/competitions/6121\#results}}.

%=================================================================================
\subsection{On MSR-VTT}
Additionally, we utilize the video category information available for all videos
in all splits of the dataset.
%%
This information is input to the language model as a one-hot vector of 20
dimensions.

In order to measure the performance differences due to the different feature
combinations and architectural changes, we use the validation set of the MSR-VTT
dataset which contains 497 videos.
%%
Performance is measured quantitatively using the standard automatic evaluation
metrics, namely METEOR, CIDEr, ROUGE-L and BLEU. 

Table~\ref{tab:resultsVal} shows the results on the validation set.
%%
The columns \emph{init} and \emph{persist} indicate the features used for those
input channels, respectively, in the language model.
%%
The column \emph{depth} is the number of layers used in the language model and
\emph{perplex} is the perplexity measure on the validation set.

Models \#1, \#2 and \#3 all use the dense trajectory (dt) features as
\emph{init} input and the mean pooled frame-level GoogLeNet features
concatenated with the video category vector (gCNN+20Categ) as the \emph{persist}
input.
%%
They vary in the number of layers in the language model.
%%
Comparing their performance we see that the 2-layer model outperforms the single
layered model by a small margin, while the 3-layer one is the inferior one.

Model \#4 is similar to \#2, but uses the improved dense trajectories (idt) as
the \emph{init} input instead.
%%
Model \#5 differs from \#2 by the fact that it uses mean pooled 3-D
convolutional features as the \emph{persist} input.
%%
We see that both \#4 and \#5 are competitive, but slightly worse than our best
single model, \#2.
%%
Upon qualitatively analyzing the model outputs, we see that each of them
performs well on different kinds of videos.
%%
For example, model \#5, which only uses input features trained for action
recognition, does well in videos involving a lot of motion, but suffers in
recognizing the overall scenery of the video.
%%
Conversely, model \#2 trained on frame-level features does better in recognizing
objects and scenes, but makes mistakes with the sequence of their appearance,
possibly due to the pooling operation.
%%
This phenomenon can also be observed in the second row of images in
Figure~\ref{fig:capSamps}. Model \#5 produces a better caption on the video in
the first column, while \#2 does better on the video in the second column.

To get maximum utility out of these diverse models, we use the CNN evaluator
network to pick the best candidate from the pool of captions generated by our
top four models, \#1, \#2, \#4 and \#5.
%%
The evaluator is trained using the gCNN+20Categ as the video feature.
%%
This result is shown as model \#6 in Table~\ref{tab:resultsVal}.
%%
We can see that the CNN evaluator significantly outperforms, in all the four
metrics, every single model it picks its candidates from.
%%
The first row of Figure~\ref{fig:capSamps} shows some examples where the CNN
evaluator picks a better caption than the one generated by our best single
model.

\label{sec:discussion}
\begin{figure*}[thp]
  \begin{center}
    \includegraphics[width=0.30\linewidth]{images/9565.jpg}
    \includegraphics[width=0.30\linewidth]{images/9430.jpg}
    \includegraphics[width=0.30\linewidth]{images/7997.jpg}\\
    \includegraphics[width=0.30\linewidth]{images/9150.jpg}
    \includegraphics[width=0.30\linewidth]{images/9799.jpg}
    \includegraphics[width=0.30\linewidth]{images/7632.jpg}
  \end{center}
  \vspace*{-5mm}
  \caption{Sample captions generated for some test set videos. The
    first row shows samples where the evaluator model \#6 outperforms
    models \#2 and \#5, and the second row cases where the evaluator
    performs worse.}
  \label{fig:capSamps}
\end{figure*}

\begin{table*}[thp]
  \caption{Performance of various features and 
    network depths on the validation set of MSR-VTT}
  \newcommand{\modpar}[4]{%
    \multirow{2}{*}{\emph{#1}} & \multirow{2}{*}{#2} & \multirow{2}{*}{#3}
    & \multirow{2}{*}{#4}}
  \centering
  \newcommand{\bs}{\small\bf}
  \begin{tabular}{||c|c|c|c|c|c|c|c|c|}
    \hline\hline
    \bf\# &\bf init &\bf persist &\bf depth &\bf perplex &\bs BLEU-4 &\bs METEOR &\bs CIDEr &\bs ROUGE-L \\\hline\hline
    1 & dt  & gCNN+20Categ & 1  & 27.31 & 0.396 & 0.268 & 0.438 & 0.588 \\
    2 & dt  & gCNN+20Categ & 2  & 27.73 & 0.409 & 0.268 & 0.433 & 0.598 \\
    3 & dt  & gCNN+20Categ & 3  & 28.44 & 0.370 & 0.262 & 0.397 & 0.575 \\\hline
    4 & idt & gCNN+20Categ & 2  & 28.13 & 0.398 & 0.268 & 0.432 & 0.587 \\
    5 & dt  & c3dfc7       & 2  & 29.58 & 0.369 & 0.268 & 0.413 & 0.577 \\\hline
    6 & \multicolumn{4}{c|}{\em CNN evaluator based ensemble of best 4 models}
                                  & \bf0.411 & \bf0.277 & \bf0.464 & \bf0.596 \\\hline
    \hline
  \end{tabular}
  \label{tab:resultsVal}
\end{table*}

%% ---------------------------------------------------------------------------

\subsection{Challenge Results}

Since the CNN evaluator model performed the best on the validation set, we
submitted that result in the MSR-VTT Challenge.
%%
Our submission appears on the leaderboard as \emph{Aalto}.
%%
The submissions were evaluated on the blind test set using the above mentioned
four automatic metrics.
%%
These results are shown in Table~\ref{tab:resultsTestMet}.
%%
Our submission achieved the best scores in the CIDEr metric and was ranked
overall second considering the average ranking across the metrics.

The submissions were also subject to human evaluation as the automatic metrics
are known to deviate from human judgements.
%%
This was seen in previous captioning challenges in the case of both
image~\cite{CocoChallengeSlides} and
video~\cite{DBLP:journals/corr/RohrbachTRTPLCS16} data.
%%
The human evaluation was based on three criteria: Coherence (C1), Relevance (C2)
and Helpfulness for the blind (C3).
%%
Table~\ref{tab:resultsTestMet} presents the results of the evaluation.
%%
The overall ranking was obtained again by considering the mean ranking across
the three metrics.
%%
As per human judgement, our submission was ranked the first among the 22 entries
in the challenge.

Analyzing the two leaderboards, the automatic metric based one and the human
evaluation based one, we see that the disagreement between the two is relatively
minor, with most teams in the top 10 changing their ranking by only one
position.
%%
This can most likely be attributed to having a large number of 20 reference
captions per video for the evaluation.

\begin{table}[th]
  \caption{Top 5 teams as per metrics on the test set}
  \centering
  \newcommand{\bs}{\small\bf}
  \scalebox{0.9}{
  \begin{tabular}{||c|c|c|c|c|}
    \hline\hline
    \bf Team  &\bs BLEU-4 &\bs METEOR &\bs CIDEr &\bs ROUGE-L \\\hline\hline
    v2t\_navigator &\bf0.408 &\bf0.282 & 0.448 &\bf0.609 \\
    \bf Aalto      & 0.398 & 0.269 &0.457 & 0.598 \\
    VideoLAB       & 0.391 & 0.277 & 0.441 & 0.606 \\
    ruc-uva        & 0.387 & 0.269 &\bf0.459 & 0.587 \\
    Fudan-ILC      & 0.387 & 0.268 & 0.419 & 0.595 \\\hline
    \hline
  \end{tabular}}
  \label{tab:resultsTestMet}
\end{table}

\begin{table}[th]
        \caption{Top 5 teams as per human evaluation}
  \centering
  \newcommand{\bs}{\small\bf}
  \begin{tabular}{||c|c|c|c|}
    \hline\hline
    \bf Team  &\bs C1 &\bs C2 &\bs C3 \\\hline\hline
    \bf Aalto      & \bf3.263 & 3.104 & \bf3.244\\
    v2t\_navigator & 3.261 & 3.091 & 3.154 \\
    VideoLAB       & 3.237 & \bf3.109 & 3.143 \\
    Fudan-ILC      & 3.185 & 2.999 & 2.979 \\
    ruc-uva        & 3.225 & 2.997 & 2.933 \\\hline
    \hline
  \end{tabular}
  \label{tab:resultsTestHum}
\end{table}

%% ===========================================================================

\section{Conclusions}


\section{Summary of Results}
\fixme{About 2 pages, Fresh writing }
Summarizing results of both image and video, how they are similar and different
(for eg. kinds of mistakes made in video vs image).
