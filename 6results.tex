\chapter{Experiments and Results}
\label{chapter:results}

So far, we have discussed several feature extraction methods and alternate
language model architectures for automatic caption generation.
%%
Now, it is time to put these proposals to test and evaluate how well they
perform on the captioning task.
%%
In this chapter the results from the experiments on three separate datasets, one
for image captioning (MS-COCO) and two for video captioning~(LSMDC and MSR-VTT),
will be discussed.
%%
We will rely on the four evaluation metrics described in
Section~\ref{sec:EvaluationMetrics}, namely BLEU, METEOR, ROUGE-L and CIDEr, to
evaluate the performance of our models on the captioning task.
%%
Additionally, on validation sets, we also compute the models' perplexities of
the ground truth captions and use this as a fifth metric of performance. 

As already discussed, these automatic evaluation metrics only approximately
track the human judgements of how good the generated captions are.
%%
To meticulously evaluate the captioning models, we need to collect human
judgements on the captions generated by them.
%%
However, it is a costly exercise to collect human evaluations for every model
that needs to be compared.
%%
Luckily enough, human judgements were collected and used to evaluate such
systems in the captioning challenges held over the course of the last year.
%%
Few of the best video captioning models presented in this thesis were also
submitted to two such video captioning challenges.
%%
The human evaluations obtained from the two challenges will be presented here,
and the results of the competitions will be summarized. 

Since the COCO dataset is the largest of the three datasets we have used, we
conduct all the language model experiments on this dataset.
%%
These experiments include the performance analysis of our proposed language
model extensions namely, addition of persist features, increasing depth with
residual connections and utilizing hierarchical decoder is conducted on the COCO
dataset.
%%
Also, since the language model used in video captioning has the same
architecture as the one used in image captioning, it is assumed that the same
results will hold there too.
%%
Thus we limit our experiments in the video captioning task to the evaluation of
different video features.

%%
Finally, a note on the notation used to represent different models presented
in this chapter.
%%
We will use the notation ``$\delta\#$'' to denote different models with $\delta$
representing the dataset on which the model is trained and $\#$ being the model
number within the dataset.
%%
Thus, $\delta=$`C' for the models trained on the COCO dataset, $\delta=$`L' for
models trained on the LSMDC dataset and $\delta=$`M' for ones trained on the
MSR-VTT dataset.

\section{Implementation details}
%%
Before getting into the evaluation, we will first briefly discuss some
implementation platform details, and some hyper-parameter choices.
%%
\paragraph*{Language Model:} The proposed LSTM language model used
for both image and video captioning is implemented using the Theano
library~\cite{Bastien-Theano-2012}.
%%
Theano allows to run the computations on a GPU with minimum effort, leading to
much quicker training and testing cycles.
%%
The language models are trained using stochastic gradient descent with the
RMSProp~\cite{rmspropTielman} algorithm and the dropout regularization is
implemented as described in \cite{ZarembaSV14}.
%%
The error is back-propagated to all the language model parameters and word
embedding matrices, but the image feature extraction models are kept constant.
%

The language model is trained by minimizing the negative log-likelihood assigned
by the model to the training samples.
%%
But, as noted before, the evaluation of the trained model is done using the automatic
evaluation metrics.
%%
This discrepancy between the training objective and the evaluation cost is
forced upon us as the automatic metrics themselves are not differentiable and it
would thus be hard to train a model directly to optimize these metrics.
%%
Still, we can justify using the perplexity as the training cost, by empirically
observing that the perplexity measure roughly tracks the automatic metrics.
%%
Concretely, optimizing the model parameters to minimize perplexity also improves the
model performance as per automatic metrics.
%%
To verify this, a quick experiment was conducted where the perplexity measure is
computed and recorded on the validation and training sets using the intermediate
models after every epoch of training.
%%
Simultaneously, these intermediate models are used to generate captions on the
validation set and automatic metrics are computed on these generated captions.
%%
Results are plotted in Figure~\ref{fig:MetVsPerplex}.
%%
We see that perplexity indeed tracks the performance on the other evaluation
metrics very well.

In all our experiments we use beam search to generate sentences from a trained
model.
%%
After experimenting with different beam sizes, we found that the beam size of
$b=5$ works well across all our models on the COCO and the MSR-VTT datasets,
whereas $b=1$ worked better on the LSMDC dataset.

%------------------------------------
\begin{figure*}[ht]
\begin{center}
  \includegraphics[width=0.7\linewidth]{images/MetVsPerplex.pdf}
\end{center}
\vspace*{-10mm}
\caption{Evolution of training and validation perplexity and automatic metrics
        computed on the COCO validation set during training.}
\label{fig:MetVsPerplex}
\end{figure*}
%% ---------------------------------------------------------------------------
\paragraph*{Visual Feature Extraction:}%
%%
The CNN feature extraction and the Faster R-CNN models are based on the Caffe
library~\cite{jia2014caffe}.
%%
Using the code released for the Faster R-CNN
network\footnote{\url{https://github.com/rbgirshick/py-faster-rcnn}}, we train it on
the MS-COCO dataset to detect the 80 object categories annotated in COCO.
%%
Then, this detector is run on the entire COCO dataset and the spatial map feature
vectors are created using the bounding boxes output by the detectors.
%%

Dense trajectory feature extraction on videos was done using the source code
provided by the authors of~\cite{DBLP:conf/cvpr/WangKSL11}.
%%
To extract the C3D features, the code and the model pre-trained on the
Sports-1M dataset provided by the authors of~\cite{DBLP:C3D} was used.

\paragraph*{CNN Evaluator:} The proposed CNN evaluator is also implemented using
the Theano library. 
%%
It is created with bi-, tri-, 4-, and 5-gram filters and $F_n=100$
filters of each type.
%%
The word vectors used in the evaluator are chosen to have $W_{dim}=100$ dimensions.
%%
In order to train the CNN evaluator, each ground truth caption--image/video pair
needs $k$ negative captions as well.
%%
In order to optimize the training process, we sample $k+1$ random image caption
pairs and treat the other $k$ captions as negative samples for each image.
%%
This is much faster than sampling separate negative captions for each
image-caption pair, and improved the overall training speed.
%%
In our experiments, we found setting $k=49$ achieved good results.

\section{Image Captioning}

In this section we report results of our experiments on image captioning
conducted on the MS-COCO dataset.
%%
In the first subsection we will discuss the MS-COCO dataset in detail.
%%
Next the results from the local evaluation conducted on the COCO validation set,
comparing various choices for language model architectures and image features,
are reported.
%%
This is followed by results from the test set obtained from the Microsoft
CodaLab portal and a comparison to some best published works on this dataset.

\subsection{MS-COCO Dataset}
In all our image captioning experiments we use the Microsoft COCO 2014
dataset~\cite{Lin2014} for training and evaluation.
%%
This dataset consists of 164,062 images split into training set of 82,783
images, validation set of 40,504 images and test set of 40,775 images. 
%%
An additional 40k test images were released in the 2015 version of the dataset,
but it is not used in the experiments reported in this thesis.
%%
The training and validation sets have five reference captions for each image
annotated by humans. 
%%
The total 413,915 reference captions from the training set have 23,528 unique
words.

The COCO dataset also has object segmentations available for each image and for
objects belonging to 80 specified categories.
%%
The categories consist of common object types such as \emph{person}, \emph{car},
\emph{bus}, \emph{skateboard}, etc.
%%
We make use of the object segmentations when training the Faster Region-based
Convolutional Neural Networks for extracting the object location features, as
described in Section~\ref{sec:frcnnfeat}.
%%
The segmentation information is, however, not utilized when training the LSTM
language model nor when using it with the validation and test images.

Before using the reference captions in the COCO dataset for training, we
tokenize the text and remove symbols and numerals.
%%
Words occurring less than five times are also removed in oder to weed out spelling
mistakes and extremely rare words, for which we have insufficient data to learn.
%%
This leaves us with a training corpus vocabulary consisting of 8,790 unique words. 
%%===========================================================================%%

To evaluate the utility of the proposed set of image features and LSTM network
architectures, we use the COCO 2014 validation set and the five reference
sentences available for all images in it.
%%
Microsoft COCO team has also made an evaluation server available on
CodaLab\footnote{\url{https://competitions.codalab.org/competitions/3221}} where
researchers can upload their captions for the test set and view the resulting
evaluation metrics.
%%
We use this portal to evaluate our models on the COCO Image Captioning Challenge
2014 test set. 
%%
Here, we also compare our performance against other well-performing or
state-of-the-art entries in the CodaLab leaderboard.

\subsection{Results on Validation Set}
In Chapters~\ref{chapter:VisFeatChapter} and ~\ref{chapter:langModel} we
discussed several image features and language model extensions, respectively, to
improve the captioning system over the baseline model.
%%
In order to obtain the best possible captioning system for the COCO dataset, we
need to measure the performance of these different features and language model
combinations.
%%
Since training a model for every combination of a feature and a choice of
language model parameters is prohibitively expensive, separate experiments are
conducted to determine the best features and the best language model choices,
while keeping the other fixed.
%%
We observe that these two aspects are fairly independent and choosing the best
feature and the best language model independently  gives us the best captioning
model.
%%
%--------------------------------------------------------------------------------------
\subsubsection{Evaluating the Init and Persist Paths}
\label{subsubsec:InitVpersist}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c||c|c||c|c|c|c|c|}
    \hline
    \bf Model & \multicolumn{2}{c||}{\bf Features 
    } & \multicolumn{5}{c|}{\bf Performance metrics}\\
     \cline{2-8}
    \bf \# & init & persist &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C1 & gCNN  & ---  & 0.289 & 0.238 & 0.514 & 0.860 & 10.43  \\
    C2 & gCNN  & SVM80& 0.292 & 0.239 & 0.516 & 0.871 & 10.34  \\
    C3 & gCNN  & gCNN &\bf 0.302 & 0.243 &\bf 0.523 & 0.897 & 10.25  \\
    C4 & SVM80 & gCNN &\bf 0.302 &\bf0.244 &\bf 0.523 &\bf0.909 & 10.30  \\
    C5 & SVM80 & SVM80& 0.261 & 0.225 & 0.492 & 0.785 & 10.78 \\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Evaluating the utility of the init and persist input channels to the
          LSTM language model on COCO validation set.}
  \label{tab:resCocInitVPers}
\end{table*}

In Chapter~\ref{chapter:langModel} we introduced a new input to the LSTM language
model, the \emph{persist} path, positing that it is beneficial for the language
model to have access to the visual features throughout the caption generation
process.
%%
This new input also enables us to provide two different features as input to the
language model.
%%
Table~\ref{tab:resCocInitVPers} presents the results from experiments trying to
determine the best use for the two input channels, \emph{init} and
\emph{persist}.
%%
The columns \emph{init} and \emph{persist} indicate what visual features were
used as the initializing and persistent inputs to the language model
respectively.
%%
In these experiments, our language model has only a single layer of LSTM cells
with both the word-embeddings and the LSTM layer being of 512 dimensions.

Our baseline model, C1, only uses the \emph{gCNN} (from GoogLeNet) image
features as the \emph{init} input.
%%
Compared to this, additionally providing the 80 dimensional detector features,
\emph{SVM80},
using the \emph{persist} channel improves the performance slightly, in model C2.
%%
Instead, if we provide the \emph{gCNN} features to both the inputs, as in model C3,
there is a dramatic improvement in the performance in all the four metrics, as
well as in the validation perplexity.
%%
This tells us that it is beneficial for the language model to have access to the
CNN features throughout the caption generation process.
%%

Instead of redundantly using the same \emph{gCNN} features in both \emph{init} and
\emph{persist} paths, we can now replace the \emph{init} feature with the \emph{SVM80}
feature to get a marginal performance improvement as seen in model C4.
%%
Model C5 tells us that using only the \emph{SVM80} features is not good and the CNN
features need to be presented in the \emph{persist} to get the best performance. 
%%

%--------------------------------------------------------------------------------------
\subsubsection{Finding the Best Image Features}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \bf Model & \bf \multirow{2}{*}{Init feature} & \multicolumn{5}{c|}{\bf Performance metrics}\\
    \cline{3-7}
    \bf \# &\bf &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C4 & SVM80               & 0.302 & 0.244 & 0.523 & 0.909 & 10.30  \\
    C6 & FRC80               & 0.316 & 0.249 &\bf0.534 & 0.952 & 10.15  \\
    C7 & SUN397              & 0.301 & 0.241 & 0.521 & 0.894 & 10.40  \\
    C8 & SUN397$\oplus$FRC80 & 0.315 &\bf0.250 & 0.532 &0.954 &10.05  \\\hline
    C9 & 4$\times$4IoU       & 0.302 & 0.244 & 0.522 & 0.913 & 10.21  \\
    C10 & 4$\times$4Gauss    & 0.308 & 0.246 & 0.527 & 0.921 & 10.15  \\
    C11 & 3+3Gauss           & 0.308 & 0.247 & 0.527 & 0.928 & 10.08  \\\hline
    C12 &\parbox[c][][c]{4cm}{\smallskip\centering 3+3Gauss$\oplus$SUN397\\$\oplus$FRC80\smallskip} 
                             &\bf0.318&\bf0.250&0.533 &\bf0.957&\bf9.93\\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Evaluating the efficacy various image features using fixed language model
          configuration on COCO validation set.}
  \label{tab:resCocFeatExpt}
\end{table*}

Next we report from the experiments to determine the best image features to pair
with the CNN features.
%%
Using the results from previous subsection as guideline, we keep the gCNN
features fixed as \emph{persist} input in the following experiments, and only change the
feature input in the \emph{init} channel.
%%
The language model parameters also remain the same as
Section~\ref{subsubsec:InitVpersist}.
%%
Table~\ref{tab:resCocFeatExpt} presents the results from these experiments.
%%
Note that, here we use the ``$\oplus$'' symbol to denote the vector
concatenation operation.

Comparing the results of models C4 and C6, we see that the \emph{FRC80} features
outperforms the SVM80 features with a specially significant gain in the CIDEr
metric.
%%
The Faster R-CNN based object features thus seem to overcome the simpler SVM
detector output based features.
%%
This also supports the hypothesis that good explicit object detectors can
effectively complement the CNN image features. 
%%
The object detectors are trained to detect multiple objects explicitly, and
although they don't encode any information about the object shape or other
attributes, just the information about the probability of occurrence of different
objects seems very beneficial to the captioning task.

Using the scene detection features, \emph{SUN397}, alone as \emph{init} input in
model C7 worsens the performance.
%%
However, augmenting the \emph{FRC80} object features with scene information by
concatenating \emph{SUN397} features, as shown in model C8, improves the
performance over C6 in 3 metrics.

Next we compare the spatial grid features using models C9, C10 and C11.
%%
We find that using the integral of Gaussian~(\ref{eqn:Gauss}) performs better
than using the intersection-over-union (IoU) measure~(\ref{eqn:Iou}) when
constructing these features as seen by comparing C9 and C10. 
%%
In general, however, the spatial grid features do not match the performance of
the \emph{FRC80} features, even though \emph{FRC80} only encodes a subset of the
information represented in the spatial grid features.
%%
This could be due to the fact that the spatial grid features are of much higher
dimension than the \emph{FRC80} feature vectors.
%%
This hypothesis is also strengthened by observing that model C11, which uses
smaller \emph{3+3Gauss} features, performs the best among the models using the
spatial grid features.

%%
Next we train model C12 by concatenating the \emph{FRC80}, \emph{SUN397}  and
\emph{3+3Gauss} features. 
This model now has access to object detection, scene type and object location
information apart from the CNN features, and is our best-performing model with
this language model configuration.

\subsubsection{How Deep Should We Go?}
\label{subsec:exptdepth}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \bf Model & \bf \multirow{2}{*}{Depth} & \multicolumn{5}{c|}{\bf Performance metrics}\\
    \cline{3-7}
    \bf \# &\bf &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C8  & 1   & 0.315 & 0.250 & 0.532 & 0.954 &10.05  \\\hline
    C13 & 2   & 0.318 & 0.252 & 0.535 &\bf0.967 & 10.14  \\
    C14 & 3   & 0.316 & 0.253 & 0.533 & 0.964   & 10.34  \\
    C15 & 4   & 0.316 & 0.250 & 0.533 & 0.956 & 10.69  \\\hline
    C16 &2-res&\bf0.320& 0.253 &\bf0.536&0.966  & 9.92   \\
    C17 &3-res& 0.316 &\bf0.254&0.532 & 0.962   &\bf9.69 \\
    C18 &4-res& 0.316 & 0.253 & 0.535 & 0.964   & 9.75 \\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Results from experiments with language model depth, with fixed input features on COCO validation set.}
  \label{tab:resCocDepthExpt}
\end{table*}

Table~\ref{tab:resCocDepthExpt} presents the results from experiments with the
depth of the LSTM language model.
%%
Column \emph{depth} specifies the number $N$ of LSTM layers in the model,
with $N$-res being an LSTM network with $N$ layers and residual connections.
%%
For these experiments, the LSTM layer size and word encoding size are still kept
at 512 dimensions, but only the number of LSTM layers is varied.
%%
The \emph{SUN397$\oplus$FRC80} features are used as the \emph{init} input and
\emph{gCNN} features are used as the \emph{persist} input.

When we increase the number of layers without adding residual connections, we
see that the perplexity metric worsens, although there is moderate improvement
in the other metrics upto the depth of three layers.
%%
This is seen comparing model C8 with models C13--C15.
%%
However, when we increase the depth to four layers, we see that it performs similar to
the single layer model in automatic evaluation metrics with perplexity being
significantly worse.
%%

Adding the residual connections significantly improves the perplexity of the
validation set while the performance on the metrics improves slightly.
%%
The biggest gain is seen in the CIDEr metric.
%%
We also find that the residual connections improve the training convergence speed.
%%
This is illustrated in Figure~\ref{fig:ResVsReg}, where we show the
progression of the perplexity measure during the training of the 4-layered models
with~(C18) and without~(C15) residual connections.
%%
We see that the performance gain seems to saturate by four layers even with residual
connections.
%%
So the choice for the best architecture is between the 2- or 3-layered model with
residual connections, considering all the five metrics.
%%
But since the 3-layered model, C17, produces more diverse captions, as seen in
the Section~\ref{subsubsec:QualAnalCoc} this configuration is chosen for further
experiments.
%------------------------------------
\begin{figure*}[t]
\begin{center}
  \includegraphics[width=0.7\linewidth]{images/ResidualVsRegPerplex.pdf}
\end{center}
\vspace*{-10mm}
\caption{Comparing the evolution of perplexity during training of a 4-layered
        model with and without residual connections on COCO dataset.}
\label{fig:ResVsReg}
\end{figure*}
%------------------------------------

\subsubsection{Hierarchical Decoder}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \bf Model & \bf \multirow{2}{*}{Class clustering} & \multicolumn{5}{c|}{\bf Performance metrics}\\
    \cline{3-7}
    \bf \# &\bf &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C17 & --    & 0.316 &\bf0.254&0.532 & 0.962   &\bf9.69  \\
    C19 & --    &\bf0.319 & 0.252 &\bf0.535 &\bf0.970 & 9.72 \\\hline
    C20 &\parbox[c][][c]{4cm}{\smallskip\centering K-Means, 200 Class \smallskip} 
                             & 0.286 & 0.245 & 0.523 & 0.906 & 10.10 \\\hline
    C21 &\parbox[c][][c]{4cm}{\smallskip\centering Brown, 200 Class\smallskip} 
                             & 0.286 & 0.245 & 0.523 & 0.906 & 10.10 \\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Results from models using a hierarchical decoder.}
  \label{tab:resClsCocValset}
\end{table*}

Previously, we determined the 3-layered model with residual connections,
C17, to be the best language model configuration.
%%
We further upgrade this model by changing the \emph{init} feature to
\emph{3+3Gauss$\oplus$SUN397$\oplus$FRC80} in model C19 shown in
Table~\ref{tab:resClsCocValset}.
%%
This slightly improves the performance, with model C19 improving over model C17
in three measures. 
%%
We use this configuration as the basis to run our experiments with the
hierarchical decoder presented in Section~\ref{sec:ClassFact}.
%%
These results are also presented in Table~\ref{tab:resClsCocValset}.
%%
Both the models with hierarchical decoder shown in
Table~\ref{tab:resClsCocValset}, C20 and C21, use the same \emph{init} and
\emph{persist} features as C19 and have three layers connected with residual
connections.

Model C20 is trained with the hierarchical decoder in which the words are
clustered to classes using the $K$-means method. 
%%
For this, we run $K$-means on word vectors learned by model C19, with $K$=200 to
obtain 200 classes. 
%%
Alternatively, C21 uses class assignments obtained from the Brown clustering
algorithm run on the training corpus, again with 200 classes.

We see that both these models perform similarly, with C21 only slightly better
than C20 in CIDEr metric and perplexity.
%%
More interestingly, both the models are considerably worse than the
corresponding simple decoder model, C19, in all the performance metrics.
%%
On manual inspection of the captions produced by them, we find that both C20 and
C21 make significantly more grammatical errors, with often missing conjunctions
and stop words.
%%
But on the bright side, the models using the hierarchical decoder generate
significantly more diverse and descriptive captions as will be seen in
Section~\ref{subsubsec:QualAnalCoc}.
%%
Thus, it seems that the hierarchical decoder indeed makes the captions richer,
but it also adversely affects the correctness of the captions generated.

\subsubsection{Ensembling}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \bf Model & \bf \multirow{2}{*}{Method} & \bf \multirow{2}{*}{Evaluators}&
    \bf \multirow{2}{*}{Eval type}  & \multicolumn{4}{c|}{\bf Performance metrics}\\
    \cline{5-8}
    \bf \# & & & &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr\\\hline
    C19 & --    & -- & -- & 0.319 & 0.252 & 0.535 & 0.970\\\hline
    C22 & Self-Eval & self      & max      & 0.319 & 0.253 & 0.533 & 0.966\\\hline
    C23 & Mutual-Eval      &  All six  & max-mean & 0.303 & 0.249 & 0.526 & 0.925\\
    C24 & Mutual-Eval      &three best & max-mean & 0.304 & 0.250 & 0.527 &
    0.932\\\cline{3-8}
    C25 & Mutual-Eval      & all six & max-max  & 0.305 & 0.251 & 0.528 & 0.935\\
    C26 & Mutual-Eval      & three best & max-max  & 0.305 & 0.251 & 0.528 & 0.939\\\hline
    C27 & CNN Evaluator    & CNN  & max&\bf0.320&\bf0.254 &\bf0.536 &\bf0.978\\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Comparison of different ensembling techniques.}
  \label{tab:resEnsembCocValset}
\end{table*}
Next results from the experiments ensembling multiple caption generating models
will be discussed.
%%
As a first step, six models are chosen to form the model set participating in
the ensemble.
%%
The six models used here include C6, C14, C17, C19, and two models trained using
the \emph{SUN397} concatenated with one of the two spatial grid features, 3+3Gauss
and 4$\times$4Gauss, respectively.
%%
The models in the ensemble were chosen to include the best-performing models
while also maintaining diversity of architectures in the ensemble.

Model C22 picks the best candidate among the pool of candidate captions using
the \emph{Self-Eval} method discussed in Section~\ref{subsec:MutEval}.
%%
Models C23 to C26 are ensembles based on the \emph{Mutual-Eval} method.
%%
Here, the models mentioned in the \emph{Evaluators} column assign probability to
all the candidate captions.
%%
In case of \emph{all six} all the models participating in the ensemble are used
to rate all the candidates.
%%
In contrast, in \emph{three best} three models with the best perplexity scores
were used to rate the candidates.
%%
Once candidate captions are scored, the technique specified in the \emph{Eval
type} column is used to aggregate these probability scores and pick the best
candidate.
%%
Finally, C27 is the ensemble model which uses the CNN-based evaluator to rate and
pick the candidates.

%%
Comparing their performance based on the evaluation metrics, we see that only the
CNN evaluator based C27 model improves over the best single model participating
in the ensemble, C19.
%%
Other two ensembling techniques fall short, with the self-evaluation based
method doing better than any of the mutual-evaluation based techniques.
%%
Among the mutual evaluation based techniques, we see that using only the three best
models is better than using all the six models as evaluators.
%%
We also see that ``max-max'' works slightly better than ``max-mean'' as a method to
aggregate the scores assigned to a caption.
%%
However, the mutual evaluation based and CNN-based ensembles produce much more
diverse captions than both the single models and the self-evaluation based
ensembles, as we will see in Section~\ref{subsubsec:QualAnalCoc}.
%%

\subsection{Language Diversity Analysis}
\label{subsubsec:QualAnalCoc}
\begin{table*}[htp]
  \centering
  \newcommand{\mlhead}[2]{%
    \parbox[c][][c]{#1}{\smallskip\centering #2 \smallskip}
    }
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \bf Model\# 
    &\mlhead{1.6cm}{\bf Mean Length}
    &\mlhead{2.1cm}{\bf Vocabulary\\ Size} 
    &\mlhead{2.1cm}{\bf\% Unique Captions} 
    &\mlhead{2cm}{\bf\% New Captions} 
    &\mlhead{2cm}{\bf~ \\ Comments} \\\hline\hline
    C1      & 9.27 &  814 & 16.10 & 11.76 & \emph{init} vs\\
    C4      & 9.08 &  923 & 22.42 & 17.23 & \emph{persist}\\\hline
    C8      & 9.02 &  962 & 23.23 & 18.25& \\
    C16     & 9.11 &  983 & 26.39 & 20.80& varying  \\
    C17     & 9.18 & 1197 & 31.14 & 24.03& depth      \\
    C18     & 9.23 & 1164 & 31.10 & 24.28&    \\\hline
    C19     & 9.01 & 1112 & 28.43 & 22.04& regular vs \\
    C21 &\bf9.58& 1191 &\bf49.16 &\bf44.39& factorized\\\hline
    C22     & 9.06 &  993 & 21.34 & 15.36& \multirow{3}{*}{\mlhead{2cm}{ensemble\\ models}}   \\
    C26     & 9.38 &\bf 1380 & 41.65 & 33.64& \\
    C27     & 9.13 & 1303 & 40.35 & 32.33& \\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Language diversity statistics of our best models.}
  \label{tab:resCocQual}
\end{table*}

The evaluation based on the automatic metrics presented in the previous sub-section
despite being the accepted standard in the literature, is not very intuitive and
does not give an idea of how rich the captions generated by the models are.
%%
To address this, we will look at few simple metrics which measure the diversity
of the captions generated by the models.
%%
Specifically, we generate captions to the 40,504 images in the COCO validation
set by using different models and look at four different heuristic measures of
diversity in these captions.
%%
These heuristic measures are 1) the mean length of the generated captions, 2)
the size of the vocabulary used by the models, 3) the percentage of unique
captions generated, and 4) the percentage of generated captions not seen in the
training set.
%%

Table \ref{tab:resCocQual} shows these statistics computed for various
models. 
%%
Comparing the statistics for C1 and C4 we see that adding the \emph{persist} path
moderately improves vocabulary size and diversity of the captions generated,
but with the average length of the captions dropping slightly.
%%
Next, comparing models C8, C16, C17 and C18, which vary only in the depth of the
language model, we see that increasing depth improves all the four statistics,
with the 3-layered model generating $\sim24\%$ new captions.

The biggest gain in sentence diversity is seen when we use the hierarchical
decoder based model, C21. 
%%
Comparing it to equivalent model C19, we see that C21 generates almost twice as
many new captions, using a vocabulary of approximately the same size.

Ensembling of multiple language models also helps improve the vocabulary size
and diversity of the captions, with the exception of model C22.
%%
Model C22, based on self-evaluation, seems to prefer un-original captions and
common words, with the percentage of new sentences dropping to just $\sim15\%$.
%%
This is the second lowest of all the models presented in
Table~\ref{tab:resCocQual}.
%%
Contrastingly, both model C26, based on mutual evaluation, and model C27, based on CNN
evaluator, pick almost double the number of new captions.
%%
They also have significantly larger vocabulary and rival the C21 model in
terms of caption diversity.

Considering both the language diversity statistics in Table\ref{tab:resCocQual}
and automatic evaluation metrics in Table~\ref{tab:resCocDepthExpt}, we conclude
that model C27 is our best captioning model on the MS-COCO validation set.
%% ---------------------------------------------------------------------------
\subsection{Comparison With State-of-the-Art}
\begin{table*}[htp]
  \newcommand{\mct}[1]{%
    \multicolumn{2}{c|}{\bf#1}}
  \centering
  \begin{adjustbox}{center}
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
    \hline\hline
    \multirow{2}{*}{\bf Leaderboard Name}
                       &\mct{BLEU-4} &\mct{METEOR} &\mct{ROUGE-L}&\mct{CIDEr}\\\cline{2-9}
                    & c5    & c40   &  c5   & c40   & c5  &  c40  &  c5  &  c40 \\\hline\hline
    AugmentCNNwithDet~(C19)& 0.315 & 0.597 & 0.251 &0.340& 0.531 &\bf0.683&\bf0.956&\bf0.968\\
    ------ (C27) & 0.310 & 0.596 & 0.250 &0.338& 0.529 & 0.681& 0.948& 0.961\\
    ATT\_VC~\cite{you2016image}& \bf0.316&0.599 & 0.250 &0.335&\bf0.535&0.682& 0.943& 0.958\\
    ------ (C17)  &  0.309 & 0.588 & 0.251 &0.342& 0.529 & 0.680& 0.943& 0.948\\
    OriolVinyals~\cite{Vinyals_2015_CVPR}      & 0.309 & 0.587 &\bf0.254&\bf0.346& 0.530 & 0.682& 0.943& 0.946\\
    MSR\_Captivator~\cite{Fang2015}  & 0.308 &\bf0.601& 0.248 &0.339& 0.526 & 0.680& 0.931& 0.937\\
    Berkeley LRCN~\cite{donahue2015long}   & 0.306 &\bf0.585& 0.247 &0.335& 0.528 & 0.678& 0.921& 0.934\\
    human~\cite{Chen2015}   & 0.217 & 0.471 & 0.252 &0.335& 0.484 & 0.626 & 0.854 & 0.910\\
    Montreal/Toronto~\cite{Xu2015show} & 0.277 & 0.537 & 0.241 &0.322& 0.516 & 0.654 & 0.865 & 0.893\\
    \hline \hline
  \end{tabular}
  \end{adjustbox}
  \caption{COCO 2014 test results. The scores are based on Microsoft
          COCO leaderboard. \emph{c5} and \emph{c40} indicate the number
  of reference captions used in evaluation. The models are sorted
  based on CIDEr score as in the leaderboard.}
  \label{tab:resCocLeaderTest}
\end{table*}

We compare the performance of our models with several state-of-the-art
models reported on the COCO 2014 captioning leaderboard and published results on
the validation set.
%%
For this purpose, we submitted captions form models C17, C19 and the CNN
ensemble model C27 to the CodaLab portal.
%%

We compare our results with ATT\_VC~\cite{you2016image},
MSR\_Captivator~\cite{Fang2015}, Berkeley LRCN~\cite{donahue2015long},
Montreal/Toronto~\cite{Xu2015show}, OriolVinyals~\cite{Vinyals_2015_CVPR}, and
human~\cite{Chen2015} reported in CodaLab. It is worth noting that the
OriolVinyals model shares the same architecture with our adopted baseline model
C1.
%%

Table~\ref{tab:resCocLeaderTest} reports the results of the benchmark%
\footnote{The leaderboard with more models and
scores is at \url{http://mscoco.org/dataset/\#captions-leaderboard}}.
%%
We compare our model with the other submissions within the top ten ranks in the
COCO leaderboard, as per METEOR metric, excluding the entries not associated
with any published work or report.
%%
%%
Note that each metric in Table~\ref{tab:resCocLeaderTest} has two columns, \emph{c5}
and \emph{c40}. 
%%
This is because COCO dataset contains 40 reference captions for a small subset
of images in the test set, referred to as \emph{c40}. 
%%
As already mentioned, using a larger number of reference captions makes the
metrics better correlated with human judgments and thus the metrics obtained on
the \emph{c40} are more reliable.
%%
The \emph{c5} metrics are obtained from the regular test set with only five
reference captions. 

We can see that the performances of our models drop slightly on the test set
compared to that on the validation set. 
%%
Also, while the ensemble model was the best model on the validation set, we
observe that C19 is a better model on the test set.
%%
This could be because some models in the ensemble do not generalize well to the
test set.
%%
Considering the overall performance of our model C19, we are outperforming
several state-of-the-art published results.
%%
More recently, there are three new entries on the leaderboard which are doing
better than our C19 model, but since they are not associated with any published
work, we refrain from discussing them here.

We should note here that the scores in CodaLab leaderboard do not necessarily
reflect the original published work results due to changes and updates.
%%
Thus, we also present a comparison with the published scores on the validation
set in Table~\ref{tab:resCocPubVal}.
%%
We see that our models outperform all published results on the validation set,
with the largest improvement seen in the CIDEr score.
%%
We could attribute most of this improvement to the rich set of image features we
use compared to other methods, most of which rely solely on CNN image features.
\begin{table*}[tp]
  \centering
  \begin{adjustbox}{center}
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline\hline
    \bf Model  &BLEU-4 &METEOR &ROUGE-L&CIDEr\\\hline
    C27 & \bf0.320&\bf0.254 &\bf0.536 &\bf0.978 \\
    C20 & 0.319 & 0.252 & 0.535 & 0.970 \\\hline
    ATT\_VC~\cite{you2016image} & 0.304& 0.243& -- & -- \\
    Berkeley LRCN~\cite{donahue2015long} & 0.300& 0.242& 0.524 & 0.896 \\
    OriolVinyals~\cite{Vinyals_2015_CVPR} & 0.277& 0.233& -- & 0.855 \\
    MSR\_Captivator~\cite{Fang2015} & 0.257& 0.236& -- & -- \\
    Montreal/Toronto~\cite{Xu2015show} & 0.250& 0.230& -- & -- \\
    \hline \hline
  \end{tabular}
  \end{adjustbox}
  \caption{Comparison of our models to the best published results on COCO 2014 validation set.}
  \label{tab:resCocPubVal}
\end{table*}


\subsection{Qualitative Examples and Discussion}
%------------------------------------
\begin{figure}[bth]
  \begin{center}
  \newcommand{\mcCell}[1]{%
          \multicolumn{1}{|c|}{#1}}
  \centering
  \begin{adjustbox}{center}
  %\tabcolsep=0.05cm
  \begin{tabular}{c|c|c|c|}
          \cline{2-4}
    &\includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000502766.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000161720.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000385707.jpg} \\\hline
    \mcCell{\textbf{\em\scriptsize C27:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a man and a dog herding sheep in a field\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a bathroom with a sink toilet and bathtub\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a bottle of wine and a glass of wine\smallskip}\\\hline
     \mcCell{\textbf{\em\scriptsize C19:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a man standing next to a herd of sheep\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a bathroom with a toilet and a sink\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize two bottles of wine sitting on a table\smallskip}\\\hline
     &\includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000251330.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000218404.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000119516.jpg} \\\hline
    \mcCell{\textbf{\em\scriptsize C27:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a view of a bridge in the snow\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a table with plates of food on it\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a person riding a bike down a city street\smallskip}\\\hline
     \mcCell{\textbf{\em\scriptsize C19:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a train crossing a bridge over a river\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a table topped with plates of food and drinks\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a city street filled with lots of traffic\smallskip}\\\hline
  \end{tabular}
  \end{adjustbox}
  \end{center}
  \vspace*{-5mm}
  \caption{Captions generated for some images from the COCO validation set by two of our
    models. The first row contains samples where the ensemble model,
    C27, performs better, and the second row cases where C19 is
    better.}
  \label{fig:cococapSamps}
\end{figure}
So far, we evaluated our models using the automatic metrics, but the metrics are
only approximative in judging the correctness of the captions.
%%
Figure~\ref{fig:cococapSamps} shows some sample captions generated by the
ensemble model C27 and the best single model C19 on images from the validation
set.
%%
We can see that the generated captions are often fairly accurate, but still
sometimes contain quite rudimentary errors.
%%
For the top left image, the CNN evaluator manages pick out a caption which
captures all the three objects in the image.
%%
In comparison, C19 only mentions ``man'' and ``sheep''.
%%
Similarly, in all the images in the top row, the CNN evaluator picks the caption
with more details than the one produced by C19.
%%
In contrast, the bottom row contains examples where the caption generated by C19
describes the image better than the one picked by C27.

%% 
One of the errors which still persists is that our models are not able to
precisely learn the relationships between the objects in an image.
%%
For example, our models find it hard to differentiate between a person riding a
bicycle versus just standing next to it, as seen in bottom right image in
Figure~\ref{fig:cococapSamps}.
%%
A similar pattern of error is seen in relationships between various object
types.

Another commonly occurring error happens in counting, wherein the captions tend
to get the number of objects wrong.
%%
We also see that some generated captions repeat the words referring to objects
instead of using numerals.
%%
Still one drawback is in the vocabulary size of the generated sentences.
%%
As already seen in Table~\ref{tab:resCocQual}, even the ensemble model uses only
1,303 words which is about $1/8^{th}$ of the words presented to the models
during the training.

%=================================================================================
%=================================================================================
\section{Video Captioning}
%=================================================================================
Next we will shift our attention to evaluating our video captioning models.
%%
As discussed before, our video captioning datasets are much smaller in terms of
training video--caption pairs compared to the COCO dataset.
%%
Thus we will focus our efforts here on identifying the best video features
suitable for the video captioning task, with majority of our experiments
conducted on the richer MSR-VTT dataset.
%%
While choosing the language model configuration for these experiments, we will
rely heavily on the insights obtained from the experiments on the COCO dataset.

We extract the \emph{gCNN} and \emph{SVM80} features only on the keyframe in the LSMDC
dataset and on one frame every second in the MSR-VTT dataset.
%%
The features extracted only from the keyframe will be indicated with the prefix
\emph{kf-} and features extracted from frames every second will be denoted by
the prefix \emph{ps-}.
%%
On the LSMDC dataset, we only use the dense trajectory features~(\emph{DT}) as
segment-level features.
%%
On MSR-VTT dataset, apart from \emph{DT} features we also experiment with
the improved dense trajectory features~(\emph{IDT}), and the features extracted from
the C3D network.

The evaluation of the video captioning models also rely on the same four
evaluation metrics and model perplexity measure on validation set as used for
image captioning. 
%%
On the test sets, both the LSMDC and the MSR-VTT challenge organizers provide
evaluation using these metrics and the human judgements, which we will present
here.

\subsection{Datasets}
First, we will discuss the LSMDC and MSR-VTT video captioning datasets in detail
here.
\subsubsection{LSMDC Dataset}
\label{subsec:LsmdcData}
The LSMDC dataset is the combination of two movie description datasets,
MPII-MD~\cite{rohrbach15cvpr} and M-VAD~\cite{AtorabiM-VAD2015}.
%%
Both the MPII-MD and M-VAD datasets contain clips extracted from movies and a
single reference caption describing the clip, but they differ in the source of
the reference caption.
%%
MPII-MD was collected from blue-ray movie clips and corresponding audio
description~(AD) and script data.
%%
The AD data was transcribed to text and manually aligned to the video clips.
%%
M-VAD was collected from DVD movie clips and only relies on AD data to annotate
the clips with a reference caption.
%%
In both datasets, any mention of people names in the collected reference
captions has been replaced with the token ``SOMEONE''.

Combining these two, LSMDC dataset has 118,081 video clips extracted from 202
unique movies and one reference caption for each clip.
%%
These clips are split into training set~(91,908), validation set~(6,542), public
test set~(10,053) and a blind test set~(9,578).
%%
The average length of these clips is just 4.8 seconds and the captions in
training set contain 22,829 unique words.
%%
This is filtered down to a vocabulary of 8,814 words, again by removing words
occurring less than five times.

We have to note here that the reference captions on the LSMDC dataset are
relatively noisier than the other two dataset we have used.
%%
They have problems due to mis-alignment of the captions with the video clips,
where the caption could be referring to objects which occur just before or after
the point the clip was cut from the movie. 
%%
Also, in some cases indiscriminate replacing of names with ``SOMEONE'' can lead
to very ambiguous captions.
%%
We can also see the effect of this in the poor performance of our models and
other state-of-the-art models on this dataset.

\subsubsection{MSR-VTT Dataset}
The MSR-VTT dataset~\cite{Xu:CVPR16} consists of 10,000 video clips with 20
human-annotated captions for each of them.
%%
Each video belongs to one of 20 categories including \emph{music},
\emph{gaming}, \emph{sports}, \emph{news}, etc.
%%
The dataset is split into training~(6,513 clips), validation~(497 clips) and
test~(2,990 clips) sets. 
%%
The training set videos are between 10 to 30 seconds long, with most of them
less than 20 seconds in duration.

The reference captions come from a vocabulary of $\sim$30k words, which is
reduced to a vocabulary of $\sim$8k after removing words occurring fewer than
five times.
%%
Although the number of video clips in the dataset is relatively small compared
to M-VAD~\cite{rohrbach15cvpr} and MPII-MD~\cite{AtorabiM-VAD2015}, the dataset
is attractive due to the diversity of its videos and due to the larger number of
reference captions per video.

By visually examining the training set, one can find many different video styles,
including slide shows with static images, recordings of computer monitor showing
users playing video games, movie trailers with lots of fast cuts, smooth single
shot videos involving activities such as cooking, news, interviews, etc.
%%
This means that the video captioning algorithm applied to this dataset needs to
handle all these diverse video styles.
%%
If one relies only on action recognition based features, the approach will suffer
with videos with lots of cuts and scene changes.
%%
On the other hand, if one relies only on frame-based features, the system will fail
to identify fine-grained differences in diverse action-oriented categories such
as sports or cooking.
%%
Consequently, in this thesis an ensemble based approach is taken to build a video
captioning system on this dataset.
%%
The solution consists of multiple captioning models, each trained on different
types of features, and a CNN-based evaluator network to pick the final candidate
caption.
%%
\subsection{LSMDC}
The keyframe used to extract \emph{kf-gCNN} and \emph{kf-SVM80} features is
sampled from the center of the video after the padding video clips to be of at
least two seconds long.
%%
In case of LSMDC dataset, it is reasonable to assume that the single keyframe is
quite representative of the video clips, as the clips are very short.
%%

\subsubsection{Results on Public Test Set}
\begin{table*}[t]
  \newcommand{\modpar}[4]{%
    \multirow{2}{*}{\emph{#1}} & \multirow{2}{*}{#2} & \multirow{2}{*}{#3}
    & \multirow{2}{*}{#4}}
  \newcommand{\bs}{\bf \small}
  \centering
  \begin{adjustbox}{center}
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
        \hline\hline
        \bs \#   &\bs init &\bs persist &\bs perplex&\bs Bleu\_4&\bs METEOR &\bs ROUGE\_L &\bs CIDEr  \\\hline\hline
        C4   & kf-SVM80 & kf-gCNN  &  --   & 0.003   &   0.053 &   0.114&   0.052 \\\hline
        L1   & kf-gCNN  & --    & 56.08 & 0.004   &   0.058 &   0.140&   0.071 \\
        L2   & kf-gCNN  & kf-SVM80 & 60.78 & 0.004   &\bf0.060 &   0.142&   0.073 \\
        L3   & kf-SVM80 & kf-gCNN  & 59.07 & 0.005   &   0.059 &   0.144&   0.087 \\\hline
        L4   & DT    & --    & 54.89 & 0.005   &   0.057 &   0.145&   0.087 \\
        L5   & DT    & kf-SVM80 & 59.75 & 0.005   &   0.057 &   0.141&   0.081 \\
        L6   & kf-SVM80 & DT    & 55.14 &\bf0.006 &   0.058 &\bf0.146&\bf0.092 \\
        L6*  & kf-SVM80 & DT    & 55.14 &   0.004 &   0.049 &   0.128 &   0.082 \\\hline 
    \end{tabular}
  \end{adjustbox}
    \caption{Results obtained on the public test set of LSMDC 2015. The model
    L6* is identical to L6 except that it uses beam size $b=5$. All the other
    models use beam size $b=1$}
    \label{tab:resLsmdcVal}
\end{table*}

To evaluate various forms of the video captioning models presented here,
the LSMDC 2015 public test set is used as the benchmark. 
%%
Table~\ref{tab:resLsmdcVal} shows the four evaluation metrics computed for different
models.
%%

In order to get a quick baseline, the C4 model trained on the COCO
dataset is used to generate captions on the LSMDC test set.
%%
This model was chosen as it is the best model using the \emph{gCNN} and
\emph{SVM80} features, which are compatible with keyframe-wise \emph{kf-gCNN}
and \emph{kf-SVM80} features.
%%
The captions generated from the C4 model are translated with a simple
rule-based translation to better match the LSMDC vocabulary.
%%
The translation is implemented using the simple %
$w_{\text{in}} \longrightarrow w_{\text{out}}$ rule:
%%
\begin{align} \label{eqTrans} w_{\text{out}} = \begin{cases} \text{SOMEONE},&
\text{if } w_{\text{in}} \in \{\text{man}, \text{woman}, \\&
\text{\mbox{\qquad\qquad person}}, \text{boy}, \text{girl} \}\\ w_{\text{in}},&
\text{otherwise.} \end{cases} \end{align}

As we can see in Table~\ref{tab:resLsmdcVal}, the performance of this COCO model
on the LSMDC dataset is quite bad.
%%
This is caused by two factors, first the vocabulary used in the LSMDC captions is
quite different from that of COCO and hence the automatic evaluation will rate the
COCO captions quite badly.
%%
Another factor is that motion related information is completely ignored in this
model.

Next, new models are trained using the reference captions in the LSMDC dataset and
keyframe features.
%%
Comparing models L1, L2 and L3 in Table~\ref{tab:resLsmdcVal}, we see that the
configuration with \emph{SVM80} features as \emph{init} input and \emph{gCNN}
features as \emph{persist} input performs the best.
%%
This also matches with the similar observation on COCO dataset.
%%
We can also see that these models clearly outperform the COCO baseline, mainly
due to the vocabulary update.

Finally, three more models are trained using the dense trajectory features and the
keyframe-based \emph{SVM80} features, presented in Table~\ref{tab:resLsmdcVal}
as models L4--L6. 
%%
Again we see that using the higher-dimensional feature, here the \emph{DT}
feature, as the \emph{persistent} input to the LSTM network gives the best
performance among this group of models.
%%
Comparing model L6 with L3 shows that using video features as opposed to just
keyframe features gives a better performance.
%%
We can see that model L6 benefits from combining both keyframe and trajectory
features as opposed to just using the trajectory features as in model L4.
%%
The result of model L6 can be regarded as the best one obtained in our LSMDC
experiments: it has the best scores in three out of four metrics.
%%
Therefore, the captions generated by the model L6 on the blind test set were
submitted to the LSMDC 2015 Challenge.

A rather surprising finding from the experiments on the LSMDC dataset is that,
using larger beam sizes in inference lead to poorer performance.
%%
This is seen from comparing models L6 and L6* in Table~\ref{tab:resLsmdcVal}.
%%
This is slightly counterintuitive, but can be understood when we look at the
lengths of the sentences produced by these two beam sizes. 
%%
For example, model L6 produces sentences with the average length of 5.33 words
with beam size $b=1$, while with beam size $b=5$ the average length drops to
just 3.79 words. 
%%
This is because with larger beam sizes the model always picks the most likely
sentence and penalizes heavily any word it is unsure of.
%%
This results in the model generating very generic sentences, such as
\emph{``SOMEONE looks at SOMEONE''}, over more descriptive ones.
%%
This trend of larger beam sizes performing worse is observed with all the LSMDC
models L1--L6, but they are not presented in Table~\ref{tab:resLsmdcVal} in the
interest of brevity.

\subsubsection{Results from the LSMDC 2015}
\label{subsec:LSMDCChall}
The submissions made to the LSMDC 2015 were evaluated using both the automatic
metrics and human judgements.
%%
However, only the human evaluation was used as the criteria to finally rank the teams.
%%
Human evaluations were collected by showing some human judges a video together
with five associated captions and asking them to rank the five captions, based
on four different criteria:
\begin{itemize}
  \item \emph{Correctness}: Content in the caption is more correct with the video. 
  \item \emph{Grammar}: Ranking the fluency and readability of the caption. 
  \item \emph{Relevance}: Which caption contains references to more salient items in the video.
  \item \emph{Helpfulness for the blind}: How helpful is the caption for a
          blind person to understand the scene.
\end{itemize}
The five captions consisted of one caption from each of the four submissions and
the reference caption for that video.
%%

Table~\ref{tab:resLsmdcTestMet} presents the automatic evaluation metrics on the
blind test set for the four LSMDC submissions.
%%
Our model L6 was ranked third among the four teams as per the automatic metrics.
%%
We should, however, note here that the evaluation metrics are particularly unreliable
on the LSMDC dataset.
%%
This is due to having only a single reference caption for the evaluation and also
the relatively poor match between the reference captions and the video content
as discussed in Section~\ref{subsec:LsmdcData}.
%%

This is illustrated when we look at the average ranking of the LSMDC submissions
as per the four human judgement criteria presented in
Table~\ref{tab:resLsmdcTestHum}.
%%
Based on human judgements, our submission won the LSMDC 2015 by obtaining
the best average ranking in three of the four criteria, and finished second in the
\emph{Helpfulness for the blind} criteria.
%%
Surprisingly, we also see that three out of the four models outperform the reference
captions on the \emph{Grammar} metric.
%%
Otherwise, there is still a big gap between the reference caption and the automatic
captioning models in the three semantic metrics.
%%
A more detailed discussion on the LSMDC results is presented
in~\cite{DBLP:journals/corr/RohrbachTRTPLCS16}.

\begin{table}[th]
  \centering
  \newcommand{\bs}{\small\bf}
  \begin{adjustbox}{center}
  \begin{tabular}{||c|c|c|c|c|}
    \hline\hline
    \bf Team  &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr \\\hline\hline
    Visual labels~\cite{rohrbach2015long} &\bf0.009&\bf0.071&\bf0.164&\bf0.112\\
    S2VT~\cite{venugopalan2015sequence} & 0.007 & 0.070 & 0.161 & 0.091\\
    \bf L6               & 0.006 & 0.061 & 0.156 & 0.090\\
    Temporal attention~\cite{yao2015describing} & 0.003 & 0.052 & 0.134 & 0.062\\\hline
    \hline
  \end{tabular}
  \end{adjustbox}
  \caption{LSMDC submissions ranked using automatic evaluation metrics.}
  \label{tab:resLsmdcTestMet}
\end{table}

\begin{table}[tbh]
  \centering
  \newcommand{\bs}{\small\bf}
  \begin{adjustbox}{center}
  \begin{tabular}{||c|c|c|c|c|}
    \hline\hline
    \bf Team  &\bs Correctness &\bs Grammar &\bs Relavance & \bf Helpful for blind\\\hline\hline
    Reference Caption    & 1.88  & 3.13  & 1.56  & 1.57\\\hline
    \bf L6               &\bf3.10&\bf2.70&\bf3.29&3.29\\
    Temporal attention~\cite{yao2015describing} & 3.14  & 2.71  & 3.31  & 3.36\\
    Visual labels~\cite{rohrbach2015long}& 3.32  & 3.37  & 3.32  &\bf3.26\\
    S2VT~\cite{venugopalan2015sequence}& 3.55  & 3.09  & 3.53  & 3.42\\
    \hline
  \end{tabular}
  \end{adjustbox}
  \caption{Human judgement scores for the LSMDC challenge submissions.}
  \label{tab:resLsmdcTestHum}
\end{table}

%=================================================================================
\subsection{MSR-VTT}
Similar to the LSMDC dataset, both frame-based and segment-based video
features are utilized in the video captioning models presented here for the MSR-VTT dataset.
%%
However, as the videos in the MSR-VTT dataset are much longer than the videos in
the LSMDC dataset, just using features from single keyframe is not sufficient.
%%
Therefore, \emph{ps-gCNN} features are extracted on one frame every second
in-place of the key-frame based \emph{kf-gCNN}.
%%
The \emph{ps-gCNN} features are then pooled using mean pooling.
%%

Apart from the dense trajectory video features, we also experiment with the
improved dense trajectory (IDT) and C3D video features on the MSR-VTT dataset.
%%
Additionally, we utilize the video category information available for all videos
in all splits of the dataset.
%%
This information is input to the language model as a one-hot vector of 20
dimensions and is referred to as \emph{20Categ}.

As in the case of LSMDC, we will discuss both the evaluation conducted locally
and the results from the video captioning competition conducted based on this
dataset, the Microsoft Video to Text Challenge.

\subsubsection{Results on Validation Set}
In order to measure the performance differences due to the different feature
combinations, we use the validation set of the MSR-VTT
dataset which contains 497 videos.
%%
Table~\ref{tab:resVttFeat} shows the results on the validation set.
%%

Models M1, M2 and M3 all use the dense trajectory~(DT) features as \emph{init}
input and the mean-pooled frame-level \emph{ps-gCNN}features concatenated with
the video category vector, \emph{20Categ}, as the \emph{persist} input, but they
vary in the number of layers in the language model.
%%
Comparing their performance we see that the 2-layered model outperforms the single
layered model by a small margin, while the 3-layered one is the inferior one.

Model M4 is similar to M2, but uses the improved dense trajectories (IDT) as
the \emph{init} input instead.
%%
Model M5 differs from M2 by the fact that it uses mean-pooled 3-D
convolutional features~(C3D) as the \emph{persist} input.
%%
We see that both M4 and M5 are competitive, but slightly worse than our best
single model, M2.
%%
Upon qualitatively analyzing the model outputs, we could see that each of them
performs well on different kinds of videos.
%%
For example, model M5, which only uses input features trained for action
recognition, does well in videos involving a lot of motion, but suffers in
recognizing the overall scenery of the video.
%%
Conversely, model M2 trained on frame-level features does better in recognizing
objects and scenes, but makes mistakes with the sequence of their appearance,
possibly due to the pooling operation.
%%
This phenomenon can also be observed in the second row of images in
Figure~\ref{fig:VttcapSamps}, where model M5 produces a better caption on the
video in the first column, while M2 does better on the video in the second
column.

To get maximum utility out of these diverse models, we use the CNN evaluator
network to pick the best candidate from the pool of captions generated by the
top four models in Table~\ref{tab:resVttFeat}, M1, M2, M4 and M5.
%%
The evaluator is trained using the \emph{ps-gCNN$\oplus$20Categ} as the video
feature.
%%
This result is shown as model M6 in Table~\ref{tab:resVttFeat}.
%%
We can see that model M6 using the CNN evaluator significantly outperforms, in
all the four metrics, every single model it picks its candidates from.

\begin{table*}[thp]
  \centering
  \begin{adjustbox}{center}
  \newcommand{\bs}{\small\bf}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \bs\# &\bs init &\bs persist &\bs depth &\bs perplex &\bs BLEU-4 &\bs METEOR &\bs CIDEr &\bs ROUGE-L \\\hline\hline
    M1 & DT  & ps-gCNN$\oplus$20Categ & 1  & 27.31 & 0.396 & 0.268 & 0.438 & 0.588 \\
    M2 & DT  & ps-gCNN$\oplus$20Categ & 2-res  & 27.73 & 0.409 & 0.268 & 0.433 & 0.598 \\
    M3 & DT  & ps-gCNN$\oplus$20Categ & 3-res  & 28.44 & 0.370 & 0.262 & 0.397 & 0.575 \\\hline
    M4 & IDT & ps-gCNN$\oplus$20Categ & 2-res  & 28.13 & 0.398 & 0.268 & 0.432 & 0.587 \\
    M5 & DT  & c3dfc7       & 2-res  & 29.58 & 0.369 & 0.268 & 0.413 & 0.577 \\\hline
    M6 & \multicolumn{4}{c|}{\em CNN ensemble of best 4 models}
                                  & \bf0.411 & \bf0.277 & \bf0.464 & \bf0.596 \\\hline
    \hline
  \end{tabular}
  \end{adjustbox}
  \caption{Performance of various features and network depths on the validation
  set of MSR-VTT.}
  \label{tab:resVttFeat}
\end{table*}

%% ---------------------------------------------------------------------------

\subsubsection{Results from the MSR-VTT Challenge}

Since the CNN evaluator model, M6, performed the best on the validation set, we
submitted captions from M6 to the MSR-VTT Challenge.
%%
Our submission appears on the
leaderboard\footnote{\url{http://ms-multimedia-challenge.com/leaderboard}} as
\emph{Aalto}.
%%
The submissions were evaluated on the blind test set using the four automatic
metrics.
%%
These results are shown in Table~\ref{tab:resultsTestMet}.
%%
Our submission achieved the best scores in the CIDEr metric and was ranked
overall second considering the average ranking across the metrics.

The submissions were also subject to human evaluation as the automatic metrics
are known to deviate from human judgements as discussed in
Section~\ref{subsec:autMetRel}.
%%
The human evaluation was based on three criteria: Coherence (C1), Relevance (C2)
and Helpfulness for the blind (C3).
%%
But unlike in case of LSMDC, these are collected as scores between zero and five
with the latter being the better.
%%
Table~\ref{tab:resultsTestHum} presents the results of the human evaluation.
%%
The overall ranking was obtained again by considering the mean ranking across
the three metrics.
%%
As per human judgement, our submission was ranked the first among the 22 entries
in the challenge and thus won the challenge in this category.

Analyzing the two leaderboards, the automatic metric based one and the human
evaluation based one, we see that the disagreement between the two is relatively
minor, with most teams in the top 10 changing their ranking by only one
position.
%%
This can most likely be attributed to having a large number of 20 reference
captions per video for the evaluation.

\begin{table}[th]
  \centering
  \newcommand{\bs}{\small\bf}
  \scalebox{0.9}{
  \begin{tabular}{||c|c|c|c|c|}
    \hline\hline
    \bf Team  &\bs BLEU-4 &\bs METEOR &\bs CIDEr &\bs ROUGE-L \\\hline\hline
    v2t\_navigator &\bf0.408 &\bf0.282 & 0.448 &\bf0.609 \\
    \bf Aalto~(M6)     & 0.398 & 0.269 &0.457 & 0.598 \\
    VideoLAB       & 0.391 & 0.277 & 0.441 & 0.606 \\
    ruc-uva        & 0.387 & 0.269 &\bf0.459 & 0.587 \\
    Fudan-ILC      & 0.387 & 0.268 & 0.419 & 0.595 \\\hline
    \hline
  \end{tabular}}
  \caption{Top 5 teams as per automatic evaluation metrics on the MSR-VTT test set.}
  \label{tab:resultsTestMet}
\end{table}

\begin{table}[th]
  \centering
  \newcommand{\bs}{\small\bf}
  \begin{tabular}{||c|c|c|c|}
    \hline\hline
    \bf Team  &\bs C1 &\bs C2 &\bs C3 \\\hline\hline
    \bf Aalto~(M6)     & \bf3.263 & 3.104 & \bf3.244\\
    v2t\_navigator & 3.261 & 3.091 & 3.154 \\
    VideoLAB       & 3.237 & \bf3.109 & 3.143 \\
    Fudan-ILC      & 3.185 & 2.999 & 2.979 \\
    ruc-uva        & 3.225 & 2.997 & 2.933 \\\hline
    \hline
  \end{tabular}
  \caption{Top 5 teams as per human evaluation.}
  \label{tab:resultsTestHum}
\end{table}

%% ===========================================================================

\subsection{Qualitative Analysis of Video Captioning}
%------------------------------------
The kind of video captions generated by the models trained on the LSMDC dataset
differ from the captions generated on the MSR-VTT data due to the difference
in the language used in the reference captions.
%%
Figure~\ref{fig:VttcapSamps} shows some examples~(screenshot from the
videos) from these two datasets and captions generated by the best models
trained on them.
%%
The top row contains videos from the LSMDC dataset, while the bottom row shows
samples from the MSR-VTT dataset. 
%%

The first two images on the top row show examples of accurate captions
generated by the L6 model.
%%
However, in the video on the right, the model gets confused by the last shot
showing a man, and completely ignores the car driving away.
%%
This kind of behaviour can be seen in many error cases, where the model
ignores the main subject of the video and generates the caption based on an
object it knows well, e.g.\@ ``man'' in this case, even if it is insignificant
in the video.

In the examples from the MSR-VTT dataset shown in the second row of
Figure~\ref{fig:VttcapSamps}, we see three distinct cases.
%%
The example on the left shows the case where model M5, based on the C3D
features, produces better caption than model M2, which uses frame-level
features.
%%
The example in the center shows the case where this is reversed and the
frame-level features do better than the action-recognition-based C3D features.
%%
Finally, the rightmost example shows a case where the CNN evaluator, M6, picks
a better caption than the ones generated by both M2, our best single model, and
M5.

\begin{figure}[thp]
  \begin{center}
  \newcommand{\mcCell}[1]{%
  \multicolumn{1}{c}{#1}}
  \centering
  \begin{adjustbox}{center}
  \tabcolsep=0.10cm
  \begin{tabular}{lll}
    \mcCell{\includegraphics[width=0.25\linewidth]{images/LsmdcVid0.png}} &
    \mcCell{\includegraphics[width=0.25\linewidth]{images/LsmdcVid1.png}} &
    \mcCell{\includegraphics[width=0.25\linewidth]{images/LsmdcVid2.png}}
    \vspace{-2mm}\\
    \textbf{\scriptsize\em L6:} \scriptsize someone runs up to the car&
    \textbf{\scriptsize\em L6:} \scriptsize someone is dancing with her&
    \textbf{\scriptsize\em L6:} \scriptsize someone looks up at the house\medskip\\
    \mcCell{\includegraphics[width=0.25\linewidth]{images/9150.png}} &
    \mcCell{\includegraphics[width=0.25\linewidth]{images/9799.png}} &
    \mcCell{\includegraphics[width=0.25\linewidth]{images/7997.png}}\vspace{-2mm} \\
    \textbf{\scriptsize\em M6:} \scriptsize a man is running in a gym &
    \textbf{\scriptsize\em M6:} \scriptsize a person is playing with a rubix cube &
    \textbf{\scriptsize\em M6:} \scriptsize cartoon characters are interacting\\
    \textbf{\scriptsize\em M2:} \scriptsize a man is running&
    \textbf{\scriptsize\em M2:} \scriptsize a man is holding a phone&
    \textbf{\scriptsize\em M2:} \scriptsize a person is playing a video game\\
    \textbf{\scriptsize\em M5:} \scriptsize a man is playing basketball&
    \textbf{\scriptsize\em M5:} \scriptsize a person is playing with a rubix cube &
    \textbf{\scriptsize\em M5:} \scriptsize a group of people are talking\\
  \end{tabular}
  \end{adjustbox}
  \end{center}
  \vspace{-5mm}
  \caption{Sample captions generated for some test set videos by our models.
  First row contains captions from model L6 on the LSMDC public test set and the
  second row contains captions from a few of our best models on samples from
  MSR-VTT test set.}
  \label{fig:VttcapSamps}
\end{figure}

\begin{figure}[ht]
\begin{center}
  \begin{adjustbox}{center}
  \begin{subfigure}[c]{0.65\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{images/VTTCiderCateg.pdf}%
    \caption{Performance by category}%
    \label{fig:VttCategPerf}
  \end{subfigure}%
  \begin{subfigure}[c]{0.65\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{images/VTTCiderLengths.pdf}
    \caption{Performance by length}
    \label{fig:VttLenPerf}
  \end{subfigure}
  \end{adjustbox}
\end{center}
\vspace*{-5mm}
\caption{Performance of model M6 as per CIDEr metric compared for
        various video lengths and sub-categories of the MSR-VTT validation set.}
\label{fig:VttPerf}
\end{figure}

Next we will try to understand if the video captioning performance is affected by
video lengths or by video categories.
%%
For this we use the MSR-VTT dataset as it has longer videos and also provides
the category tag for each video.
%%
Figure~\ref{fig:VttPerf} shows the results from this analysis.
%%
Here we first evaluate the captions generated by model M6 for each of the 497
videos in the validation set of the MSR-VTT dataset.
%%
Then the CIDEr scores for videos belonging to the same category are aggregated
and plotted to create Figure~\ref{fig:VttCategPerf}.
%%
Similarly, CIDEr scores for the videos with the same length in seconds (after
rounding to nearest integer) is aggregated to produce
Figure~\ref{fig:VttLenPerf}.
%% ---------------------------------------------------------------------------

From Figure~\ref{fig:VttCategPerf} we see that the captioning performance varies
greatly across categories.
%%
The model seems to perform very well on categories such as \emph{how to},
\emph{gaming},
\emph{sports} and \emph{vehicles}.
%%
In the case of the \emph{how to} category, this can be attributed to the relatively
simplistic nature of the videos.
%%
The \emph{how to} videos generally consist of a close-up of a person performing
certain distinctive actions, and are thus probably simpler to caption using the
action recognition features. 
%%
A similar structure exists in \emph{gaming} videos, which mostly contain screen
captures of people playing video games.
%%
We see that most of the categories where captioning does well have a single
visual theme and distinctive category of actions associated with them. 
%%
In contrast, the categories where the captioning does poorly, e.g.\@ \emph{tv},
\emph{news}, \emph{education}, etc.\@, are unified conceptually and not necessarily
visually.
%%
For example, the \emph{news} category can contain a variety of scenes, both
indoor and outdoor, and a variety of actions. 
%%
These videos also tend to have many sharp scene changes, thus causing the action
recognition features to perform poorly on them.

Interestingly, the length of the video seems to have only a small correlation with
the performance as seen in Figure~\ref{fig:VttLenPerf}.
%%
Very long videos ($>20$ seconds) seem to perform only slightly worse than the
shorter ones.
%%
We should also note here that the estimates of performance of the longer videos
are noisier, as indicated by the error bars in figure~\ref{fig:VttLenPerf}, as
there are fewer videos falling in these categories. 
%%
The degradation in longer videos can be expected as the video features we use,
both action recognition and frame-based features, do not account for long-term
temporal dependencies.
%%
Nevertheless, it is still surprising that the degradation is relatively minor.

\section{Summary of Results and Conclusions}
In this chapter, results from the image captioning experiments on the MS-COCO
dataset and video captioning experiments on the LSMDC and MSR-VTT datasets were
presented.
%%
Now we will summarize these results and highlight the important findings from
these experiments.

From the experiments on the MS-COCO dataset, we saw that the proposal to enhance
the CNN based \emph{gCNN} image features with explicit object and scene detector
features greatly improved the performance. 
%%
Counterintuitively, we observe that the use of object location features does
not necessarily add to the model: only the smallest of these features,
\emph{3+3Gauss}, moderately improved the performance by automatic metrics when
used in conjunction with detector features.

Two proposed enhancements to the language model, namely using two separate
feature input channels and increasing the depth with residual connections,
contribute positively.
%%
Using two feature input channels helped utilize more than one image feature and
greatly improved the language model performance.
%%
Increasing the language model depth moderately improved the performance as per
metrics, while it considerably improved the vocabulary and diversity of the
captions generated.
%%
We also found that the idea of using residual connections between two layers
works well even when used with LSTM networks.
%%
Using residual connections improved the training convergence speeds of our
language model and achieved a lower perplexity measure.
%%
However, the attempts to implement a hierarchical decoder in the language model,
despite greatly improving the generated caption diversity, affected adversely 
the correctness of the generated captions and failed to match up to the model
with a single stage decoder.
%%

Among the proposed ensembling techniques, CNN-based evaluator network achieved
the best performance.
%%
In image captioning, while it slightly improved in automatic metrics over the
best single model in the ensemble, it contributed positively by increasing the
language diversity. 
%%
Furthermore in the case of video captioning on the diverse MSR-VTT dataset, we
found that this ensembling technique clearly outperformed all the models
participating in the ensemble and was our best model.

From video captioning experiments, we learned that a combination of action-
recognition based and frame-based video features works well for the captioning
task.
%%
The performance of the video captioning systems is still relatively poorer than
that of the image captioning models.
%%
Nevertheless, the models presented here for video caption generation can be
considered as the state-of-the-art for this task.
%%
This is supported by the results from our participation in the LSMDC and MSR-VTT
video captioning challenges, both of which we won, as judged by human
evaluators.

