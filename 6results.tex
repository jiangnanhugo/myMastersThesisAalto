\chapter{Experiments and Results}
\label{chapter:results}

So far, we have discussed several feature extraction methods and alternate
language model architectures for automatic caption generation.
%%
Now, it is time to put these proposals to test  and evaluate how well they
perform on the captioning task.
%%
In this chapter we will present the results from the experiments on three
separate datasets, one for image captioning (MS-COCO) and two for video
captioning~(LSMDC and MSR-VTT).
%%
We will rely on the four evaluation metrics described in
section~\ref{sec:EvaluationMetrics}, namely BLEU, METEOR, ROUGE-L and CIDEr, to
evaluate the performance of our models on the captioning task.
%%
Additionally, on validation sets, we also compute the models' perplexity of the ground
truth captions and use this as a fifth metric of performance. 

But as already discussed, these evaluation metrics only approximately track the
human judgements of how good the generated captions are.
%%
To meticulously evaluate the captioning models, we need to collect human
judgements on the captions generated by them.
%%
However, it is a costly exercise to collect human evaluations for every model we
wish to compare.
%%
Luckily enough, human judgements were collected and used to evaluate such
systems in the captioning challenges held over the course of last year.
%%
We will present results from our participation in two video captioning
challenges, and present human evaluations for few of our best models, obtained
from these competitions.

Since the COCO dataset is the largest of the three datasets we have used, we
conduct all the language model experiments on this dataset.
%%
This means the performance analysis of our proposed language model extensions
namely, addition of persist features, increasing depth with residual connections
and class based factorization of output is conducted on this dataset.
%%
Also, since the language model we use in video captioning has the same
architecture as the one used in image captioning, we assume the same results
hold there too.
%%
Thus we limit our experiments in video captioning task to testing different
video features.

\section{Implementation details}
%%
Before getting into evaluation, we will first briefly discuss some
implementation platform details, and some hyper-parameter choices.
%%
\paragraph*{Language Model :} We implement the proposed LSTM language model used
for both image and video captioning using the Theano
library~\cite{Bastien-Theano-2012}.
%%
Using Theano allows us to easily run the computation on a GPU, leading to
much quicker training and testing cycles.
%%
The language models are trained using stochastic gradient descent with the
RMSProp~\cite{rmspropTielman} algorithm and we have implemented regularization
using dropout as described in \cite{ZarembaSV14}.
%%
The error is back-propagated to all the language model parameters and word
embedding matrices, but the image feature extraction models are kept constant.
%

We train the language model by minimizing the log-likelihood assigned by the
model to training samples.
%%
But, as noted before, evaluation of the trained model is done using the automatic
evaluation metrics.
%%
This discrepancy between the training objective and the evaluation cost is
forced upon us as the automatic metrics themselves are not differentiable and it
would be hard to train a model directly to optimize these metrics.
%%
Still, we can justify using the perplexity as the training cost, by empirically
observing that perplexity measure roughly tracks the automatic metrics.
%%
Concretely, optimizing the model parameters to minimize perplexity also improves the
model performance as per automatic metrics.
%%
To verify this, we did a quick experiment where we compute the perplexity
measure on validation and training set after every epoch of training.
%%
We also generate captions on the validation set using the intermediate models
after every epoch and compute the automatic metrics.
%%
Results from this is plotted in figure~\ref{fig:MetVsPerplex}.
%%
We see that perplexity indeed tracks the performance on evaluation metrics very
well.

In all our experiments we use beam-search to generate sentences from a trained
model.
%%
After experimenting with different beam sizes, we found that beam size of $b=5$
works well across all our models on the COCO and the MSR-VTT datasets, whereas
$b=1$ worked better on the LSMDC dataset.

%------------------------------------
\begin{figure*}[ht]
\begin{center}
  \includegraphics[width=0.7\linewidth]{images/MetVsPerplex.pdf}
\end{center}
\vspace*{-10mm}
\caption{Evolution of training and validation perplexity and automatic metrics
        computed on validation set, during training.}
\label{fig:MetVsPerplex}
\end{figure*}
%% ---------------------------------------------------------------------------
\paragraph*{Visual Feature extraction : }%
%%
The CNN feature extraction and the Faster R-CNN models are based on the Caffe
library~\cite{jia2014caffe}.
%%
Using the code released for the Faster R-CNN
network\footnote{\url{https://github.com/rbgirshick/py-faster-rcnn}}, we train it on
the MS-COCO dataset to detect the 80 object categories annotated in COCO.
%%
Then, this detector is run on the entire COCO dataset and the spatial map feature
vectors are created using the bounding boxes output by the detectors.
%%

Dense trajectory feature extraction on videos was done using the source code
provided by the authors in~\cite{DBLP:conf/cvpr/WangKSL11}.
%%
To extract C3D features, we used the code and the model pre-trained on the
Sports-1M dataset provided by the authors of~\cite{DBLP:C3D}.

\paragraph*{CNN Evaluator:} We implement our proposed CNN evaluator also using
the Theano library. 
%%
It is created with bi-, tri-, 4-, and 5-gram filters and $N_{\text{filt}}=100$
filters of each type.
%%
The word vectors used in the evaluator are chosen to have $N_{\text{word vec
dim}}=100$ dimensions.
%%
In order to train the CNN evaluator, each ground truth caption-image/video pair
needs $k$ negative captions aswell.
%%
In order to optimize the training process, we sample $k+1$ random image caption
pairs and treat the other $k$ captions as negative samples for each image.
%%
This is much faster than sampling separate negative captions for each
image-caption pair, and improved the overall training speed.
%%
In our experiments, we found setting $k=49$ achieved good results.

\section{Image Captioning}

In this section we report results of our experiments on image captioning
conducted on the MS-COCO dataset.
%%
In the first subsection we will discuss the MS-COCO dataset in detail.
%%
Next we report the results of our internal evaluation on the validation set
comparing various models and features.
%%
This is followed by results from the test set obtained from the Microsoft
CodaLab portal and a comparison to some best published work on this dataset.

\subsection{MS-COCO dataset}
In all our experiments we use the Microsoft COCO 2014 data set~\cite{Lin2014}
for training and evaluation.
%%
This data set consists of 164,062 images split into training set of 82,783
images, validation set of 40,504 images and test set of 40,775 images. 
%%
An additional 40k test images were released in the 2015 version of the dataset,
but we have not used it in our experiments.
%%
The training and validation sets have five reference captions for each image
annotated by humans. 
%%
The total 413915 reference captions from the training set have 23,528 unique
words.

COCO dataset also has object segmentations available for each image and for
objects belonging to 80 specified categories.
%%
The categories consist of common object types such as \emph{person}, \emph{car},
\emph{bus}, \emph{skateboard}, etc.
%%
We make use of the object segmentations when training the Faster Region-based
Convolutional Neural Networks for extracting the object location features, as
described in Section~\ref{sec:frcnnfeat}.
%%
The segmentation information is, however, not utilized when training the LSTM
language model nor when using it with the validation and test images.

Before using the reference captions in the COCO data set for training, we
tokenize the text and remove symbols and numerals.
%%
Words occurring less than 5 times are also removed in oder to weed out spelling
mistakes and extremely rare words, for which we have insufficient data to learn.
%%
This leaves us with a training corpus vocabulary consisting of 8790 words. 
%%===========================================================================%%

To evaluate the utility of the proposed set of image features and LSTM network
architectures, we use the COCO 2014 validation set and the five reference
sentences available for all images in it.
%%
Microsoft COCO team has also made an evaluation server available on
CodaLab\footnote{\url{https://competitions.codalab.org/competitions/3221}} where
researchers can upload their captions for the test set and view the resulting
evaluation metrics.
%%
We use this portal to evaluate our models on the COCO Image Captioning Challenge
2014 test set. 
%%
Here, we also compare our performance against other well-performing or
state-of-the-art entries in the CodaLab leaderboard.

\subsection{Results on Validation Set}
In Chapters~\ref{chapter:VisFeatChapter} \& ~\ref{chapter:langModel} we proposed
several image features and language model extensions respectively to improve the
captioning system over the baseline model.
%%
In order to obtain the best possible captioning system for the COCO dataset, we
need to measure the performance of these different features and language model
combinations.
%%
Since training a model for every combination of feature and choice of language
model parameters is prohibitively expensive, we run separate experiments to
determine the best features and the best language model choices, while keeping
the other fixed.
%%
We observe that these two aspects are fairly independent and choosing the best
feature and the best language model independently  gives us our best captioning
model.
%%
%--------------------------------------------------------------------------------------
\subsubsection{Evaluating the Init and Persist Paths}
\label{subsubsec:InitVpersist}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c||c|c||c|c|c|c|c|}
    \hline
    \bf Model & \multicolumn{2}{c||}{\bf Features 
    } & \multicolumn{5}{c|}{\bf Performance metrics}\\
     \cline{2-8}
    \bf \# & init & persist &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C1 & gCNN  & ---  & 0.289 & 0.238 & 0.514 & 0.860 & 10.43  \\
    C2 & gCNN  & SVM80& 0.292 & 0.239 & 0.516 & 0.871 & 10.34  \\
    C3 & gCNN  & gCNN &\bf 0.302 & 0.243 &\bf 0.523 & 0.897 & 10.25  \\
    C4 & SVM80 & gCNN &\bf 0.302 &\bf0.244 &\bf 0.523 &\bf0.909 & 10.30  \\
    C5 & SVM80 & SVM80& 0.261 & 0.225 & 0.492 & 0.785 & 10.78 \\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{ Evaluating the utility of the init and persist input channels to the
          LSTM language model}
  \label{tab:resCocInitVPers}
\end{table*}

In Chapter~\ref{chapter:langModel} we introduced a new input to the LSTM language
model, the \emph{persist} path, positing that it is beneficial for the language
model to have access to the visual features throughout the caption generation
process.
%%
This new input also enables us to provide two different features as input to the
language model.
%%
Table~\ref{tab:resCocInitVPers} presents the results from experiments trying to
determine the best use for the two input channels, \emph{init} and
\emph{persist}.
%%
The columns \emph{init} and \emph{persist} indicate what visual features were
used as the initializing and persistent inputs to the language model
respectively.
%%
In these experiments, our language model has only a singel layer of LSTM cells
with both the word-embeddings and LSTM layer being of 512 dimensions.

Our baseline model, C1, only uses the \emph{gCNN} (from GoogleLeNet) image
features as the \emph{init} input.
%%
Compared to this, additionally providing the 80 dimensional detector features,
\emph{SVM80},
using the \emph{persist} channel improves the performance slightly, in model C2.
%%
Instead, if we provide the \emph{gCNN} features to both the inputs, as in model C3,
there is dramatic improvement in the performance in all the four metrics, as
well as in validation perplexity.
%%
This tells us that it is beneficial for the language model to have access to the
CNN features throughout the caption generation process.
%%

Instead of redundantly using the same \emph{gCNN} features in both \emph{init} and
\emph{persist} paths, we can now replace the \emph{init} feature with the \emph{SVM80}
feature to get a marginal performance improvement as seen in model C4.
%%
Model C5 tells us that using only \emph{SVM80} features is not good and CNN
features need to be presented in the \emph{persist} to get the best performance. 
%%

%--------------------------------------------------------------------------------------
\subsubsection{Finding the best Image features}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \bf Model & \bf \multirow{2}{*}{Init feature} & \multicolumn{5}{c|}{\bf Performance metrics}\\
    \cline{3-7}
    \bf \# &\bf &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C4 & SVM80               & 0.302 & 0.244 & 0.523 & 0.909 & 10.30  \\
    C6 & FRC80               & 0.316 & 0.249 &\bf0.534 & 0.952 & 10.15  \\
    C7 & SUN397              & 0.301 & 0.241 & 0.521 & 0.894 & 10.40  \\
    C8 & SUN397$\oplus$FRC80 & 0.315 &\bf0.250 & 0.532 &0.954 &10.05  \\\hline
    C9 & 4$\times$4IoU       & 0.302 & 0.244 & 0.522 & 0.913 & 10.21  \\
    C10 & 4$\times$4Gauss    & 0.308 & 0.246 & 0.527 & 0.921 & 10.15  \\
    C11 & 3+3Gauss           & 0.308 & 0.247 & 0.527 & 0.928 & 10.08  \\\hline
    C12 &\parbox[c][][c]{4cm}{\smallskip\centering 3+3Gauss$\oplus$SUN397\\$\oplus$FRC80\smallskip} 
                             &\bf0.318&\bf0.250&0.533 &\bf0.957&\bf9.93\\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{ Evaluating the efficacy various image feautures using fixed language model
          configuration}
  \label{tab:resCocFeatExpt}
\end{table*}

Next we report from our experiments to determine the best image features to pair
with the CNN features.
%%
Using the results from previous subsection as guideline, we keep the gCNN
features fixed as \emph{persist} input in the following experiments, and only change the
feature input in the \emph{init} channel.
%%
The language model parameters also remain the same as
subsection~\ref{subsubsec:InitVpersist}.
%%
Table~\ref{tab:resCocFeatExpt} presents the results from these experiments.
%%
Note that, here we use the ``$\oplus$'' symbol to denote the vector
concatenation operation.

Comparing the results of models C4 and C6, we see that the \emph{FRC80} features
outperforms the SVM80 features with a specially significant gain in the CIDEr
metric.
%%
The Faster R-CNN based object features thus seem to overcome the simpler SVM
detector output based features.
%%
This also supports our hypothesis that good explicit object detectors can
effectively compliment the CNN image features. 
%%
The object detectors are trained to detect multiple objects explicitly, and
although they don't encode any information about the object shape or other
attributes, just the information about probability of occurence of different
objects seems very beneficial to the captioning task.

Using the scene detection features, \emph{SUN397}, alone as \emph{init} input in
model C7 worsens the performance.
%%
But augmenting the \emph{FRC80} object features with scene information by
concatenating \emph{SUN397} features as shown in model C8 improves the
performance over C6 in 3 metrics.

Next we compare the spatial grid features using models C9, C10 and C11.
%%
We find that using the integral of Gaussian performs better than using the
intersection-over-union (IoU) measure when constructing these features as seen
by comparing C9 and C10. 
%%
In general, however, the spatial grid features do not match the performance of
the \emph{FRC80} features, even though \emph{FRC80} only encodes a subset of the
information represented in the spatial grid features.
%%
This could be due to the fact that the spatial grid features are of much higher
dimension than the \emph{FRC80} feature vectors.
%%
This hypothesis is also strengthened by observing that model C11, which uses
smaller \emph{3+3Gauss} features, performs the best among the models using the
spatial grid features.

%%
Next we train model C12 with concatenating the \emph{FRC80} , \emph{SUN397}  and
\emph{3+3Gauss}. 
This model now has access to object detection , scene type and object location
information apart from the CNN features and is our best performing model with
this language model configuration.

\subsubsection{How deep should we go?}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \bf Model & \bf \multirow{2}{*}{Depth} & \multicolumn{5}{c|}{\bf Performance metrics}\\
    \cline{3-7}
    \bf \# &\bf &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C8  & 1   & 0.315 & 0.250 & 0.532 & 0.954 &10.05  \\\hline
    C13 & 2   & 0.318 & 0.252 & 0.535 &\bf0.967 & 10.14  \\
    C14 & 3   & 0.316 & 0.253 & 0.533 & 0.964   & 10.34  \\
    C15 & 4   & 0.316 & 0.250 & 0.533 & 0.956 & 10.69  \\\hline
    C16 &2-res&\bf0.320& 0.253 &\bf0.536&0.966  & 9.92   \\
    C17 &3-res& 0.316 &\bf0.254&0.532 & 0.962   &\bf9.69 \\
    C18 &4-res& 0.316 & 0.253 & 0.535 & 0.964   & 9.75 \\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Results from experiments with language model depth, with fixed input features}
  \label{tab:resCocDepthExpt}
\end{table*}

Table~\ref{tab:resCocDepthExpt} presents the results from experiments with depth
of the LSTM language model.
%%
Column \emph{depth} specifies the number $N$ of LSTM layers in the model,
with $N$-res being an LSTM network with $N$ layers and residual connections.
%%
For these experiments, LSTM layer size and word encoding size is still held at
512 dimensions, but only the number of LSTM layers is changed.
%%
The \emph{SUN397$\oplus$FRC80} features are used a \emph{init} input and
\emph{gCNN} features are used as \emph{persist} input.

When we increase the number of layers without adding residual connections, we
see that that perplexity metric worsens, although there is moderate improvement
in other metrics upto a depth of 3 layers.
%%
This is seen comparing model C8 with models C13-C15.
%%
But, when we increase the depth to 4 layers, we see that it performs similar to
single layer model in automatic evaluation metrics with perplexity being
significantly worse.
%%

Adding the residual connections significantly improves the perplexity of
validation set while the performance on the metrics improves slightly, with the
biggest gain seen in CIDEr metric.
%%
We also found that residual connections improve the training convergence speed.
%%
This is illustrated in figure~\ref{fig:MetVsPerplex}, where we show the
progression of perpelxity measure during training of the 4-layered models
with~(C18) and without~(C15) residual connections.
%%
We see that performance gain seem to saturate by 4 layers even with residual
connections.
%%
So the choice for the best architecture is between 2 or 3 layered model with
residual connections, considering all the 5 metrics.
%%
But since 3 layered model, C17, produces more diverse captions, as seen in
subsection~\ref{subsubsec:QualAnalCoc} we choose this configuration for further
experiments.
%------------------------------------
\begin{figure*}[t]
\begin{center}
  \includegraphics[width=0.7\linewidth]{images/ResidualVsRegPerplex.pdf}
\end{center}
\vspace*{-10mm}
\caption{Comparing the evolution of perplexity during training of a 4-layered
        model with and without residual connections.}
\label{fig:MetVsPerplex}
\end{figure*}
%------------------------------------

\subsubsection{Class based factorization}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    \bf Model & \bf \multirow{2}{*}{Class clustering} & \multicolumn{5}{c|}{\bf Performance metrics}\\
    \cline{3-7}
    \bf \# &\bf &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr&\bs Pplx \\\hline
    C19 & --    & 0.319 & 0.252 & 0.535 & 0.970 & 9.72 \\\hline
    C20-cls &\parbox[c][][c]{4cm}{\smallskip\centering K-Means, 200 Class \smallskip} 
                             & 0.286 & 0.245 & 0.523 & 0.906 & 10.10 \\\hline
    C21-cls &\parbox[c][][c]{4cm}{\smallskip\centering Brown, 200 Class\smallskip} 
                             & 0.286 & 0.245 & 0.523 & 0.906 & 10.10 \\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Results from models using hierarchical decoder}
  \label{tab:resClsCocValset}
\end{table*}

Previously, we determined that the 3 layered model with residual connections,
C17, to be the best language model configuration.
%%
We further upgrade this model by changing the init feature to
\emph{3+3Gauss$\oplus$SUN397$\oplus$FRC80} in model C19 shown in
table~\ref{tab:resClsCocValset}.
%%
This slightly improves the performance, with model C19 improving over model C17 in
three measures. 
%%
We use this configuration as the basis to run our experiments with the
Hierarchical decoder presented in section~\ref{sec:ClassFact}.
%%
These results are also presented in table~\ref{tab:resClsCocValset}.
%%
All the models presented in table~\ref{tab:resClsCocValset} here use the same
\emph{init} and \emph{persist} features and have three layers connected with
residual connections.

Model C20-cls is trained with the hierarchical decoder which in which the words
are clustered to classes using the K-means method. 
%%
For this, we run k-means on word vectors learned by model C19, with k=200 to
obtain 200 classes. 
%%
Alternatively, C21-cls uses class assignments obtained from the Brown clustering
algorithm run on the training corpus, again with 200 classes.

We see that both these models perform similarly, with C21-cls only slightly
better than C20-cls in CIDEr metric and perplexity.
%%
More interestingly, both the models are considerably worse than the
corresponding simple decoder model, C19, in all the performance metrics.
%%
On manual inspection of the captions produced by them, we found that both
C20-cls and C21-cls make significantly more grammatical errors, with often
missing conjunctions and stop words.
%%
But on the bright side, the models using hierarchical decoder generate
significantly more diverse and descriptive captions as seen in
section~\ref{subsubsec:QualAnalCoc}.
%%
Thus, it seems that hierarchical decoder indeed makes the captions richer, but
it also adversely affects the correctness of the captions generated.

\subsubsection{Ensembling}
\begin{table*}[htp]
  \centering
  \newcommand{\bs}{\small}
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \bf Model & \bf \multirow{2}{*}{Method} & \bf \multirow{2}{*}{Evaluators}&
    \bf \multirow{2}{*}{Eval type}  & \multicolumn{4}{c|}{\bf Performance metrics}\\
    \cline{5-8}
    \bf \# & & & &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr\\\hline
    C19 & --    & -- & -- & 0.319 & 0.252 & 0.535 & 0.970\\\hline
    C22 & Self-Eval & self      & max      & 0.319 & 0.253 & 0.533 & 0.966\\\hline
    C23 & Mutual-Eval      &  All six  & max-mean & 0.303 & 0.249 & 0.526 & 0.925\\
    C24 & Mutual-Eval      &  3-top    & max-mean & 0.304 & 0.250 & 0.527 &
    0.932\\\cline{3-8}
    C25 & Mutual-Eval      &  All six  & max-max  & 0.305 & 0.251 & 0.528 & 0.935\\
    C26 & Mutual-Eval      &  3-top    & max-max  & 0.305 & 0.251 & 0.528 & 0.939\\\hline
    C27 & Evaluator N/W & CNN  & max&\bf0.320&\bf0.254 &\bf0.536 &\bf0.978\\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Comparison of different ensembling techniques and ensembling sets}
  \label{tab:resEnsembCocValset}
\end{table*}
Next we present results from our experiments ensembling multiple caption
generating models.
%%
First we pick six models to form the model set participating in the ensemble.
%%
The six models used here include C6, C14, C17, C19, and two models trained
using concatenating the SUN397 with two spatial grid features 3+3Gauss and
4$\times$4Gauss, respectively.
%%
The models in the ensemble were chosen to include the best performing models
while also maintaining diversity of architectures in the ensemble.

Model C22 picks the best candidate among the pool of candidate captions using
the \emph{Self-Eval} method discussed in section~\ref{subsec:MutEval}.
%%
Models C23 to C26 are ensembles based on the \emph{Mutal-Eval} method.
%%
Here, the models mentioned in the column \emph{Evaluators} assign probability to
all the candidate captions.
%%
In case of \emph{All six} all the models participating in the ensemble are used
to rate all the candidates.
%%
In contrast, in \emph{3-top} three models with best perplexity score was used to
rate the candidates.
%%
Once candidate captions are scored, the technique mentioned in the column
\emph{Eval type} is used to aggregate these probability scores and pick the best
candidate. 
%%
Finally, C27 is the ensemble model which uses the CNN based evaluator n/w to rate and
pick the candidates.

%%
Comparing their performance based on evaluation metrics, we see that only the
CNN evaluator based C27 model improves over the best single model participating
in the ensemble, C19.
%%
Other two ensembling techniques fall short, with the self-evaluation based
method doing better than any of the mutual-evaluation based techniques.
%%
Among the mutual evaluation based techniques, we see that using only three best
models is better than using all the six models as evaluators.
%%
We also see that "max-max" works slightly better than "max-mean" as a method to
aggregate the scores assigned to a caption.
%%
But to their merit, mutual evaluation based and CNN based ensembling produces
much more diverse captions than both single models and self-evaluation based
ensembling, as we will see in section~\ref{subsubsec:QualAnalCoc}.
%%

\subsection{Language Diversity Analysis of the Captions}
\label{subsubsec:QualAnalCoc}

\begin{table*}[htp]
  \centering
  \newcommand{\mlhead}[2]{%
    \parbox[c][][c]{#1}{\smallskip\centering #2 \smallskip}
    }
  \begin{adjustbox}{center}
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \bf Model\# 
    &\mlhead{1.5cm}{\bf Mean \\Length}
    &\mlhead{2.1cm}{\bf Vocabulary size} 
    &\mlhead{2.1cm}{\bf\% Unique captions} 
    &\mlhead{2cm}{\bf\% New captions} 
    &\mlhead{2cm}{\bf Comments} \\\hline\hline
    C1      & 9.27 &  814 & 16.10 & 11.76 & Init vs\\
    C4      & 9.08 &  923 & 22.42 & 17.23 & Persist\\\hline
    C8      & 9.02 &  962 & 23.23 & 18.25& \\
    C16     & 9.11 &  983 & 26.39 & 20.80& Varying  \\
    C17     & 9.18 & 1197 & 31.14 & 24.03& Depth      \\
    C18     & 9.23 & 1164 & 31.10 & 24.28&    \\\hline
    C19     & 9.01 & 1112 & 28.43 & 22.04& Regular\\
    C21-cls &\bf9.58& 1191 &\bf49.16 &\bf44.39& vs Cls   \\\hline
    C22     & 9.06 &  993 & 21.34 & 15.36& Ensemble   \\
    C26     & 9.38 &\bf 1380 & 41.65 & 33.64& models   \\
    C27     & 9.13 & 1303 & 40.35 & 32.33& \\\hline
  \end{tabular}
  \end{adjustbox}
  \caption{Language diversity statistics of our best models }
  \label{tab:resCocQual}
\end{table*}

The evaluation based on automatic metrics presented in the previous sub-section
despite being the accepted standard in literature, is not very intuitive and
doesn't give an idea of how rich the captions generated by the models' are.
%%
To address this, in this sub-section we will look at few simple metrics which
measure the diversity of the captions generated by the models'.
%%
Specifically, we generate captions to the 40504 images in the validation set
using different models and look at four different heuristic measures of
diversity in these captions
%%
These heuristic measures are mean length of the generated captions, the size of
the vocabulary used by the models, what percentage of generated captions are
unique and what percentage of generated captions are not seen in the training
set.
%%

Table \ref{tab:resCocQual} shows these statistics computed for various
models. 
%%
Comparing the statistics for C1 and C4 we see that adding the persist path
moderately improves vocabulary size and diversity of the captions generated,
but with average length of the captions dropping slightly
%%
Next, comparing models C8, C16, C17 and C18, which vary only in th depth of the
language model, we see that increasing depth improves all the four statistics,
with the 3-layered model generating \url{~}$24\%$ new captions.

Biggest gain in sentence diversity is seen when we use the hierarchical decoder
based model, C21-cls. 
%%
Comparing it to equivalent model C19, we see that C21-cls generates almost twice
as many new captions, using a vocabulary of almost the similar size

Ensembling of multiple language models also helps improve the vocabulary size
and diversity of the captions, with the exception of model C22.
%%
Model C22, based on self-evaluation, seems to prefer un-original captions and
common words, with the percentage of new sentences dropping to just \url{~}15\%.
%%
This is the second lowest of all the models presented in
table~\ref{tab:resCocQual}.
%%
Contrastingly, both model C26 based on mutual evaluation and model C27 based on CNN
evaluator picl almost double the number of new captions.
%%
They also have significantly larger vocabulary and rival the C21-cls model in
terms of caption diversity.

Considering both the language diversity statistics and automatic evaluation
metrics, we conclude that model C27 is our best captioning model on the
MS-COCO validation set.
%% ---------------------------------------------------------------------------
\subsection{Comparison with State-of-the-art}
\begin{table*}[htp]
  \newcommand{\mct}[1]{%
    \multicolumn{2}{c|}{\bf#1}}
  \centering
  \begin{adjustbox}{center}
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
    \hline\hline
    \multirow{2}{*}{\bf Leaderboard Name}
                       &\mct{BLEU-4} &\mct{METEOR} &\mct{ROUGE-L}&\mct{CIDEr}\\\cline{2-9}
                    & c5    & c40   &  c5   & c40   & c5  &  c40  &  c5  &  c40 \\\hline\hline
    AugmentCNNwithDet~(C20)& 0.315 & 0.597 & 0.251 &0.340& 0.531 &\bf0.683&\bf0.956&\bf0.968\\
    ------ (CNN C27) & 0.310 & 0.596 & 0.250 &0.338& 0.529 & 0.681& 0.948& 0.961\\
    ATT\_VC~\cite{you2016image}& \bf0.316&0.599 & 0.250 &0.335&\bf0.535&0.682& 0.943& 0.958\\
    ------ (C17)  &  0.309 & 0.588 & 0.251 &0.342& 0.529 & 0.680& 0.943& 0.948\\
    OriolVinyals~\cite{Vinyals_2015_CVPR}      & 0.309 & 0.587 &\bf0.254&\bf0.346& 0.530 & 0.682& 0.943& 0.946\\
    MSR\_Captivator~\cite{Fang2015}  & 0.308 &\bf0.601& 0.248 &0.339& 0.526 & 0.680& 0.931& 0.937\\
    Berkeley LRCN~\cite{donahue2015long}   & 0.306 &\bf0.585& 0.247 &0.335& 0.528 & 0.678& 0.921& 0.934\\
    human~\cite{Chen2015}   & 0.217 & 0.471 & 0.252 &0.335& 0.484 & 0.626 & 0.854 & 0.910\\
    Montreal/Toronto~\cite{Xu2015show} & 0.277 & 0.537 & 0.241 &0.322& 0.516 & 0.654 & 0.865 & 0.893\\
    \hline \hline
  \end{tabular}
  \end{adjustbox}
  \caption{COCO 2014 test results. The scores are based on Microsoft
  COCO leaderboard. c\# indicates the number
  of reference captions used in evaluation. The models are sorted
  based on CIDEr score as in the leaderboard.}
  \label{tab:resCocLeaderTest}
\end{table*}

\begin{table*}[htp]
  \centering
  \begin{adjustbox}{center}
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline\hline
    \bf Model  &BLEU-4 &METEOR &ROUGE-L&CIDEr\\\hline
    C27 & \bf0.320&\bf0.254 &\bf0.536 &\bf0.978 \\
    C20 & 0.319 & 0.252 & 0.535 & 0.970 \\\hline
    ATT\_VC~\cite{you2016image} & 0.304& 0.243& -- & -- \\
    Berkeley LRCN~\cite{donahue2015long} & 0.300& 0.242& 0.524 & 0.896 \\
    OriolVinyals~\cite{Vinyals_2015_CVPR} & 0.277& 0.233& -- & 0.855 \\
    MSR\_Captivator~\cite{Fang2015} & 0.257& 0.236& -- & -- \\
    Montreal/Toronto~\cite{Xu2015show} & 0.250& 0.230& -- & -- \\
    \hline \hline
  \end{tabular}
  \end{adjustbox}
  \caption{Comparison of our models to the best published results on COCO 2014 validation set.}
  \label{tab:resCocPubVal}
\end{table*}

We compare the performance of our models with with several state-of-the-art
models reported on the COCO 2014 captioning leaderboard and published results on
the validation set.
%%
For this purpose, we submitted captions form models C17, C19 and the CNN
ensemble model C27 to the CodaLab portal.
%%

We compare our results with ATT\_VC~\cite{you2016image},
MSR\_Captivator~\cite{Fang2015}, Berkeley LRCN~\cite{donahue2015long},
Montreal/Toronto~\cite{Xu2015show}, OriolVinyals~\cite{Vinyals_2015_CVPR}, and
human~\cite{Chen2015} reported in CodaLab. It is worth noting that the
OriolVinyals model shares the same architecture with our adopted baseline model.
%%

Table~\ref{tab:resCocLeaderTest} reports the results of the benchmark%
\footnote{The leaderboard with more models and
scores is at \url{http://mscoco.org/dataset/\#captions-leaderboard}}.
%%
We compare our model with the other submissions within the top ten ranks in the
COCO leaderboard, as per meteor metric, excluding the entries not associated
with any published work or report.
%%
%%
Note that each metric in Table~\ref{tab:resCocLeaderTest} has two columns, \emph{c5}
and \emph{c40}. 
%%
This is because COCO data set contains 40 reference captions for a small subset
of images in the test set, referred to as \emph{c40}. 
%%
As already mentioned, using a larger number of reference captions makes the
metrics better correlated with human judgments and thus the metrics obtained on
the \emph{c40} are more reliable.
%%
The \emph{c5} metrics are obtained from the regular test set with only five
reference captions. 

We can see that the performances of our models drop slightly on the test set
compared to that on the validation set. 
%%
Also, while the ensemble model was the best model on the validation set, we
observe that the C19 is a better model on the test set.
%%
This could be because some models in the ensemble do not generalize well to the
test set.
%%
Considering the overall performance of our model C19, we are outperforming
several state-of-the art published results.
%%
More recently, there are three new entries on the leaderboard which are doing
better than our C19 model, but since they are not associated with any published
work, we refrain from discussing them here.

We should note here that the scores in CodaLab leaderboard do not necessarily
reflect the original published work results due to changes and updates.
%%
Thus, we also present a comparison with the published scores on the validation
set in Table~\ref{tab:resCocPubVal}.
%%
We see that our models outperform all published results on the validation set,
with the largest improvement seen in the CIDEr score.
%%
We could attribute most of this improvement to the rich set of image features we
use compared to other methods, most of which rely solely on CNN image features.

\subsection{Qualitative Examples and discussion}
%------------------------------------
\begin{figure}[bth]
  \begin{center}
  \newcommand{\mcCell}[1]{%
          \multicolumn{1}{|c|}{#1}}
  \centering
  \begin{adjustbox}{center}
  %\tabcolsep=0.05cm
  \begin{tabular}{c|c|c|c|}
          \cline{2-4}
    &\includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000502766.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000161720.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000385707.jpg} \\\hline
    \mcCell{\textbf{\em\scriptsize C27:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a man and a dog herding sheep in a field\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a bathroom with a sink toilet and bathtub\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a bottle of wine and a glass of wine\smallskip}\\\hline
     \mcCell{\textbf{\em\scriptsize C19:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a man standing next to a herd of sheep\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a bathroom with a toilet and a sink\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize two bottles of wine sitting on a table\smallskip}\\\hline
     &\includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000251330.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000218404.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000119516.jpg} \\\hline
    \mcCell{\textbf{\em\scriptsize C27:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a view of a bridge in the snow\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a table with plates of food on it\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a person riding a bike down a city street\smallskip}\\\hline
     \mcCell{\textbf{\em\scriptsize C19:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a train crossing a bridge over a river\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a table topped with plates of food and drinks\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a city street filled with lots of traffic\smallskip}\\\hline
  \end{tabular}
  \end{adjustbox}
  \end{center}
  \vspace*{-5mm}
  \caption{Captions generated for some validation images by two of our
    models. The first row contains samples where the ensemble model,
    C27, performs better, and the second row cases where C19 is
    better.}
  \label{fig:cococapSamps}
\end{figure}
So far, we evaluated our models using the automatic metrics, but the metrics are
only approximative in judging the correctness of the captions.
%%
Figure~\ref{fig:cococapSamps} shows some sample captions generated by the
ensemble model C27 and the best single model C19 on images from the validation
set.
%%
We can see that the generated captions are often fairly accurate, but still
contain some quite rudimentary errors.
%%
For the top left image, the CNN evaluator manages pick out a caption which
captures all three objects in the image.
%%
In comparison, C19 only mentions the "man" and the "sheep".
%%
Similarly, in all the images in the top row, CNN evaluator picks the caption
with more details than the one produced by C19.
%%
In contrast, bottom row contains examples where the caption generated by C19
describes the image better than the one picked by C27.

%% 
One of the errors which still persists is that our models are not able to
precisely learn the relationships between the objects in an image.
%%
For example, our models find it hard to differentiate between a person riding a
bicycle versus just standing next to it as seen in bottom right image in
figure~\ref{fig:cococapSamps}.
%%
A similar pattern of error is seen in relationships between various object
types.

Another commonly occurring error happens in counting, wherein the captions tend
to get the number of objects wrong.
%%
We also see that some generated captions repeat the words referring to objects
instead of using numerals.
%%
Still one drawback is in the vocabulary size of the generated sentences.
%%
As already seen in Table~\ref{tab:resCocQual}, even the ensemble model uses only
1,303 words which is about $1/8^{th}$ of the words presented to the models
during the training.

%=================================================================================
%=================================================================================
\section{Video Captioning}
%=================================================================================
Next we will shift our attention to evaluating our video captioning models.
%%
As discussed before, our video captioning datasets are much smaller in terms of
training video-caption pairs compared to the COCO dataset.
%%
Thus we will focus our efforts here on identifying the best video features
suitable for the video captioning task, with majority of our experiments
conducted on the richer MSR-VTT dataset.
%%
While choosing the language model configuration for these experiments, we will
rely heavily on the insights obtained from the experiments on COCO dataset.

We extract the \emph{gCNN} and \emph{SVM80} features only on key-frames in LSMDC
dataset and on one frame every second in MSR-VTT dataset.
%%
The features extracted only from key-frames will be indicated with a prefix
\emph{kf-} and features extracted from frames every second will be denoted by
the prefix \emph{ps-}.
%%
On LSMDC dataset, we only use the dense trajectory features~(\emph{DT}) as
segment-level features.
%%
On MSR-VTT dataset, apart from \emph{DT} features we also experiment with
improved dense trajectory features~(\emph{IDT}), and the features extracted from
the C3D network.

The evaluation of the video captioning models also rely on the same four
evaluation metrics and model perplexity measure on validation set. 
%%
But on the test set, both the LSMDC and the MSR-VTT challenge organizers provide
evaluation using the metrics and the human judgements, which we will present
here.

\subsection{Datasets}
First, we will discuss the LSMDC and MSR-VTT video captioning datasets in detail
here.
\subsubsection{LSMDC dataset}
\label{subsec:LsmdcData}
LSMDC dataset is the combination of two movie description datasets, MPII and
MVAD.
%%
Both MPII an MVAD datasets contain clips extracted from movies and a single
reference caption describing the clip, but they differ in the source of the
reference caption.
%%
MPII was collected from blue-ray movie clips and corresponding audio
description~(AD) and script data.
%%
The AD data was transcribed to text and manually aligned to the video clips.
%%
M-VAD was collected from dvd movie clips and only relies on AD data to annotate
the clips with reference caption.
%%
In both the datasets, any mention of people names in the collected reference
captions was replaced with the token "SOMEONE".

Combining these two, LSMDC dataset has 118\,081 video clips extracted from 202
unique movies and one reference caption for each clip.
%%
These clips are split into training set~(91\,908), validation set~(6,542), public
test set~(10,053) and a blind test set~(9\,578).
%%
The average length of these clips are just 4.8 seconds and the captions
in training set contain 22\,829 unique words.
%%
We filter this down to a vocabulary of 8814 words, again by removing words
occurring less than 5 times.

We have to note here that the reference captions on the LSMDC dataset are
relatively noisier than the other two dataset we have used.
%%
They have problems due mis-alignment of the captions with the video clips,
where the caption could be referring to objects which occur just before or after
the point the clip was cut from the movie. 
%%
Also, in some cases indiscriminate replacing of names with "SOMEONE" can lead to
very ambiguous captions.
%%
We can also see the effect of this in the poor performance of our models and other
state-of-the-art models on this dataset.

\subsubsection{MSR-VTT dataset}
The MSR-VTT dataset consists of 10\,000 video clips with 20 human-annotated
captions for each of them.
%%
Each video belongs to one of 20 categories including \emph{music},
\emph{gaming}, \emph{sports}, \emph{news}, etc.
%%
The dataset is split into training~(6\,513 clips), validation~(497 clips) and
test~(2\,990 clips) sets. 
%%
The training set videos are between 10 to 30 seconds long, with most of them
less than 20 seconds long.

The reference captions come from a vocabulary of $\sim$30k words, which we have
filtered down to $\sim$8k after removing words occurring fewer than 5 times.
%%
Although the number of videos clips in the dataset is relatively small compared
to M-VAD~\cite{rohrbach15cvpr} and MPII-MD~\cite{AtorabiM-VAD2015}, the dataset
is attractive due to the diversity of its videos and due to the larger number of
reference captions per video.

Visually examining the training set, we have found many different video styles,
including slide shows with static images, recordings of computer monitor showing
users playing video games, movie trailers with lots of fast cuts, smooth single
shot videos involving activities like cooking, news, interviews, etc.
%%
This means that the video captioning algorithm applied to this dataset needs to
handle all these diverse video styles.
%%
If we rely only on action recognition based features, the approach will suffer
with videos with lots of cuts and scene changes.
%%
On the other hand, if we rely only on frame-based features, the system will fail
to identify fine-grained differences in diverse action-oriented categories like
sports or cooking.
%%
Thus on this dataset, we use multiple captioning models, each trained on
different types of features, and the evaluator network to pick the final
candidate caption.
%%
\subsection{On LSMDC}
The keyframe used to extract \emph{kf-gCNN} and \emph{kf-SVM80} features is
sampled from the center of the video after padding video clips to be of at least
2 seconds long.
%%
In case of LSMDC dataset, it is reasonable to assume that the single keyframe is
quite representative of the video clips, as the clips are very short.
%%

\subsubsection{Results on Public Test Set}
\begin{table*}[t]
  \newcommand{\modpar}[4]{%
    \multirow{2}{*}{\emph{#1}} & \multirow{2}{*}{#2} & \multirow{2}{*}{#3}
    & \multirow{2}{*}{#4}}
  \newcommand{\bs}{\bf \small}
  \centering
  \begin{adjustbox}{center}
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
        \hline\hline
        \bs \#   &\bs init &\bs persist &\bs perplex&\bs Bleu\_4&\bs METEOR &\bs ROUGE\_L &\bs CIDEr  \\\hline\hline
        C4   & kf-SVM80 & kf-gCNN  &  --   & 0.003   &   0.053 &   0.114&   0.052 \\\hline
        L2   & kf-gCNN  & --    & 56.08 & 0.004   &   0.058 &   0.140&   0.071 \\
        L3   & kf-gCNN  & kf-SVM80 & 60.78 & 0.004   &\bf0.060 &   0.142&   0.073 \\
        L4   & kf-SVM80 & kf-gCNN  & 59.07 & 0.005   &   0.059 &   0.144&   0.087 \\\hline
        L5   & DT    & --    & 54.89 & 0.005   &   0.057 &   0.145&   0.087 \\
        L6   & DT    & kf-SVM80 & 59.75 & 0.005   &   0.057 &   0.141&   0.081 \\
        L7*  & kf-SVM80 & DT    & 55.14 &\bf0.006 &   0.058 &\bf0.146&\bf0.092 \\\hline
    \end{tabular}
  \end{adjustbox}
    \caption{Results obtained on the public test set of LSMDC 2015. 
      }
    \label{tab:resLsmdcVal}
\end{table*}

To evaluate various forms of our model we used the LSMDC 2015 public test
set as the benchmark. 
%%
Table~\ref{tab:resLsmdcVal} shows the four evaluation metrics computed for different
models.
%%
In addition to the metrics, we also show the perplexity of the model on the
public test set and the average lengths of the generated sentences.
%%

In order to get a quick baseline, we used the C4 model trained on the COCO
dataset to to generate captions on the LSMDC test set.
%%
This model was chosen as it is the best model using the \emph{gCNN} and
\emph{SVM80} features, which are compatible with \emph{kf-gCNN} and
\emph{kf-SVM80} features.
%%
The captions generated with this model are translated with with a simple
rule-based translation to better match the LSMDC vocabulary.
%%
It is implemented using the simple%
$w_{\text{in}} \longrightarrow w_{\text{out}}$ rule:
%%
\begin{align} \label{eqTrans} w_{\text{out}} = \begin{cases} \text{SOMEONE},&
\text{if } w_{\text{in}} \in \{\text{man}, \text{woman}, \\&
\text{\mbox{\qquad\qquad person}}, \text{boy}, \text{girl} \}\\ w_{\text{in}},&
\text{otherwise.} \end{cases} \end{align}

As we can see in Table~\ref{tab:resLsmdcVal}, the performance of this COCO model
on LSMDC dataset is quite bad.
%%
This is caused by two factors, first the vocabulary used in LSMDC captions is
quite different to that of COCO and hence automatic evaluation will rate the
COCO captions quite badly.
%%
Other factor, is that motion related information is completely ignored in this
model.

Next, we train models using the reference captions in the LSMDC dataset and
key-frame features.
%%
Comparing models L2, L3 and L4 in table~\ref{tab:resLsmdcVal}, we see that the
configuration with \emph{SVM80} features as \emph{init} input and \emph{gCNN}
features as \emph{persist} input perfroms the best.
%%
This also matches with the our similar observation on COCO dataset.
%%
We can also see that these models clearly out-perform the COCO baseline, mainly
due to the vocabulary update.

Finally, we trained three models using the dense trajectory features and the
keyframe-based \emph{SVM80} features, presented in Table~\ref{tab:resLsmdcVal}
as models L5--L7. 
%%
Again we see that using the higher-dimensional feature, here the \emph{DT}
feature, as the \emph{persistent} input to the LSTM network gives the best
performance among this group of models.
%%
Comparing model L7 with L4 shows that using video features as opposed to just
keyframe features gives a better performance.
%%
We can see that model L7 benefits from combining both keyframe and trajectory
features as opposed to just using the trajectory features as in model L5.
%%
The result of model L7 can be regarded as the best one obtained in our LSMDC
experiments, it has the best scores in 3 out of 4 metrics, and therefore we have
used it in our final blind test data submission to the LSMDC 2015 Challenge.

A rather surprising finding from the experiments on LSMDC dataset is that, here
using larger beam sizes in inference lead to poorer performance.
%%
This is slightly counterintuitive, but can be understood when we look at the
lengths of the sentences produced by these two beam sizes. 
%%
For example, model L7 produces sentences with the average length of 5.33 words
with beam size 1, while with beam size 5 the average length drops to just 3.79
words. 
%%
This is because with higher beam sizes the model always picks the most likely
sentence and penalizes heavily any word it is unsure of.
%%
This results in the model picking very generic sentences like \emph{``SOMEONE
looks at SOMEONE''} over more descriptive ones.
%%
Thus all the results presented in this section use the beam size of 1.

\subsubsection{Results from the LSMDC Challenge}
\label{subsec:LSMDCChall}
The submissions made to the LSMDC challenge were evaluated using both the
automatic metrics and human judgements.
%%
However, only human evaluation was used as criteria to finally rank the teams.
%%
Human evaluations were collected by showing some human judges a video and 5
associated captions and asking them to rank the five captions, based on 4
different criteria listed below:
\begin{itemize}
  \item \emph{Correctness} : Content in the caption is more correct with the video 
  \item \emph{Grammar} : Ranking the fluency and readability of the caption. 
  \item \emph{Relevance} : Which caption contains the references to more salient items in the video
  \item \emph{Helpfulness for the blind} : How helpful is the caption to help a
          blind person understand the scene.
\end{itemize}
The five captions consisted of one caption from each of the four submissions and
the reference caption for that video.
%%

Table~\ref{tab:resLsmdcTestMet} presents the automatic evaluation metrics on the
blind test set for the four LSMDC submissions.
%%
Our model L7 was ranked 3rd among the four teams as per the automatic metrics.
%%
But we should note here that the evaluation metrics are particularly unreliable
on the LSMDC dataset.
%%
This is due to having only a single reference caption for evaluation and also
the relatively poor match between the reference captions and the video content
as discussed in section~\ref{subsec:LsmdcData}.
%%

This is illustrated when we look at the average ranking of LSMDC submissions
asper the four human judgement criteria presented in
table~\ref{tab:resLsmdcTestHum}.
%%
Based on human judgements, our submission won the LSMDC challenge by obtaining
the best average ranking in 3 of the 4 criteria, and came second in the
"Helpfulness for the blind" criteria.
%%
Surprisingly, we also see that 3 out of the 4 models outperform the reference
captions on the \emph{Grammar} metric.
%%
But there is still a big gap between the reference caption and the automatic
captioning models in the three semantic metrics.
%%
A more detailed discussion on the LSMDC results is presented
in~\cite{DBLP:journals/corr/RohrbachTRTPLCS16}

\begin{table}[th]
  \centering
  \newcommand{\bs}{\small\bf}
  \begin{adjustbox}{center}
  \begin{tabular}{||c|c|c|c|c|}
    \hline\hline
    \bf Team  &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr \\\hline\hline
    Visual labels~\cite{rohrbach2015long} &\bf0.009&\bf0.071&\bf0.164&\bf0.112\\
    S2VT~\cite{venugopalan2015sequence} & 0.007 & 0.070 & 0.161 & 0.091\\
    \bf L7               & 0.006 & 0.061 & 0.156 & 0.090\\
    Temporal attention~\cite{yao2015describing} & 0.003 & 0.052 & 0.134 & 0.062\\\hline
    \hline
  \end{tabular}
  \end{adjustbox}
  \caption{LSMDC submission ranked using automatic evaluation metrics}
  \label{tab:resLsmdcTestMet}
\end{table}

\begin{table}[th]
  \centering
  \newcommand{\bs}{\small\bf}
  \begin{adjustbox}{center}
  \begin{tabular}{||c|c|c|c|c|}
    \hline\hline
    \bf Team  &\bs Correctness &\bs Grammar &\bs Relavance & \bf Helpful for blind\\\hline\hline
    Reference Caption    & 1.88  & 3.13  & 1.56  & 1.57\\\hline
    \bf L7               &\bf3.10&\bf2.70&\bf3.29&3.29\\
    Temporal attention~\cite{yao2015describing} & 3.14  & 2.71  & 3.31  & 3.36\\
    Visual labels~\cite{rohrbach2015long}& 3.32  & 3.37  & 3.32  &\bf3.26\\
    S2VT~\cite{venugopalan2015sequence}& 3.55  & 3.09  & 3.53  & 3.42\\
    \hline
  \end{tabular}
  \end{adjustbox}
  \caption{Human judgement scores for the LSMDC challenge submissions}
  \label{tab:resLsmdcTestHum}
\end{table}

%=================================================================================
\subsection{On MSR-VTT}
Similar to LSMDC dataset, we utilize both frame based and segment based video
features in our captioning models for MSR-VTT dataset.
%%
But since the videos in the MSR-VTT dataset are much longer than the videos in
LSMDC dataset, just using features from single key-frame is not sufficient.
%%
Thus we extract \emph{ps-gCNN} features on one frame every second.
%%
These features are then pooled using mean pooling.
%%

Apart from the dense trajectory video features, we also experiment with the
improved dense trajectory (IDT) and C3D video features on the MSR-VTT dataset.
%%
Additionally, we utilize the video category information available for all videos
in all splits of the dataset.
%%
This information is input to the language model as a one-hot vector of 20
dimensions and is referred to as \emph{20Categ}.

And as in case of LSMDC, we will present both our local evaluation and results
from the video captioning competition conducted based on this dataset, the
Microsoft Video to Text Challenge.

\subsubsection{Results on Validation Set}
In order to measure the performance differences due to the different feature
combinations , we use the validation set of the MSR-VTT
dataset which contains 497 videos.
%%
Table~\ref{tab:resVttFeat} shows the results on the validation set.
%%

Models M1, M2 and M3 all use the dense trajectory (DT) features as \emph{init}
input and the mean pooled frame-level \emph{ps-gCNN}features concatenated with
the video category vector, \emph{20Categ}, as the \emph{persist} input.
%%
But, they vary in the number of layers in the language model.
%%
Comparing their performance we see that the 2-layer model outperforms the single
layered model by a small margin, while the 3-layer one is the inferior one.

Model M4 is similar to M2, but uses the improved dense trajectories (IDT) as
the \emph{init} input instead.
%%
Model M5 differs from M2 by the fact that it uses mean pooled 3-D
convolutional features as the \emph{persist} input.
%%
We see that both M4 and M5 are competitive, but slightly worse than our best
single model, M2.
%%
Upon qualitatively analyzing the model outputs, we see that each of them
performs well on different kinds of videos.
%%
For example, model M5, which only uses input features trained for action
recognition, does well in videos involving a lot of motion, but suffers in
recognizing the overall scenery of the video.
%%
Conversely, model M2 trained on frame-level features does better in recognizing
objects and scenes, but makes mistakes with the sequence of their appearance,
possibly due to the pooling operation.
%%
This phenomenon can also be observed in the second row of images in
Figure~\ref{fig:VttcapSamps}. Model M5 produces a better caption on the video in
the first column, while M2 does better on the video in the second column.

To get maximum utility out of these diverse models, we use the CNN evaluator
network to pick the best candidate from the pool of captions generated by our
top four models, M1, M2, M4 and M5.
%%
The evaluator is trained using the gCNN$\oplus$20Categ as the video feature.
%%
This result is shown as model M6 in Table~\ref{tab:resVttFeat}.
%%
We can see that the CNN evaluator significantly outperforms, in all the four
metrics, every single model it picks its candidates from.

\begin{table*}[thp]
  \centering
  \begin{adjustbox}{center}
  \newcommand{\bs}{\small\bf}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
    \bs\# &\bs init &\bs persist &\bs depth &\bs perplex &\bs BLEU-4 &\bs METEOR &\bs CIDEr &\bs ROUGE-L \\\hline\hline
    M1 & DT  & ps-gCNN$\oplus$20Categ & 1  & 27.31 & 0.396 & 0.268 & 0.438 & 0.588 \\
    M2 & DT  & ps-gCNN$\oplus$20Categ & 2-res  & 27.73 & 0.409 & 0.268 & 0.433 & 0.598 \\
    M3 & DT  & ps-gCNN$\oplus$20Categ & 3-res  & 28.44 & 0.370 & 0.262 & 0.397 & 0.575 \\\hline
    M4 & IDT & ps-gCNN$\oplus$20Categ & 2-res  & 28.13 & 0.398 & 0.268 & 0.432 & 0.587 \\
    M5 & DT  & c3dfc7       & 2-res  & 29.58 & 0.369 & 0.268 & 0.413 & 0.577 \\\hline
    M6 & \multicolumn{4}{c|}{\em CNN ensemble of best 4 models}
                                  & \bf0.411 & \bf0.277 & \bf0.464 & \bf0.596 \\\hline
    \hline
  \end{tabular}
  \end{adjustbox}
  \caption{Performance of various features and 
    network depths on the validation set of MSR-VTT}
  \label{tab:resVttFeat}
\end{table*}

%% ---------------------------------------------------------------------------

\subsubsection{Results from the MSR-VTT Challenge}

Since the CNN evaluator model, M6, performed the best on the validation set, we
submitted captions from M6 to the MSR-VTT Challenge.
%%
Our submission appears on the
leaderboard\footnote{\url{http://ms-multimedia-challenge.com/leaderboard}} as
\emph{Aalto}.
%%
The submissions were evaluated on the blind test set using the four automatic
metrics.
%%
These results are shown in Table~\ref{tab:resultsTestMet}.
%%
Our submission achieved the best scores in the CIDEr metric and was ranked
overall second considering the average ranking across the metrics.

The submissions were also subject to human evaluation as the automatic metrics
are known to deviate from human judgements.
%%
This was seen in previous captioning challenges in the case of both
image~\cite{CocoChallengeSlides} and
video~\cite{DBLP:journals/corr/RohrbachTRTPLCS16} data.
%%
The human evaluation was based on three criteria: Coherence (C1), Relevance (C2)
and Helpfulness for the blind (C3).
%%
But unlike in case of LSMDC, these are collected as scores between 0 an 5 with 5
being better.
%%
Table~\ref{tab:resultsTestHum} presents the results of the human evaluation.
%%
The overall ranking was obtained again by considering the mean ranking across
the three metrics.
%%
As per human judgement, our submission was ranked the first among the 22 entries
in the challenge and thus won the challenge in this category.

Analyzing the two leaderboard, the automatic metric based one and the human
evaluation based one, we see that the disagreement between the two is relatively
minor, with most teams in the top 10 changing their ranking by only one
position.
%%
This can most likely be attributed to having a large number of 20 reference
captions per video for the evaluation.

\begin{table}[th]
  \centering
  \newcommand{\bs}{\small\bf}
  \scalebox{0.9}{
  \begin{tabular}{||c|c|c|c|c|}
    \hline\hline
    \bf Team  &\bs BLEU-4 &\bs METEOR &\bs CIDEr &\bs ROUGE-L \\\hline\hline
    v2t\_navigator &\bf0.408 &\bf0.282 & 0.448 &\bf0.609 \\
    \bf Aalto      & 0.398 & 0.269 &0.457 & 0.598 \\
    VideoLAB       & 0.391 & 0.277 & 0.441 & 0.606 \\
    ruc-uva        & 0.387 & 0.269 &\bf0.459 & 0.587 \\
    Fudan-ILC      & 0.387 & 0.268 & 0.419 & 0.595 \\\hline
    \hline
  \end{tabular}}
  \caption{Top 5 teams as per automatic evaluation metrics on the test set}
  \label{tab:resultsTestMet}
\end{table}

\begin{table}[th]
  \centering
  \newcommand{\bs}{\small\bf}
  \begin{tabular}{||c|c|c|c|}
    \hline\hline
    \bf Team  &\bs C1 &\bs C2 &\bs C3 \\\hline\hline
    \bf Aalto      & \bf3.263 & 3.104 & \bf3.244\\
    v2t\_navigator & 3.261 & 3.091 & 3.154 \\
    VideoLAB       & 3.237 & \bf3.109 & 3.143 \\
    Fudan-ILC      & 3.185 & 2.999 & 2.979 \\
    ruc-uva        & 3.225 & 2.997 & 2.933 \\\hline
    \hline
  \end{tabular}
  \caption{Top 5 teams as per human evaluation}
  \label{tab:resultsTestHum}
\end{table}

%% ===========================================================================

\subsection{Qualitative Analysis of Video Captioning}
%------------------------------------
Th kind of video captions generated by the models trained on LSMDC dataset
differs from the captions generated on the MSR-VTT data, due to the difference
in the language used in reference captions.
%%
Figure~\ref{fig:VttcapSamps} shows some examples from these two datasets and
captions generated by the best models trained on them.
%%
The top row contains videos from LSMDC dataset, while the bottom row shows
samples from MSR-VTT dataset. 
%%

The first two images on the top row show the examples of accurate captions
generated by our L7 model.
%%
But the in the video on the right, the model gets confused by the last shot
showing a man, and completely ignores the car driving away.
%%
We observe this kind of behaviour is seen in many error cases, where the model
ignores the main subject of the video and generates the caption based on an
object it knows well, eg. "man" in this case", even if it is insignificant in
the video.

In the examples from the MSR-VTT dataset shown in second row of
figure~\ref{fig:VttcapSamps}, we see three distinct cases.
%%
The example on the left shows the case where model M5, based on C3D features,
produces better caption than model M2, which uses frame-level features.
%%
The example in the centre shows the case where this is reversed and frame level
features do better than action recognition based C3D features.
%%
Finally, rightmost example shows the case where where the CNN evaluator, M6,
picks a better caption than the ones generated by both M2, our best single
model, and M5.

\begin{figure}[thp]
  \begin{center}
  \newcommand{\mcCell}[1]{%
  \multicolumn{1}{c}{#1}}
  \centering
  \begin{adjustbox}{center}
  \tabcolsep=0.10cm
  \begin{tabular}{lll}
    \mcCell{\includegraphics[width=0.25\linewidth]{images/LsmdcVid0.png}} &
    \mcCell{\includegraphics[width=0.25\linewidth]{images/LsmdcVid1.png}} &
    \mcCell{\includegraphics[width=0.25\linewidth]{images/LsmdcVid2.png}}
    \vspace{-2mm}\\
    \textbf{\scriptsize\em L7:} \scriptsize someone runs up to the car&
    \textbf{\scriptsize\em L7:} \scriptsize someone is dancing with her&
    \textbf{\scriptsize\em L7:} \scriptsize someone looks up at the house\medskip\\
    \mcCell{\includegraphics[width=0.25\linewidth]{images/9150.png}} &
    \mcCell{\includegraphics[width=0.25\linewidth]{images/9799.png}} &
    \mcCell{\includegraphics[width=0.25\linewidth]{images/7997.png}}\vspace{-2mm} \\
    \textbf{\scriptsize\em M6:} \scriptsize a man is running in a gym &
    \textbf{\scriptsize\em M6:} \scriptsize a person is playing with a rubix cube &
    \textbf{\scriptsize\em M6:} \scriptsize cartoon characters are interacting\\
    \textbf{\scriptsize\em M2:} \scriptsize a man is running&
    \textbf{\scriptsize\em M2:} \scriptsize a man is holding a phone&
    \textbf{\scriptsize\em M2:} \scriptsize a person is playing a video game\\
    \textbf{\scriptsize\em M5:} \scriptsize a man is playing basketball&
    \textbf{\scriptsize\em M5:} \scriptsize a person is playing with a rubix cube &
    \textbf{\scriptsize\em M5:} \scriptsize a group of people are talking\\
  \end{tabular}
  \end{adjustbox}
  \end{center}
  \vspace{-5mm}
  \caption{Sample captions generated for some test set videos by our models.
  First row contains captions from model L7 on the LSMDC public test set and the
  second row contains captions from few of our best models on samples from
  MSR-VTT test set. }
  \label{fig:VttcapSamps}
\end{figure}

\begin{figure}[ht]
\begin{center}
  \begin{adjustbox}{center}
  \begin{subfigure}[c]{0.65\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{images/VTTCiderCateg.pdf}%
    \caption{Performance by category}%
    \label{fig:VttCategPerf}
  \end{subfigure}%
  \begin{subfigure}[c]{0.65\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{images/VTTCiderLengths.pdf}
    \caption{Performance by length}
    \label{fig:VttLenPerf}
  \end{subfigure}
  \end{adjustbox}
\end{center}
\vspace*{-5mm}
\caption{Performance of video captioning system as per CIDEr metric compared for
        various video lengths and sub-categories of the MSR-VTT validation set.}
\label{fig:VttPerf}
\end{figure}

Next we try to understand if the video captioning performance is affected by
video lengths or by video categories.
%%
For this we use the MSR-VTT dataset as it has longer videos and also provides
the category tag for each video.
%%
Figure~\ref{fig:VttPerf} shows the results from this analysis.
%%
Here we first evaluate the captions generated by model M6 for each of the 497 videos
in the validation set of the MSR-VTT dataset.
%%
Then the CIDEr scores for videos belonging to the same category is aggregated
and plotted to obtain figure~\ref{fig:VttCategPerf}.
%%
Similarly, CIDEr scores for the videos with the same length in seconds (after
rounding to nearest integer) is aggregated to produce
figure~\ref{fig:VttLenPerf}.
%% ---------------------------------------------------------------------------

From figure~\ref{fig:VttCategPerf} we see that the captioning performance varies
greatly across categories.
%%
The model seems to perform very well on categories like "how to", "gaming",
"sports" and "vehicles".
%%
In case of "how to" category, this can be attributed to the relatively
simplistic nature of these videos.
%%
These videos generally consist of a close-up of a person performing certain
distinctive actions, and thus probably simpler to caption using action
recognition features. 
%%
Similar structure exists in "gaming" videos, which mostly contains screen
capture of people playing video games.
%%
We see that most of the categories where captioning does well have a single
visual theme and distinctive category of actions associated with them. 
%%
In contrast, the categories where the captioning does poorly, e.g.\@ "tv",
"news", "education" etc., are unified conceptually and not necessarily visually.
%%
For example, "news" category can contain variety of scenes, both indoor and
outdoor, and variety of actions. 
%%
These videos also tend to have many sharp scene changes and thus leading the
action recognition features to do poorly on them.

Interestingly, length of the video only seems to have a small correlation with
the performance as seen in figure~\ref{fig:VttLenPerf}.
%%
Very long videos (>20 seconds) seem to perform only slightly worse than the
shorter videos.
%%
We should also note here that the estimates of performance of the longer videos
are noisier, as indicated by the error bars in figure~\ref{fig:VttLenPerf}, as
there are fewer videos falling in this category. 
%%
The degradation in longer videos can be expected as the video features we use,
both action recognition and frame based features, don't account for long term
temporal dependencies.
%%
Nevertheless, it is still surprising that the degradation is relatively minor.

\section{Summary of Results and Conclusions}
In this chapter, we presented results from our image captioning experiments on
MS-COCO dataset and video captioning experiments on LSMDC and MSR-VTT datasets.
%%
Now we will summarize these results and highlight the important findings from
these experiments.

From the experiments on MS-COCO dataset, we saw that our proposal to enhance the
CNN based \emph{gCNN} image features with explicit object and scene detector
features greatly improved the performance. 
%%
Counter intuitively, we observe that use of object location features doesn't
necessarily add to the model, with only the smallest of these features, the
\emph{3+3Gauss}, moderately improving the performance by automatic metrics when
used in conjunction with detector features.

Our proposed language model enhancements, using two separate feature input
channels and increasing depth with residual connections, contribute positively.
%%
Using two feature input channels helped use utilize more than one image feature
and greatly improved the model performance.
%%
Increasing the language model depth moderately improved the performance as per
metrics, while it considerably improved the vocabulary and diversity of the
captions generated.
%%
We also found that residual connections improved the training convergence
speeds and achieved lower perplexity measure.
%%
But our attempts to implement hierarchical decoder in the language model,
despite greatly improving the generated caption diversity, adversely affected
the correctness of the generated captions and failed to match up to the model
with single stage decoder.
%%

Among the proposed ensembling techniques, CNN based evaluator network achieved
the best performance.
%%
In image captioning, while it slightly improved in automatic metrics over the
best single model in the ensemble, it contributed positively by increasing the
language diversity. 
%%
But in case of video captioning on the diverse MSR-VTT dataset, we found that
this ensembling technique clearly outperformed all the models participating in
the ensemble and was our best model.

From video captioning experiments, we learned that a combination of action
recognition based and frame based video features works well for the captioning
task.
%%
The performance of the video captioning systems is still relatively poorer than
the image captioning models.
%%
Nevertheless, the models presented here for video caption generation can be
considered state-of-the art for this task.
%%
This is supported by the results from our participation in the LSMDC and MSR-VTT
video captioning challenges, both of which we won, as judged by human
evaluators.

