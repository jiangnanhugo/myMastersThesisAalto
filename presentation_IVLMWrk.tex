\documentclass{beamer}
\usetheme{Frankfurt}
\usecolortheme{beaver}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
\setlength{\itemsep}{\fill}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{longtable}

\def\cvs{${[}$Id: slides.tex,v 1.19 2015/12/09 16:42:43 shettyr1 Exp ${]}$}

%\beamertemplatenavigationsymbolsempty
\bibliographystyle{acm}

\newcommand{\slide}[1]{\scalebox{1.0}{\includegraphics[page=#1]{slides}}}
\newcommand{\mct}[1]{\multicolumn{2}{c|}{\bf#1}}
\newcommand{\mcCell}[1]{\multicolumn{1}{|c|}{#1}}
\newcommand{\bs}{\small\bf}
\newcommand{\red}[1]{\protect\begin{color}{red}#1\protect\end{color}}
\newcommand{\green}[1]{\protect\begin{color}{green}#1\protect\end{color}}
\newcommand{\mlhead}[2]{%
    \parbox[c][][c]{#1}{\smallskip\centering #2 \smallskip}
    }
\begin{document}

%%-----------------------------------------------------------------------------

\title{Exploiting Scene Context for Image Captioning}
\author[Rakshith Shetty and Jorma Laaksonen]{Rakshith Shetty, Hamed R.\@-Tavakoli and Jorma Laaksonen}
\institute{Department of Computer Science, \\Aalto University}
\date{\today}

\frame{\titlepage} 

%%\frame{\frametitle{Table of contents}\tableofcontents} 

%%-----------------------------------------------------------------------------

\begin{frame}{Understanding Visual Media}
  \begin{itemize}
  \item<1-> Images = \{objects, attributes, relations\}\\
  \begin{figure}[h]
    \begin{columns}
    \column{.5\linewidth}
    %\hspace{-8mm}%
    \hfill\includegraphics[width=0.6\textwidth]{images/COCO_train2014_000000544856.jpg}
    \hspace{-5mm}\column{.5\linewidth}
    \centering
    \caption{\only<3->{a giraffe walking through a patch of high dried out grass.}}
    \end{columns}
  \end{figure}
  %\item<2->Videos = \{Images + temporal relations\}\\[4mm]
  \item<2-> \only<2>{\Large How do we summarize this information?}
           \only<3>{\Large Humans rely on natural language descriptions}
           \only<4>{\Large Can computers generate captions too?}
  \end{itemize}
\end{frame}

%%-----------------------------------------------------------------------------
\begin{frame}{Why Captioning?}
%\includegraphics[width=1\textwidth]{images/VideoCaptionModified.png}
\begin{itemize}
\item Moving beyond single label classification.
\item Better search and indexing of visual data.
\item Enabling better accessibility to the blind.
\item Enable computers to connect textual and visual data. 
\end{itemize}
\end{frame}
%%-----------------------------------------------------------------------------
%\begin{frame}{Table of Contents}
%\begin{itemize}
%\item Background 
%\item Baseline model
%\item How do we evaluate captions?
%\item Enhancing visual features and language model 
%\item Results
%\item Conclusions 
%\end{itemize} 
%\end{frame}
%%-----------------------------------------------------------------------------

%%=============================================================================
\section{Background and Related Work}
%%-----------------------------------------------------------------------------
\begin{frame}{Up Next.....}
\tableofcontents[currentsection] 
\end{frame}
%%-----------------------------------------------------------------------------
\subsection{Problem Statement}
%%-----------------------------------------------------------------------------
\begin{frame}{Basic Framework}
\textbf{Given an input image, $V$, output a caption, $S$}
\begin{itemize}
\item $S$ is not unique
\item Learn a probabilistic model, $P(S|V)$
\item $S$, is a sequence of words $(w_0, w_1,\cdots, w_{L-1})$
\begin{equation}
\label{eq:langB1} P(S|V) = P(w_0, w_1, \cdots, w_{L-1}|V)
\end{equation}
\end{itemize} 
\end{frame}
\subsection{Visual Features}
%%-----------------------------------------------------------------------------
\begin{frame}{Encoder--Decoder Recipe}
\begin{itemize}
\item Two-staged approach
    {
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\textwidth]{images/EncDec.pdf}
        \end{figure}
     }
\item Deep convolutional neural networks act as encoder
    \begin{itemize}
        \item Pre-trained on ImageNet for classification 
        \item Activations from fully connected layers
    \end{itemize}
\item Recurrent language models act as the decoder 
    \begin{itemize}    
        \item LSTM networks are used
        \item Softmax output to define probability over words 
    \end{itemize}
\end{itemize} 
\end{frame}
%%-----------------------------------------------------------------------------
\subsection{Baseline Model}
\begin{frame}{Captioning Techniques in Literature}
\begin{itemize}
  \item Two different approaches:
      \begin{enumerate}
        \item Retrieval-based 
        \item Generative models (Ours)
      \end{enumerate}
  \item Best approaches from the literature
          \begin{itemize}
  \item NIC~\cite{Vinyals_2015_CVPR} -- our baseline 
      \begin{itemize}
        \item CNN+LSTM, end-to-end 
      \end{itemize}
  \item MSR\_Captivator~\cite{Fang2015}
      \begin{itemize}
        \item CNN + word detectors + exponential language model
      \end{itemize}
  \item ATT\_VC~\cite{you2016image}
      \begin{itemize}
        \item concept detector + attention + LSTM 
      \end{itemize}
  %\item<5-> Retrieval Vs Generative: 
  %    \begin{itemize}
  %      \item Retrieval problem: Easy to evaluate, no novelty 
  %      \item Generative problem: Novel captions, harder to evaluate
  %    \end{itemize}
\end{itemize} 
\end{itemize} 
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Baseline Model}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.4\linewidth]{images/Thesis_lstmLangGen.pdf}
    \end{figure}
    \begin{itemize}
        \item Adopted from~\cite{Vinyals_2015_CVPR} 
        \item LSTM network based language model
        \item The sequence $(V,START,w_0, w_1, \cdots,w_{L-1})$ is the input
        \item Words are represented as word vectors 
        \item Visual feature is presented only in the initialization 
    \end{itemize}
\end{frame}
%%-----------------------------------------------------------------------------
%\begin{frame}{Training and Generation}
%    \begin{itemize}
%        \item Pick a caption--image pair and maximize the probability assigned to caption 
%        \item Minimize the negative log likelihood 
%            \begin{align*}
%              \mathcal{NL}(w_0,\cdots, w_{L-1} | V) = -\sum_{t=0}^{L-1} \log(p(w_t|w_{t-1},V))
%            \end{align*}
%        \item Stochastic gradient descent using RMSProp 
%        \item Training limited to language model parameters 
%        \item Beamsearch is used in test mode to generate captions 
%    \end{itemize}
%\end{frame}
%%-----------------------------------------------------------------------------
\subsection{Evaluation Methods}
%%-----------------------------------------------------------------------------
\begin{frame}{How do we evaluate generated captions?}
\begin{figure}[t]
    \begin{minipage}[c]{0.45\linewidth}
        %\begin{center}
            \includegraphics[width=1.0\textwidth]{images/COCO_train2014_000000440903.jpg}
        %\end{center}
    \end{minipage}\hfill
    \begin{minipage}[c]{0.52\linewidth}
            \textbf{C1:}\footnotesize a beach with people relaxing on a \underline{\red{sunny day}}. \\
            \textbf{C2}:\footnotesize people are relaxing on the beach where there is a \underline{\red{big rock}}. \\
            \textbf{C3}:\footnotesize a beach with a group of people with surf boards and \underline{\red{umberellas}}. \\
            \textbf{C4}:\footnotesize a group of people enjoy a beach near a lagoon filled with \underline{\red{crystal blue
            water}}. \\
            \textbf{C5}:\footnotesize a \underline{\red{man walking}} on a beach with his surf board in a case. \\
    \end{minipage}
  \vspace*{-3mm}
\end{figure}
    \begin{itemize}
        \item Captions are not unique -- no single right answer
        \item Both semantically and syntactically diverse valid captions
        \item Candidate caption can be compared to reference captions 
    \end{itemize}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Evaluation metrics}
\begin{itemize}
    \item Metrics adopted from machine translation 
       \begin{itemize}
           \item \textbf{BLEU-4}: Score based on n-gram matches
           \item \textbf{METEOR}: Incorporates a dictionary based soft matches
           \item \textbf{ROUGE-L}: Matches longest common sub-sequence of words 
       \end{itemize}
    \item Proposed specifically for Captioning 
       \begin{itemize}
           \item \textbf{CIDEr}: Based on n-gram matches and TF-IDF weighting to boost uncommon words
       \end{itemize}
    \item Used in recent captioning competitions, widely reported in literature 
    \item Based on MS-COCO Image Captioning Challenge 2015, \emph{METEOR} and \emph{CIDEr} better track human judgement
\end{itemize}
\end{frame}
%%-----------------------------------------------------------------------------
\subsection{Datasets}
%%-----------------------------------------------------------------------------
\begin{frame}{MS-COCO dataset}
\begin{itemize}
    \item Microsoft Common Objects in Context (MS COCO)
       \begin{itemize}
            \item $\sim164$k images: $\sim80$k train, $\sim40$k val \& $\sim40$k test 
            \item 5 Human Annotated captions per image.
            \item 80 commonly occurring object categories and bounding box annotation
            \item Evaluation server implementing the four metrics 
       \end{itemize}
\end{itemize}\vspace{-3mm}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{images/Coco_sample.png}
        \vfill
    \end{figure}
\end{frame}
%%-----------------------------------------------------------------------------
%%=============================================================================
%%=============================================================================
\section{Enhancing Visual Features}
%%-----------------------------------------------------------------------------
\begin{frame}{Up Next.....}
\tableofcontents[currentsection] 
\end{frame}
%%-----------------------------------------------------------------------------
\subsection{Image Features}
%%-----------------------------------------------------------------------------
\begin{frame}{Proposed Image Features}
    \begin{itemize}
            \item Primary features are extracted from GoogLeNet~(\textbf{\emph{gCNN}})
        \begin{itemize}
            \item 22 Layer deep
            \item Extracted from the 5th inception layer 
        \end{itemize}
        \item CNN features: Not clear if multiple objects are encoded 
        \item Localization information is lost by design! 
        \item What about the background!? 
        \item Hence three additional context features 
           \begin{itemize}
               \item Explicit object detector features
               \item Explicit scene detector features 
               \item Object location features 
           \end{itemize}
    \end{itemize}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Explicit Scene Detector}
    \begin{itemize}
        \item Utilize SUN dataset with 108,756 images with 397 scene types 
        \item \small{CNNs pre-trained on MIT places} $+$ \small{CNNs pre-trained on ImageNet} $\rightarrow$ train SVM detectors 
        \item One for each category in SUN (397 objects) 
        \item Referred to as~(\textbf{\emph{SUN397}})  
    \end{itemize}
  \begin{figure}[h]
    \begin{columns}
    \column{.5\linewidth}
    %\hspace{-8mm}%
    \centering
    \includegraphics[width=1.0\textwidth]{images/503790.png}
    \hspace{-5mm}\column{.5\linewidth}
    \centering
    \includegraphics[width=1.0\textwidth]{images/69356.png}
    \end{columns}
  \end{figure}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Object Detection and Localization Features}
    \begin{itemize}
        \item Faster R-CNN networks output bounding boxes 
        \item Trained on COCO dataset using the box annotations 
        \item Boxes are mapped to a regular grid, one for each category 
        \item Can be collapsed to get object detector vector~(\textbf{\emph{FRC80}}) 
        \begin{figure}[h]
          %\hspace{-8mm}%
          \includegraphics[width=0.9\textwidth]{images/FrcnnFeats.pdf}
        \end{figure}
    \end{itemize}
\end{frame}
%%-----------------------------------------------------------------------------
%\begin{frame}{Dense Trajectory Features}
%\begin{columns}
%\column{0.65\linewidth}
%\begin{itemize}
%        \item Based on tracking interest points
%        \begin{itemize}
%            \item Densely sample few interest points 
%            \item Track them and form trajectories 
%            \item Extract local descriptors around trajectories 
%        \end{itemize}
%    \end{itemize}
%    \column{.35\linewidth}
%    \begin{figure}[h] 
%      \includegraphics[width=1.0\textwidth]{./images/IDenseTrajVis.png}\hfill 
%    \end{figure}
%\end{columns}
%\begin{columns}
%\hspace{-8mm}\column{1.0\linewidth}
%\begin{itemize}
%   \item Trajectory type can be identified by the local descriptors 
%       \begin{itemize}
%           \item Different actions produce different trajectory types
%           \item Actions can be identified by trajectory types 
%       \end{itemize}
%   \item Video can contain arbitrary no of trajectories 
%       \begin{itemize}
%           \item Language model expects fixed sized vector
%       \end{itemize}
%   \item Encode into fixed size using bag-of-words
%       \begin{itemize}
%           \item Create a codebook using K-means
%           \item Assign each trajectory to nearest entry in codebook 
%           \item Video represented as a histogram with codebook as bins
%       \end{itemize}
%\end{itemize}
%\end{columns}
%\end{frame}
%%%-----------------------------------------------------------------------------
%\begin{frame}{3-D CNN Based Features}
%    \begin{itemize}
%        \item Attempts to extend success of CNNs to videos~\cite{3dCNN_ji2013, KarpathyCVPR14, DBLP:C3D} 
%        \item Change 2D filters in lower layers to 3D filters 
%        \item \textbf{\emph{C3D}}~\cite{DBLP:C3D} is a popular architecture
%        \item We use version pre-trained on Sports-1M dataset 
%        \item Only process one segment at a time (16-frames)
%            \begin{itemize}
%                \item Hence limited context
%            \end{itemize}
%    \item Extract features from the \emph{fc7} layer 
%    \end{itemize}
%\end{frame}
%%%=============================================================================
\section{Enhancing Language Model}
%%-----------------------------------------------------------------------------
\begin{frame}{Up Next.....}
\tableofcontents[currentsection] 
\end{frame}
\subsection{Extending the LSTM language model}
%%-----------------------------------------------------------------------------
\begin{frame}{Additional Input Channel}
    \begin{itemize}
        \item Multiple image features are available 
        \item Only \emph{init} channel available 
            \begin{itemize}
                \item Shares weight with word vector
            \end{itemize}
        \item New input channel to present features at all times
        \item Refer to as \emph{persist}
        \item Context features in \emph{init} and Visual feature in \emph{persist} 
        \item No weight sharing 
            \begin{itemize}
                \item Different embedding spaces for word vector and Visual feature 
            \end{itemize}
    \end{itemize}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Residual Connections}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.4\textwidth]{images/ACMM_MultilayerResidualLSTM.pdf}
    \end{figure}
    \begin{itemize}
        \item Use multiple layers of LSTM 
        \item Add a residual connection between layers~\cite{He2015} 
        \item Higher layer only needs to learn a residual function 
    \end{itemize}
\end{frame}
%%-----------------------------------------------------------------------------
%\begin{frame}{Hierarchical Decoder}
%\begin{itemize}
%    \item Every word prediction is a difficult $Z$-way classification
%       \begin{itemize}
%           \item $Z$ is vocabulary size, $Z=8792$ in COCO
%           \item Errors add up quickly 
%           \item All output words are treated independently 
%       \end{itemize}
%    \item Instead split it into two-step hierarchical classification
%       \begin{itemize}
%            \item Semantically group words into classes 
%            \item First predict the class of the word 
%            \item Next predict the word within the class
%       \end{itemize}
%    \item Words can be grouped by two methods 
%       \begin{itemize}
%           \item Brown Clustering -- hierarchical clustering
%           \item K-means clustering of word vectors 
%       \end{itemize}
%\end{itemize}
%\end{frame}
%%-----------------------------------------------------------------------------
\subsection{Ensembling}
\begin{frame}{Ensembling the caption generators}
\begin{itemize}
    \item Ensemble generators trained with different features and parameters
    \item Models in ensemble set produce a caption and a corresponding probability 
    \item Pick based on the assigned probability? 
    \item One can also use models to mutually evaluate each other 
       \begin{itemize}
           \item Each model rates others captions too
           \item Combine the rating using max-max or max-mean 
       \end{itemize}
    \item Unsatisfactory performance. 
\end{itemize}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Ensembling -- CNN evaluator}
\begin{columns}       
    \column{0.3\linewidth}
    \begin{figure}[h]
        \centering
        \includegraphics[width=1.0\textwidth]{images/CnnEval.pdf}
        \vfill
    \end{figure}
    \column{0.7\linewidth}
    \begin{itemize}
        \item Train evaluator to compute similarity b/w visual input and caption 
           \begin{itemize}
               \item A CNN to encode the sentence 
               \item Visual feature is embedded to the same space 
               \item Cosine similarity b/w the two vectors 
           \end{itemize}
        \item Trained to assign highest score to correct caption--visual pair 
        \item Generative vs Discriminative 
    \end{itemize}
\end{columns}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Ensembling -- Example}
\begin{columns}       
    \column{0.4\linewidth}
    \begin{figure}[h]
        \centering
        \includegraphics[width=1.0\linewidth]{images/COCO_val2014_000000530509.jpg}
        \vfill
    \end{figure}
    \column{0.6\linewidth}
    \begin{figure}[thp]
      \begin{center}
      \centering
      \scalebox{0.8}{
      \begin{tabular}{|l|c|c|}
        \hline
        \textbf{\scriptsize\em Caption} &\textbf{\scriptsize\em Log-Prob} &\textbf{\scriptsize\em CNN} \\\hline
        \textbf{\scriptsize\em \#1:} \scriptsize \green{a young boy holding a kite in a park} & -6.31 &\bf 0.80 \\\hline
        \textbf{\scriptsize\em \#2:} \scriptsize a man and a girl are flying a kite    & -7.43 & 0.65 \\\hline
        \textbf{\scriptsize\em \#3:} \scriptsize a young boy holding a kite in a park  & -6.64 &\bf 0.80 \\\hline
        \textbf{\scriptsize\em \#4:} \scriptsize a young boy holding a kite in a field & -7.85 & 0.79 \\\hline
        \textbf{\scriptsize\em \#5:} \scriptsize \red{a woman holding a kite in a park}&\bf-6.19 & 0.74 \\\hline
        \textbf{\scriptsize\em \#6:} \scriptsize a woman holding a kite in a park      & -6.70 & 0.74 \\\hline
      \end{tabular}
      }
        \vfill
      \end{center}
    \end{figure}
\end{columns}
\end{frame}
%%-----------------------------------------------------------------------------
\section{Results}
%%-----------------------------------------------------------------------------
\begin{frame}{Up Next.....}
\tableofcontents[currentsection] 
\end{frame}
\subsection{Image Captioning Experiments}
%%-----------------------------------------------------------------------------
\begin{frame}{Experiments with Features}
\begin{itemize}
    \item<1-> Which features work best as context features?
       \begin{itemize}
           \item<1-> Language model accessing CNN features throughout is helpful 
           \item<1-> \emph{SUN397} and \emph{FRC80} improve the performance significantly.
           \item<1-> Counterintuitively, localization features don't help much 
           \item<1-> Best model: \textbf{\emph{init}} = \emph{3+3Gauss$\oplus$SUN397$\oplus$FRC80}\\ \textbf{\emph{persist}} = \emph{gCNN}
       \end{itemize}
\end{itemize}

\begin{table}[th]
  \scalebox{0.7}{
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline\hline
       \multirow{2}{*}{\bf Model}  & \multirow{2}{*}{\bf Contextual
       Feature} &{\bf validation} & \multicolumn{4}{c|}{\bf performance metrics} \\
        \cline{4-7}
    & &\bf perplexity  &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr \\\hline\hline
    M1 &   ---           & 10.82  & 0.259 & 0.222 & 0.490 & 0.750 \\ \hline
    M2 & FRC80           & 10.15  &0.316 & 0.249 & 0.534 & 0.952 \\
    M3 & SUN397+FRC80    &10.05  & 0.315 & \bf0.250 &0.532 & 0.954 \\
    M4 & 4$\times$4Gauss & 10.15  & 0.308 & 0.246 & 0.527 & 0.921 \\
    M5 & 3+3Gauss        & 10.08  & 0.308 & 0.247 & 0.527 & 0.928 \\
    -- &\parbox[c][][c]{4cm}{\smallskip\centering 3+3Gauss$\oplus$SUN397\\$\oplus$FRC80\smallskip} 
                             &\bf9.93&\bf0.318&\bf0.250&\bf0.533 &\bf0.957\\\hline
    \hline
  \end{tabular}
  }
\end{table}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Experiments with Depth}
\begin{itemize}
    \item How does depth affect the performance ?
       \begin{itemize}
           \item<1-> Depth of 2/3 layers increases performance as per metrics moderately, perplexity is worse!
           \item<1-> Residual connections improve perplexity greatly 
           \item<1-> Residual connections also improve convergence speeds 
           \only<2>{\hspace{-10mm}
           \begin{figure}[h]
                \centering
                \includegraphics[width=0.65\textwidth]{images/ResidualVsRegPerplex.pdf}
           \end{figure}}
       \end{itemize}
\end{itemize}
\begin{table}[th]
  \centering
  \scalebox{0.7}{
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline\hline
       \multirow{2}{*}{\bf Model} & \multicolumn{2}{c|}{\bf parameters }   &{\bf
       validation} & \multicolumn{4}{c|}{\bf performance metrics} \\
        \cline{2-3}\cline{5-8}
    &\bf depth &\bf r &\bf perplexity  &\bs BLEU-4 &\bs METEOR &\bs ROUGE-L &\bs CIDEr \\\hline\hline
    M1 & 1 & --- & 10.82   & 0.259 & 0.222 & 0.490 & 0.750 \\ \hline
    M3 & 1 & --- & 10.05  & 0.315 & 0.250 & 0.532 & 0.954 \\
    M6 & 2 & no & 10.14  & 0.318 & 0.252 & 0.535 & 0.967 \\
    M7 & 3 & no & 10.34  & 0.316 & 0.253 & 0.533 & 0.964 \\
    M8 & 2 & yes & 9.92  &\bf0.320& 0.253 &\bf0.536& 0.966 \\
    M9 & 3 & yes &\bf 9.69  & 0.316 &\bf0.254& 0.532& 0.962 \\\hline
    \hline
  \end{tabular}
  }
\end{table}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Experiments with Ensembling}
\begin{itemize}
    \item Benefits of Ensembling  
       \begin{itemize}
        \item<1-> CNN evaluator improves the performance moderately
        \item<1-> Evaluator better than self-evaluation
        \item<1-> Caption diversity is greatly improved 
       \end{itemize}
\end{itemize}
\begin{table}[ht]
  \centering
  \scalebox{0.80}{
  \begin{tabular}{|c|c|c|}
    \hline\hline
    \multirow{2}{*}{\bf Model} & \multicolumn{2}{c|}{\bf performance metrics} \\
	\cline{2-3}
    & \bf METEOR &\bf CIDEr \\\hline\hline
    Baseline         & 0.222 & 0.750 \\
    Single Best      & 0.252 & 0.970 \\
    Ensemble -- Self & 0.253 &\bf0.966 \\
    Ensemble -- CNN  & \bf0.254 &\bf0.978 \\\hline
    \hline
  \end{tabular}
  }
\end{table}
\begin{table}[tbh]
  \centering
  \scalebox{0.80}{
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    \bf Model 
    &\mlhead{1.6cm}{\bf Mean Length}
    &\mlhead{2.1cm}{\bf Vocabulary\\ Size} 
    &\mlhead{2.1cm}{\bf\% Unique Captions} 
    &\mlhead{2cm}{\bf\% New Captions} \\\hline
    Baseline      &\bf9.27 &  513 & 16.10 & 11.76\\
    Single Best & 9.01 & 1112 & 28.43 & 22.04 \\
    Ensemble -- Self Eval & 9.06 &  993 & 21.34 & 15.36   \\
    Ensemble -- CNN Eval  & 9.13 &\bf 1303 &\bf 40.35 &\bf 32.33 \\\hline
  \end{tabular}
  }
  \caption{Language diversity statistics of a few selected models.}
  \label{tab:resCocQual}
\end{table}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Comparison to Published Results on COCO Dataset}
\begin{table}[t]
  \centering
  \scalebox{0.6}{
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
    \hline\hline
    \multirow{2}{*}{\bf Leaderboard Name}
                       &\mct{BLEU-4} &\mct{METEOR} &\mct{ROUGE-L}&\mct{CIDEr}\\\cline{2-9}
                    & c5    & c40   &  c5   & c40   & c5  &  c40  &  c5  &  c40 \\\hline\hline
    AugmentCNNwithDet~(C19)& 0.315 & 0.597 & 0.251 &0.340& 0.531 &\bf0.683&\bf0.956&\bf0.968\\
    ------ (C27) & 0.310 & 0.596 & 0.250 &0.338& 0.529 & 0.681& 0.948& 0.961\\
    ATT\_VC~\cite{you2016image}& \bf0.316&0.599 & 0.250 &0.335&\bf0.535&0.682& 0.943& 0.958\\
    ------ (C17)  &  0.309 & 0.588 & 0.251 &0.342& 0.529 & 0.680& 0.943& 0.948\\
    OriolVinyals~\cite{Vinyals_2015_CVPR}      & 0.309 & 0.587 &\bf0.254&\bf0.346& 0.530 & 0.682& 0.943& 0.946\\
    MSR\_Captivator~\cite{Fang2015}  & 0.308 &\bf0.601& 0.248 &0.339& 0.526 & 0.680& 0.931& 0.937\\
    Berkeley LRCN~\cite{donahue2015long}   & 0.306 &\bf0.585& 0.247 &0.335& 0.528 & 0.678& 0.921& 0.934\\
    human~\cite{Chen2015}   & 0.217 & 0.471 & 0.252 &0.335& 0.484 & 0.626 & 0.854 & 0.910\\
    Montreal/Toronto~\cite{Xu2015show} & 0.277 & 0.537 & 0.241 &0.322& 0.516 & 0.654 & 0.865 & 0.893\\
    \hline \hline
  \end{tabular}
  }
  \caption{Test Set Results from COCO Leaderboard}
\end{table}

\begin{table}[h]
  \centering
  \scalebox{0.6}{
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline\hline
    \bf \# &BLEU-4 &METEOR &ROUGE-L&CIDEr\\\hline
    C27 & \bf0.320&\bf0.254 &\bf0.536 &\bf0.978 \\
    C20 & 0.319 & 0.252 & 0.535 & 0.970 \\\hline
    ATT\_VC~\cite{you2016image} & 0.304& 0.243& -- & -- \\
    Berkeley LRCN~\cite{donahue2015long} & 0.300& 0.242& 0.524 & 0.896 \\
    OriolVinyals~\cite{Vinyals_2015_CVPR} & 0.277& 0.233& -- & 0.855 \\
    MSR\_Captivator~\cite{Fang2015} & 0.257& 0.236& -- & -- \\
    Montreal/Toronto~\cite{Xu2015show} & 0.250& 0.230& -- & -- \\
    \hline \hline
  \end{tabular}
  }
  \caption{Best published results on COCO 2014 validation set.}
\end{table}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Sample Results on COCO}
\begin{figure}[h]
  \begin{center}
  \centering
  %\tabcolsep=0.05cm
  \scalebox{0.7}{
  \begin{tabular}{c|c|c|c|}
          \cline{2-4}
    &\includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000502766.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000161720.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000385707.jpg} \\\hline
    \mcCell{\textbf{\em\scriptsize CNN:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a man and a dog herding sheep in a field\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a bathroom with a sink toilet and bathtub\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a bottle of wine and a glass of wine\smallskip}\\\hline
     \mcCell{\textbf{\em\scriptsize SB:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a man standing next to a herd of sheep\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a bathroom with a toilet and a sink\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize two bottles of wine sitting on a table\smallskip}\\\hline
     &\includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000251330.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000218404.jpg} &
    \includegraphics[width=0.25\linewidth,height=2.5cm]{images/COCO_val2014_000000119516.jpg} \\\hline
    \mcCell{\textbf{\em\scriptsize CNN:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a view of a bridge in the snow\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a table with plates of food on it\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a person riding a bike down a city street\smallskip}\\\hline
     \mcCell{\textbf{\em\scriptsize SB:}}& \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a train crossing a bridge over a river\smallskip} &
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a table topped with plates of food and drinks\smallskip}&
     \parbox[c][][c]{0.25\linewidth}{\smallskip \scriptsize a city street filled with lots of traffic\smallskip}\\\hline
  \end{tabular}
  }
  \end{center}
  \caption{Captions generated for some images from the COCO validation set by two of our
    models. The first row contains samples where the ensemble model,
    CNN,  performs better, and the second row cases where SB is
    better.}
  \label{fig:cococapSamps}
\end{figure}

\end{frame}
%%-----------------------------------------------------------------------------
%\begin{frame}{Sample Validation Set Results}
%    \textbf{The Good}\\[2mm]
%    \includegraphics[width=0.5\textwidth]{images/230350439_genCap.png}\hspace*{0.01\textwidth} \includegraphics[width=0.5\textwidth]{images/110270280_genCapEdited.png}\\[2mm]
%    \includegraphics[width=0.5\textwidth]{images/110510033_genCap.png}\hspace*{0.01\textwidth} \includegraphics[width=0.5\textwidth]{images/140760125_genCap.png}\\[2mm]
%    \textbf{The Bad}\\[2mm]
%    \includegraphics[width=0.5\textwidth]{images/110260059_genCap.png}\hspace*{0.01\textwidth} \includegraphics[width=0.5\textwidth]{images/110260532_genCap.png}\\[2mm]
%    \textbf{The Ugly}\\[2mm]
%    \includegraphics[width=0.5\textwidth]{images/110510496_genCap.png}\hspace*{0.01\textwidth} \includegraphics[width=0.5\textwidth]{images/140770020_genCap.png}\\
%\end{frame}
%%-----------------------------------------------------------------------------
%%=============================================================================
\section{Conclusions}
%%-----------------------------------------------------------------------------
%\begin{frame}{Next.....}
%\tableofcontents[currentsection] 
%\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{Conclusions}
\begin{itemize}
        \item The proposed context features (i.e.\@ object and scene detectors) complement the CNN features well 
    \item Two extensions proposed to the language model were useful. 
       \begin{itemize}
           \item The \emph{persist} input channel greatly improved performance
           \item The residual connections achieve lower training cost, and convergence time
       \end{itemize}
    \item Evaluator based ensembling improved performance and caption diversity
    \item It achieves state-of-the art performance compared to other published work
\end{itemize}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}{}
\begin{center}
    \Large Thank You for Listening\\[6mm]
    \Large Any Questions? 
\end{center}
\end{frame}
%%-----------------------------------------------------------------------------
\begin{frame}[allowframebreaks]
        \frametitle{References}
        \bibliography{bib_thesis_used_only}
\end{frame}
\end{document}
