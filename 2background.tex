\chapter{Vision and Language: Background} \label{chapter:background} 
%%===========================================================================%%
\section{Role in human and machine learning}
Long held interest in AI of integrating vision and language (FIND REFERENCES). BUT WHY?? 
\fixme{Fresh writing: 1 page}
%%===========================================================================%%
\section{Related work and literature}
\fixme{Upto 2-3 pages, from ACMMM paper and Project Report}
%%----------------------------------%%
\subsection{Visual features}
%%----------------------------------%%
Visual media, be it image or video, are inherently very high-dimensional data.
%%
This large dimensionality poses a challenge for machine learning systems trying
to extract higher level semantic information directly from such visual inputs, such as in
case of captioning.
%%
To address this, images and videos have been traditionally represented with
smaller feature vectors which attempt to encode the most important information
present in them, while ignoring redundancies. 
%%
This feature extraction step is very important in any image understanding
pipeline as this is the only input which subsequent modules see and can be a
major bottleneck in how well the entire system performs. 
%%
Thus, we will now review the relevant literature and identify some best
performing image and video feature extraction techniques, which will be used in
designing our captioning system later on.

%=================================================================================
\subsubsection{Image Features}
Traditionally, tasks like object recognition have relied on using
hand-engineered features to represent images. 
%%
But recently deep Convolutional Neural Networks~(CNN), which learn to extract features
necessary for the task entirely from the data, have become a popular choice for
image feature extraction.
%%
This was triggered by the spectacular improvement in image classification
accuracy seen on the Imagenet Large Scale Visual Recognition Challenge~(ILSVRC)
2012, with the first use of CNNs in this challenge.
%%
In this challenge involving classifying the input images to one of thousand
classes, the submission by~\cite{Krizhevsky2012} using a deep CNN outperformed
all the other submission by a large margin.
%%
This set-off further exploration into CNN architectures and has driven up the
performance in the Imagenet classification task to even surpass human
performance~\cite{he2015delving}.
%%

More interestingly, these deep CNNs trained on the large Imagenet dataset for
the classification task have been shown to generalize very well to other
datasets and tasks as well.
%%
In~\cite{yosinski2014transferable}, it is shown that using the weights from the
CNNs pre-trained on Imagenet as initialization when using similar CNNs for other
datasets and tasks significantly helps learn a better model.
%%
Alternatively, even using activations from a higher layer of a Imagenet
pre-trained CNN as off-the shelf image features is also shown to produce
state-of-the art results~\cite{Donahue2014,Razavian2014CVPR} in several datasets
and tasks, including object detection, scene recognition etc.
%%
We will use this second approach, i.e.\@ use activations from CNNs pre-trained
on Imagenet as feature input to our captioning model, without any fine-tuning of
the CNNs for this task.
%%
GoogleLeNet~\cite{DBLP:journals/corr/SzegedyLJSRAEVR14} and
VGG~\cite{Simonyan14c} architectures, which won the different categories of
ILSVRC 2014 competition, have been popular source of such feature extraction in
the community with the ready availability of the code and pre-trained models.

%=================================================================================
\subsubsection{Video Features}
Unlike in the case of images, where the convolutional neural network (CNN) image
features have become the \emph{de facto} standard features for many image
understanding related tasks, no single video feature extraction method has
achieved the best performance across tasks and datasets.
%%
Dense trajectories~\cite{DBLP:conf/cvpr/WangKSL11} and Improved dense
trajectories~\cite{Wang2013} have been popular video feature extraction methods
in the task of action recognition.
%%
Here, some interest points are densely sampled in an initial frame and then tracked
across frames, to form trajectories.
%%
Furthermore, a set of local descriptors are extracted around the trajectory
co-ordinates to obtain a rich representation of the trajectory.
%%
These trajectories can encode motion patterns from variety of sources, including
actions from agents in the video, camera motion etc.
%%

Following the success of deep CNN models on static images, there have been
attempts to train 3-D CNNs which operate directly on video
segments~\cite{3dCNN_ji2013, KarpathyCVPR14, DBLP:C3D}.
%%
However, these models need a lot of training data and are usually pre-trained on
some large action recognition dataset, e.g.\@ the Sports-1M
dataset~\cite{KarpathyCVPR14}.

All the above features encode action related information very well but fail to
capture the information about identity of the objects in the video.
%%
The task of caption generation also requires us to describe the objects seen in
the video and their attributes, in addition to recognizing actions.
%%
This can be solved by extracting features from individual
frames~\cite{venugopalan2015sequence} or key-frames~\cite{shetty2015video} using
CNNs pre-trained on the ImageNet dataset.

In this work we explore both these paradigms of video feature extraction and
both hand-engineered trajectory features and deep video features for the task of
automatic captioning.
\subsection{Natural language modelling and generation}
%%----------------------------------%%
Generative language models are widely used, in speech recognition and synthesis,
to document analysis and automatic dialog systems.
%%
Our task of caption generation also involves learning a conditional generative language
model which can generate captions given the input visual feature.
%%
For this purpose we will now review a few different language modelling
approaches, and discuss their suitability for our task.

The simple N-gram language models, which are based on counting co-occurrence
statistics of sequence of n-words, are surprisingly good baselines for a lot of
language modelling tasks.
%%
Simple n-gram models are constrained to generate sentences only using n-grams it
has seen in the training set.
%%
Maximum entropy language model~\cite{berger1996maximum}(ME-LM) overcomes this by using
the principle of maximum entropy while learning the model.
%%
The principle of maximum entropy dictates that among all the probabilistic
models which satisfy the constraints of the training data, one should pick the
model which is the most uniform.
%%
Thus it allows the model to share some probability for unseen n-grams as well.

But both the above models suffer from using having to use a short context of previous
words when predicting the next word, limited by the n-gram size, which can lead
to longer generated sentences being in-coherent.
%%
Alternatively, one could use recurrent neural networks as generative language
models as proposed initially in~\cite{sutskever2014sequence}.
%%
Recurrent neural networks have the benefit of having access to infinite context
through its hidden state.
%%
Additionally, these models do not need pre-defined language features, unlike in case
ME-LM, and can learn the necessary word representations from the data. 
%%
Indeed, such recurrent network based language models are quite popular in
machine translation task~\cite{bahdanau2014neural}

\subsection{Intermediate Problem: Multi-Modal embeddings}
%%----------------------------------%%
A precursor to the problem of caption generation from input visual features, is
the simpler problem of learning to map both visual features and the
corresponding natural language labels into a common semantic space.
%%
This was posed as a solution to automatically annotate images
in~\ref{weston2010large}.
%%
Here, using a simple bag-of-visual words image representation, both image and
words are projected into a common space, with training objective of ranking
correctly the matching image and annotation pairs. 

In~\cite{frome2013devise}, using CNN features as image representations and
word vectors from Word2Vec~\cite{mikolov2013distributed} embeddings as word
representation, a linear embedding is learnt to map the image vectors onto
word vectors corresponding to the lables associated with the image.
%%
This allows the model to do ``zero-shot'' recognition on image classes it has
not seen before, by finding the nearest label to the embedded image feature.
%%
A much simpler approach is used in~\cite{norouzi2013zero}, where the mapping to
semantic embedding space is done by a convex combinations of $n$ word vectors
associated with top $n$ classes identified on the image.
%%
This does away with the need to learn the embedding, while still achieving
impressive results in zero-shot classification on Imagenet dataset

This literature forms the basis for the way visual features are used in
captioning literature.
%%
In many early works, both image features and word vectors are input to the LSTM
language model using the same input matrix, forcing the model to learn a common
joint embedding for them.

%%----------------------------------%%
\subsection{Recent Literature on Caption Generation}
%%----------------------------------%%
\red{NEEDS REVIEW and Extension}
Visual caption generation (or, more generally, visual description generation)
techniques include a wide range of methods and models.
%%
They can be broadly categorized into two groups: ones generating captions by
retrieving from a database~\cite{Farhadi2010, Hodosh2013,Karpathy2014},
and ones using natural language generation techniques to produce
captions~\cite{Li2011,kulkarni2013babytalk,Vinyals_2015_CVPR,Fang2015}.
%%

Early work on captioning images presented in~\cite{Farhadi2010} was retrieval
based, wherein a similarity score between sentences and images is computed and
is used to retireve the best matching caption to an input image. 
%%
This method relied on few hand engineered image features and dependcy parsing
based features for sentences.
%%
In ~\cite{Hodosh2013}, authors pose the image description as a ranking problem
involving correctly ranking a set of captions by their relevance to the input
image.
%%
They argue that, ability to rank the captions correctly is a good measure of
semantic image understanding, with the added benifit that it is much easier to
automatically evaluate such ranked lists than evaluating generated novel
captions.
%%
Such retrieval based system is further enhanced by use of deep image feature and
word embeddings in~\cite{Karpathy2014}.

One of the early models to successfully generate novel image captions was
described~\cite{kulkarni2013babytalk}, albeit relying on pre-defined sentence
templates to generate captions.
%%
In ~\cite{Li2011}, this template based language model is replaced with a n-gram
based one learnt from large scale natural language data from the web.

Following the successful use of recurrent network based language models on
tasks like automatic speech recognition~\cite{mikolov2010recurrent} and machine
translation~\cite{bahdanau2014neural}, it was quickly adapted to the image
captioning literature as well~\cite{Karpathy_2015_CVPR, Vinyals_2015_CVPR,
donahue2015long}.
%%
All these methods consists of two-stage encoder--decoder models, with 
encoder being the image feature extraction module and decoder being the
recurrent language model.
%%
One major advantage of this approach is it allows end to end training of the
entire system.
%%
In case of ~\cite{Karpathy_2015_CVPR}, CNN based image features and a simple
recurrent neural network~(RNN) based language model are used as encoder and
decoder respectively.
%%
The authors also propose techniques to align different parts of a caption to
different regions in the image. 
%%
Similarly in~\cite{Vinyals_2015_CVPR}, authors also use CNN image features, but
an LSTM based network is used for the language model.
%%
\red{donahue2015long}
%%

As opposed to such end-to-end learning systems, \cite{Fang2015} takes a modular
approach. 
%%
They first train a set of object or concept detectors using multiple instance
learning. 
%%
Then, a maximum entropy language model takes these detector outputs as input to
generate the candidate sentence.
