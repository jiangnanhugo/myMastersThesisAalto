\chapter{Automatic Captioning Pipeline: Model, Data and Evaluation}
\label{chapter:baseline}
%%===========================================================================%%
In this chapter, we will examine in detail an entire image captioning pipeline.
First a baseline captioning model adapted from \cite{Vinyals_2015_CVPR} is
presented. We then discuss some popular datasets which are used to train the
models. \red{These datasets contain images/videos and associated ground truth
captions which the models learn to predict}. We will also discuss the metrics
used to evaluate the captions generated by the models. 

We will use the model presented here as the baseline against which we compare
the improvements presented in the rest of the thesis.

\section{Baseline Architecture}
Our caption generation model consists of two stages: the visual feature
traction stage followed by a language model
%%
In the first stage we use various techniques to extract descriptors of the
visual contents of input image or video. These descriptors are then represented
as one or more vectors of fixed dimension. 
%%
The language model then uses these feature vectors and generates a suitable
caption to describe the image. 
%%
This pipeline is illustrated in Figure~\ref{fig:fullModel}. 
%%
In the following subsections we will discuss in detail the different
image features and the language model architectures we have
experimented with.

\begin{figure*}[t]
  \begin{center}
    \includegraphics[width=0.8\linewidth]{images/AcMM_fullModel.png}
  \end{center}
  \vspace*{-3mm}
  \caption{Block diagram showing our image captioning pipeline from an
    input image to the generated caption.}
  \label{fig:fullModel}
\end{figure*}

\fixme{Upto 3 pages, from ACMMM paper and Project Report}

\subsection{Image Feature extraction}


%%----------------------------------%%
\subsection{Recurrent Language Model}
%%----------------------------------%%
Recurrent Neural networks(RNN) are a good match for the language modeling
problem as they have infinite depth in time in a sense, allowing a RNN based
language model to utilize preceding context when making predictions. This is
because the current state of a RNN, $h_t$ depends on its previous state,
$h_{t-1}$, and thereby on all its previous inputs, $x_t$. This is described in
equations \ref{eq:rnn_h} and \ref{eq:rnn_y}. Here N is the hidden layer
non-linearity and $y_t$ is the network output at time t.

\begin{eqnarray}
    \label{eq:rnn_h} h_t &=& N(W_{xh}x_t+W_{hh}h_{t-1}+b_h)\\
    \label{eq:rnn_y} y_t &=& W_{hy}h_t + b_y
\end{eqnarray} 

A powerful class of recurrent networks called Long Short-Term Memory
(LSTM) models \cite{Hochreiter:1997:LSM:1246443.1246450} are widely used today.
Each LSTM unit has a memory cell whose update is controlled by a set of
non-linear gates as shown in figure \ref{fig:lstmcell}. This allows the network
to store important information for a long duration in time, giving them ability
to better handle longer sequences. This model also addresses the problem of
vanishing gradients observed when training vanilla RNN models
\cite{Bengio93Vanishing}\cite{pascanu2012difficulty}.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.5\linewidth]{Figures/LstmCell.png}
	\end{center}
	\caption{Block diagram of a single LSTM cell. dotted lines indicate
		gate controls and solid lines are data flow. Triangle indicates
		sigmoid non-linearity.}
	\label{fig:lstmcell}
\end{figure}

In an LSTM cell the update to the memory value $m$ is controlled using the
input gate and the forget gate.  The output is controlled using the output
gate. The gates are not binary and are instead implemented using sigmoidal
non-linearity keeping them completely differentiable. 
%%
The input and forget gates give the LSTM units the ability to preserve the
content of their memory cells over long periods making it easier to learn
longer patterns.
%%
This process is formalized in the equations below.

\begin{align}
	i(t) &= \sigma(W_{ix}x(t-1) + W_{iy}y(t-1))\\
	o(t) &= \sigma(W_{ox}x(t-1) + W_{oy}y(t-1))\\
	f(t) &= \sigma(W_{fx}x(t-1) + W_{fy}y(t-1))\\
	m(t) &= f(t)\cdot m(t-1) + i(t)\cdot \tanh(W_{mx}x(t)+W_{my}y(t-1))\\
	y_{lstm}(t) &= o(t) \cdot m(t)
\end{align}


\subsection{Training and Regularization}
%%----------------------------------%%
\subsection{Test Mode: Beam Search}
%%----------------------------------%%
\subsection{Implementation Platform Details}
%%----------------------------------%%
%%===========================================================================%%
\section{Datasets}
\fixme{Upto 4 pages, mostly fresh writing, but parts can be taken from previous
papers}
\subsection{MS-COCO}
\subsection{LSMDC}
\subsection{MSR-VTT}
\subsection{Other Datasets}
%%===========================================================================%%
\section{Evaluation Metrics}
\fixme{Btw 1-2 pages} 
Basics can be taken from Project Report, but I should also
discuss results from COCO competition and which metrics are better with proof
