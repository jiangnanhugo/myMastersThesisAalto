\chapter{Exploring Visual Features}
\label{chapter:VisFeatChapter}

Finding good feature vector representations for the input images/videos is a
very important task for successful design of captioning system.
%%
Such a feature representation should be compact, but also able to encode all the
relevant information for the task. 
%%
For image captioning task, the system should know all the objects in the image,
their most essential properties such as color, their absolute position and
relative location to each other, along with the type of the scene these objects
are located in.
%%
In case of video captioning, apart from all the above mentioned information, the
feature vector should also encode sufficient temporal information to enable
recognizing actions, order of events etc.

In this chapter, we will describe the visual features we have used in
our captioning model.
%%
The chapter is divided into two main sections, with one discussing the visual
features used to represent images and the other describing the features used to
encode videos.

%%===========================================================================%%
\section{Image Feature Extraction}
\fixme{Upto 4 pages: Mostly from Paper and Report, except the visualizations}
\label{sec:ImageFeat}

Features we use to represent images in our captioning model, are primarily,
activation values extracted from the deep Convolutional Neural Network (CNN)
layers.
%%
We extract features from couple of different CNN architectures, as it is yet
unclear which are the best features for the captioning task.
%%
As seen before, our baseline image captioning model only one of these CNN
features as input, and achieves reasonable performance.


The CNN features encode a rich variety of information, including scene context,
object type etc. as seen from its performance in the baseline model.
%%
But this representation is still very compact and probably (as seen later in
experiments) in-efficient for the language model to be able to extract the
information it needs to generate correct captions.
%%
It is also unclear to what extent these features encode multiple objects and
object locations, since they are trained on a task involving recognizing a
single object class.

Thus, in a bid to improve the performance over baseline model, we provide the
language model with additional features which explicitly encode presence of
objects, scene types and object location.
%%
In this regard we train explicit object detectors and scene detectors based on
the CNN features, and also construct features encoding object localization based
on outputs from Faster Region-based Convolutional Neural Network (R-CNN).

SHOW VISUALIZATIONS IF POSSIBLE
    --> Feature vector TSNE
    --> Non obvious information in object detectors, For eg. difference between
    using binary vs real valued detector scores
%%----------------------------------%%
\subsection{Convolutional Neural Networks}
%%----------------------------------%%
Convolutional Neural Networks have in the recent years become the most
widely used models for practically all tasks related to image
classification and understanding.
%%
It was shown in~\cite{Donahue2014} and~\cite{Razavian2014CVPR} that
activations of the fully connected layers of a CNN trained for image
classification task act as a general feature representation of the
image and can be successfully used to solve other tasks as well.
%%
In line with this, we extract image features from different CNN architectures
pre-trained on two large datasets namely, ImageNet~\cite{ImagenetOrig} and MIT
Places~\cite{Zhou2014NIPS}, originally aimed for object and scene
classification, respectively.
%%

The CNNs we use are based on the widely used
GoogLeNet~\cite{DBLP:journals/corr/SzegedyLJSRAEVR14} and VGG~\cite{Simonyan14c}
architectures. 
%%
Both of these architectures were originally proposed achieve good results in the
ILSVRC 2014 object classification challenge, with them coming first and second
respectively in the competition.

\subsubsection{GoogleLeNet} 
\label{subsec:gCNN}
The main idea behind the GoogleLeNet architecture was to use small dense
structures like 1x1, 3x3 convolutions to mimic a large sparse layer.
%%
For this purpose they utilize the \emph{Inception} modules consisting of 1x1,
3x3 and 5x5 convolutions and max pooling layers.
%%
This network achieved the top-5 error rate of $6.67\%$ in the 1000 class ILSVRC 2014
Classification Challenge, finishing first in the competition.

We use two different versions of the GoogLeNet features in our experiments.
%%
Features from GoogLeNet trained on Imagenet dataset is used as direct input to
our language model (referred to as "\emph{gCnn}" in the rest of the text),
whereas features from the GoogLeNet trained on MIT Places data (referred to as
"\emph{pCnn}" in the rest of the text) is used to construct scene recognition
features which are used as auxillary inputs to our language model.
%%

To extract \emph{gCNN} features, we use the activations from the
\emph{5th Inception module}, having the dimensionality of 1024.
%%
We augment these features with the reverse spatial pyramid pooling proposed
in~\cite{Gong2014} with two scale levels.
%%
First scale is just the full image rescaled to the size of 224x224.
%%
The second level consists of a $3\times3$ grid of overlapping patches of size
128x128 with stride of 64, and horizontal flipping.
%%
The activations of these regions are then reduced to a single 1024 dimensional
feature vector by using average pooling or by just using the central crop.
%%
Finally, the activations of the two scales are concatenated resulting to
2048-dimensional features.
%%
Note that due to two differnt pooling methods on the second scale, we obtain
two somewhat different feature vectors of 2048 dimensions from the same network.
%%
Our final \emph{gCnn} feature vector of size 4096 dimensions is obtained by
concatenating these two feature vectors.
%%
This is also the feature vector used as the input to our baseline image
captioning model. 

The same procedure described above for the ImageNet data trained GoogLeNet has
been followed also with the Places data trained GoogLeNet, with the exception
that instead of the Inception module, the \emph{3rd classification branch} has
been used as the activation layer where the feature vectors have been extracted.
%%
In this case, in addition to mean and center pooling in the second scale of
reverse spatial pyramid, we also use max pooling, and thus obtain three
different features with the dimensionality of 2048 each.
%%
Note that the term \emph{pCnn} refers collevtively to this set of 3 features.

\subsubsection{VGG network} 
VGG network was introduced in ~\cite{Simonyan14c}, where the authors study the
effect of depth on the performance of convolutional networks.
%%
The salient feature of this architecture is the exclusive use of small 3x3 and
2x2 convolutional filters throughout the network.
%%
This helps keep the number of parameters small even with the increased depth.
%%
This network achieves the top-5 error rate of $7.3\%$ in the 1000 class ILSVRC
2014 Classification Challenge, finishing second in the competition.

We use two variants of the VGG net, namely 16-layered and the 19-layered ones
from ~\cite{Simonyan14c} in our experiments.
%%
Features extracted from these two networks are referred to as ``\emph{VGG16}''
and ``\emph{VGG19}'' respectively throughout the rest of the text.
%%
From both the variants, we extract the activations of the network on the second
fully-connected 4096-dimensional \emph{fc7} layer for the given input images
whose aspect ratio is distorted to a square.
%%
Ten regions, as suggested in~\cite{Krizhevsky2012}, are extracted from all
images and average pooling of the region-wise features are used to generate the
final features.
%%

\subsection{Object Detectors}
\label{subsec:svm80}
%%----------------------------------%%

As mentioned before,  we augment the CNN features with explicit object detector
features where each dimension represents the presence or absence of one of the
80 object categories defined in the COCO data set~\cite{Lin2014}.
%%
For this purpose we extract the previously described five CNN-based ImageNet
trained VGG and GoogLeNet features from the images of the COCO
2014~\cite{Lin2014} training set and trained an SVM classifier for each of the
80 object categories.
%%
In particular, we here utilized linear SVMs with homogeneous kernel
maps~\cite{Vedaldi2010} of order $d=2$ to approximate the intersection kernel.
%%
Furthermore, we used two rounds of hard negative mining~\cite{Li2013} and
sampled $5\,000$ negative examples on each round.

\red{For each image we thus have 15 SVM outputs for each class (five features times
initial and two hard negative trained models) that we combine with simple
arithmetic mean in the late fusion stage.}
%%
The 80 fused output values, one for each object category, are then concatenated
to form a class membership vector for each image.
%%
These vectors we optionally use as inputs to the LSTM network and we denote it
as ``\emph{SVM80}'' in the rest of the thesis.

\subsection{Scene Detectors}
%%----------------------------------%%
%%

In order to provide the language model with explicit information on the visual
environment or the scene type of the images, we used the SUN Scene
Categorization Benchmark database~\cite{Xiao2010} and~\cite{Xiao2014} to create
a bank of visual detectors specialized for scene recognition.
%%
The version of the database we used contains 108,756 images associated with one
of 397 scene categories.

We extracted both ImageNet data trained and MIT Places data trained GoogLeNet
CNN features, as described in the previous section, for the images in the SUN
database.
%%
We then used features of all the images (not only the training split) for
training Radial Basis Function (RBF) Support Vector Machines (SVMs) with the
LIBSVM software~\cite{LIBSVM}.
%%
As we had three slightly different versions of each of the feature types, we
obtained the total of six SVM detectors for each scene category.

We applied each of the detectors to the images of the COCO dataset and used the
simple arithmetic mean for the late fusion of the detector outputs.
%%
The concatenation of the fused category-wise detector outputs results in
397-dimensional feature vectors for the respective images.
%%
These feature vectors are referred to as ``\emph{SUN397}'' in the rest of the
thesis.

%% --------------------------------------
\subsection{Spatial Map Encodings}
%%----------------------------------%%
Another important source of information relevant to generating captions is the
relative location of the objects in the image. 
%%
Knowing the locations of the objects helps to infer their roles in the scene and
also to choose the right adjectives to describe their positions.

For this purpose we use an object detector network, specifically the Faster
Region-based Convolutional Neural Network (R-CNN) proposed in
\cite{ren15fasterrcnn}, to detect the objects in image and to predict their
bounding boxes.
%%
We then project these bounding boxes onto a regular grid (eg.\@ $4\times4$) over
the image. 
%%
Each of the 80 object categories maintains its own grid and each grid cell
accumulates the intersection over union (IoU) value of any detected bounding box
of that object overlapping with it.
%%
This IoU value is also scaled with detection confidence score generated by the
Faster R-CNN for that bounding box, as shown in (\ref{eqn:Iou}).
%%
Thus, we get 80 spatial maps of size $m\times n$. 
%%
These representations are then concatenated to produce an $m\times
n\times80$~-dimensional feature vector.
%%
The value of the feature vector component corresponding to the class $c$ and the
grid cell position $(i,j)$ is thus computed as:
%%
\begin{equation} \label{eqn:Iou} F_c(i,j) = \sum_{b_k \in \text{\it
BB}(c)}p(b_k)\frac{ A(b_k \cap G(i,j))}{A(b_k \cup G(i,j))} \;, \end{equation}
%%
where $A(\cdot)$ is the pixel area, $\text{\it BB}(c)$ are the bounding boxes
detected for objects of class $c$, $p(b_k)$ is the confidence assigned by the
detector to box $b_k$ and $G(i,j)$ is the grid cell at position $(i,j)$.
%%
We abbreviate these features as ``$m$$\times${}$n$IoU'' in the result tables.

We also experiment with replacing the bounding box with a Gaussian whose mean is
at the center of the bounding box and standard deviation is the length of the
box diagonal.
%%
Then, instead of the IoUs, each grid cell accumulates the integrals of the
Gaussians it overlaps with (over the overlapping region) as:
%%
\begin{equation} \label{eqn:Gauss} F_c(i,j) = \sum_{b_k \in \text{\it
        BB}(c)}\text{\hspace*{-7mm}} \iint\displaylimits_%
{\text{\hspace{9mm}}b_k \cap G(i,j)}\text{\hspace*{-5mm}}
p(b_k)N(\text{center}(b_k),\text{diag}(b_k)) \; , \end{equation}
%%
where $N(\mu,\sigma)$ are Gaussians of given mean and standard deviation.
%%
We abbreviate these features as ``$m$$\times${}$n$Gauss'' in the result tables.

As an alternative to the $m\times n$ non-overlapping grid, we have used also
``$m+n$ partitioning'' of the images.
%%
By this we mean that the images are split in $m$ horizontal and $n$ vertical
regions of equal width or height, respectively.
%%
The localization information provided by this representation is not as accurate
as that of the $m\times n$ grid model.
%%
Instead, it can be argued to be more robust against variations in the relative
placements of the objects.
%%
The calculation of these features is analogous to that of the grid-based ones,
only the definition of the grid cells $G(i,j)$ is different.
%%
We abbreviate these features as ``$m$+$n$Gauss'' in the figure and in the result
tables.

Setting the grid size parameters $m=n=1$ we lose all localization information
and get Faster R-CNN based feature vectors dimensionally equivalent to the full
image, SVM-based object detection features described in the previous section.
%%
These vectors are shortened as ``FRC80'' in Figure~\ref{fig:fullModel} and in
the results of Table~\ref{tab:resultsVal} when used as inputs to the language
model.

%%
We have to note here that it is computationally expensive relatively to extract
these features, as it relies on running the Faster-RCNN network on the input
image which, despite the moniker, is still much slower than running a single CNN
like GoogleLeNet on the input image.

\red{Visualizations of feature vectors ability to distinguish presence and
absence of pairs of objects? Or t-sne of vectors corresponding to different no
of objects etc..?}

%%===========================================================================%%
\section{Video Feature Extraction}
\fixme{Upto 3 pages: mostly fresh writing}
\label{sec:VideoFeat}
We use two different paradigms for video feature extraction.
%%
The first one is to treat the video as just a sequence of 2-D static images and
use CNNs trained on ImageNet~\cite{ImagenetOrig} to extract static image
features from these frames.
%%
The second approach is to treat the video as 3-D data, consisting of a
sequence of video segments, and use methods which also consider the variations
along the time dimension for feature extraction.
%%

In both the above methods, pooling methods are used to combine multiple
frame/segment level features into one video-level feature vector.
%%
Number of feature vectors can also vary depending on the length of the video.
%%
But, since our language model takes only single feature of vector of fixed size
as input, we need to further compress these multiple feature vectors into a
single video feature vector. 
%%
We will explore two different methods to accomplish this, one relying on simple
pooling techniques like mean/max pooling and the other based on training a
feature encoding recurrent network.
%%

%%----------------------------------%%
\subsection{Key-frame and multi-frame features}
ALSO COVER POOLING HERE
RANK POOLING???

In this approach, we will treat the video as a bag of static images, and use the
same feature extraction techniques we used for image captioning to extract
features from these video frames.
%%
The idea here is to capture details of the scenes shown in the video, objects
present in it and their attributes.
%%
If the clip is very short and only consists of a single shot, then it might be
sufficient to extract these features from a single key-frame extracted from the
center of the video. 
%%
But the if the video clip is long, we will need to extract the frame-level
features from multiple frames, to sufficiently summarize the content in the
video.

%%
In order to keep the computational time to be reasonable, we refrain from
running all the feature extraction methods discussed before for images.
%%
Instead, we only extract the gCNN features and the explicit object recognition
features~(\emph{SVM80}) from the video frames.
%%
We also improve efficiency, by sampling only the key-frame in case of LSMDC
dataset and one frame every second in MSR-VTT dataset for feature extraction.
%%
The feature extraction procedure for both gCNN and SVM80 features remain the
same as described before for images in sections~\ref{subsec:gCNN} and
\ref{subsec:svm80}. 

%% ---------------------------------------------------------------------------
\subsection{Segment-level Features}
%%----------------------------------%%
Although, the frame level features do well in capturing the overall scene
context of the videos, they fail to recognize actions and other motion related
events.
%%
This is because lot of action in vidoes involve very local motion patterns,
which is hard to capture when looking at frames individually.
%%
Such actions can occur even without any whole objects moving, and thus causing
the CNN feature extractors to not notice any change.
%%

But this information is vital for video description task, as describing the
salient actions occuring in the video correctly is an important part of it.
%%
Thus we need video feature extraction methods, which can effectively encode
information about local motion patterns existing in the video into a fixed size
feature vector.
%%
For this purpose we use two feature extraction methods which treat videos as a
series of video segments and operate on these video segments to extract motion
information.
These are dense trajectories~\cite{DBLP:conf/cvpr/WangKSL11, Wang2013} and 3-D CNN
network based C3D features~\cite{DBLP:C3D} and we will discuss them in detail in
following sub sections.

%%----------------------------------%%
\subsubsection{Trajectory Features}

Dense trajectories video features have been, despite being hand-engineered
features without any learning from the data, one of the best performing features
on various video analysis tasks involving action recognition.
%%
The technique involves sampling interest points from an initial frame in the
video and to track these interest points across the time.
%%
The location of interest points which can be tracked reliably are concatenated
to obtain several trajectories in the video.
%%
Local descriptors are then extracted from patches around the points in the
trajectory which serve as features representing the trajectory.
%%
We use the same four local descriptors as described in
~\cite{DBLP:conf/cvpr/WangKSL11, Wang2013}, namely histogram of oriented
gradients~(HOG), histograms of optical flow~(HOF) and motion boundary
descriptors in horizontal and vertical directions~(MBHx \& MBHy).
%%
Then, videos can be represented based on the type of trajectories it contains.
%%
This can be accomplished by using vector encoding methods like bag-of-words or
fisher vector encoding to encode the arbitrary number of trajectory features
extracted from a video into a single vector of fixed dimensions.
%%

We have tried both the standard~\cite{DBLP:conf/cvpr/WangKSL11} and the improved
versions~\cite{Wang2013} of the dense trajectory features.
%%
For both versions, the trajectories and their descriptors are first extracted
from the entire video, with limiting the trajectory length to be maximum of 15
frames.
%%
All of these five features, i.e. trajectory co-ordinates and the 4 descriptors,
are separately encoded into a fixed-size vector using bag-of-features encoding
with a codebook of 1000 vectors.
%%

In bag-of-features encoding, first a fixed codebook containing representative
sample features from the training set is chosen.
%%
In our case, the codebook is obtained using k-means clustering on random 250k
trajectory samples from the training set, with k=1000.
%%
Then each feature vector from the set of features of a video is assigned to its
nearest codebook vector.
%%
These assignments counts are accumulated for each codebook entry to give us a
histogram for each video, with the histogram having 1000 bins. representation of the 
%%
Finally, concatenating the vector encodings of each of the descriptors we get a
video feature vector of 5000 dimensions. 
\red{SHOW VISUALIZATIONS of Trajectories}

\subsubsection{3D Convolutional Network}
%%----------------------------------%%
As an alternative to the hand crafted dense trajectory features, we also extract
video-segment features using a deep neural network based on 3-D
convolutions. 
%%
Specifically, we use C3D~\cite{DBLP:C3D} network, pre-trained on the Sports-1M
dataset.
%%

Inspired by the success of deep 2-D convolutional neural networks as image
feature extractors, C3D model attempts to employ similar deep networks to learn
video representations.
%%
But since videos have an additional temporal dimension, the 2-D convolutions
used in image CNNs are replaced with 3 dimensional convolutional filters in C3D.
%%
Influenced by the use of small 3x3 kernels in the VGG~\cite{Simonyan14c}
network, C3D uses only 3x3x3 convolutional filters.
%%
Also to prevent loss of temporal information quickly, the pooling operations are
also kept to small windows of size 2x2x2.
%%

The network we use is pre-trained on the large Sports-1M dataset, for the task
of classifying the video into one among 487 classes of sports related actions.
%%
To extract features using C3D, input videos are first cut into non-overlapping
video segments of 16 frames long.
%%
Then each segment is input into the C3D network and the activations from the
\emph{fc6} and \emph{fc7} layer of the network is extracted as the segment level
features.
%%
This gives us a set of 4096-dimensional feature vectors which represent the
input video.

\red{Visualizations of video feature vectors T-sne with 20 categories ??}

\subsection{Feature Encoding Network}

% Comment: If your sentence ends in a capital letter, like here, you should
% write \@ before the period; otherwise LaTeX will assume that this is not
% really an end of the sentence and will not put a large enough space after the
% period. That is, LaTeX assumes that you are (for example), enumerating using
% capital roman numerals, like I. do something, II. do something else. In this
% case, the periods do not end the sentence.

% Similarly, if you do need a normal space after a period (instead of
% the longer sentence separator), use \  (backslash and space) after the
% period. Like so: a.\ first item, b.\ second item.

